{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9c3ea9c",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "BreakPhaseDataProcessor module processes data from the experiment \n",
    "processing participants physiological data xlxs files and \n",
    "pulling break lenght data from virtual environment dataset to merge it for analysis\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def2ac4b",
   "metadata": {},
   "source": [
    "# init (setup of the environment):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aaf0a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupFunction(setupFull,suppressWarnings):\n",
    "    \n",
    "    if setupFull:\n",
    "        #!pip install pathlib\n",
    "        #!pip install -U setuptools pip\n",
    "        #!pip uninstall cupy\n",
    "        #!git clone --recursive https://github.com/cupy/cupy.git\n",
    "        #!cd cupy\n",
    "        #!pip cupy install --no-cache-dir\n",
    "        \n",
    "        # Make sure to run notebook with admin privilages when installing packages\n",
    "\n",
    "        !pip install https://github.com/paulvangentcom/heartrate_analysis_python/archive/master.zip\n",
    "\n",
    "        #update pip if using git+\n",
    "        !pip install --upgrade pip\n",
    "\n",
    "        !pip install git+https://github.com/paulvangentcom/heartrate_analysis_python.git\n",
    "\n",
    "        !pip3 install --upgrade pandas --user\n",
    "\n",
    "    # for graphing library: \n",
    "        !pip install bokeh --user\n",
    "        !pip install numpy --no-index --trusted-host=None --find-links=./ --user\n",
    "        !pip install scipy --no-index --trusted-host=None --find-links=./ --user\n",
    "        !pip install scikit-learn --no-index --trusted-host=None --find-links=./ --user\n",
    "        !pip install cvxopt --no-index --trusted-host=None --find-links=./ --user\n",
    "        !pip install kiwisolver --no-index --trusted-host=None --find-links=./ --user\n",
    "        !pip install cycler --no-index --trusted-host=None --find-links=./ --user\n",
    "        !pip install dateutils --user\n",
    "        !pip install pyparsing --user\n",
    "        !pip install matplotlib --no-index --trusted-host=None --find-links=./ --user\n",
    "        !pip install PyQt5 --user\n",
    "        !pip install pylint --user\n",
    "        !pip install spyder --user\n",
    "        !pip install https://github.com/neuropsychology/NeuroKit.py/zipball/master --user\n",
    "        !pip install https://github.com/neuropsychology/Neuropsydia.py/zipball/master --user\n",
    "        !pip install gcloud --user\n",
    "        !pip install --upgrade setuptools --user\n",
    "    #PROBLEMS WITH LIB BELOW\n",
    "        #pip install https://github.com/lciti/cvxEDA/blob/792844420df7d83c2e061cc8a7bb8ca24d4c29b5/src/cvxEDA.py --user\n",
    "        #ERROR: Cannot determine archive format\n",
    "        #!pip install https://github.com/lciti/cvxEDA/archive/master.zip --user\n",
    "        \n",
    "        !pip install https://github.com/HIIT/Ledapy/archive/master.zip --user\n",
    "    \n",
    "    if(suppressWarnings):\n",
    "        warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0266a323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import heartpy as hp\n",
    "\n",
    "from bokeh.plotting import figure, output_file, save\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cvxopt as cv\n",
    "import cvxopt.solvers\n",
    "\n",
    "\n",
    "\n",
    "setupFull = False\n",
    "suppressWarnings = False # tons of errors from data being defragmented, but most likley due to last update\n",
    "setupFunction(setupFull, suppressWarnings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fb9e71",
   "metadata": {},
   "source": [
    "For multiprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee8ab571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading # used in first fundamental approach\n",
    "import concurrent.futures #used for more optimized multithreading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cd7e122",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function cvxEDA embeded here due to an error: \n",
    "File \"setup.py\" not found for legacy project https://github.com/lciti/cvxEDA/archive/master.zip\n",
    "______________________________________________________________________________\n",
    " File:                         cvxEDA.py\n",
    " Last revised:                 07 Nov 2015 r69\n",
    " ______________________________________________________________________________\n",
    " Copyright (C) 2014-2015 Luca Citi, Alberto Greco\n",
    " \n",
    " This program is free software; you can redistribute it and/or modify it under\n",
    " the terms of the GNU General Public License as published by the Free Software\n",
    " Foundation; either version 3 of the License, or (at your option) any later\n",
    " version.\n",
    " \n",
    " This program is distributed in the hope that it will be useful, but WITHOUT\n",
    " ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n",
    " FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\n",
    " \n",
    " You may contact the author by e-mail (lciti@ieee.org).\n",
    " ______________________________________________________________________________\n",
    " This method was first proposed in:\n",
    " A Greco, G Valenza, A Lanata, EP Scilingo, and L Citi\n",
    " \"cvxEDA: a Convex Optimization Approach to Electrodermal Activity Processing\"\n",
    " IEEE Transactions on Biomedical Engineering, 2015\n",
    " DOI: 10.1109/TBME.2015.2474131\n",
    " If you use this program in support of published research, please include a\n",
    " citation of the reference above. If you use this code in a software package,\n",
    " please explicitly inform the end users of this copyright notice and ask them\n",
    " to cite the reference above in their published research.\n",
    " ______________________________________________________________________________\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "\n",
    "IMPORTANT BELOW!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "\"\"\"\n",
    "# can try to use CuPy instead of numpy to push the parallel processing of this data to GPU\n",
    "#import CuPy\n",
    "import cvxopt as cv\n",
    "import cvxopt.solvers\n",
    "\n",
    "def cvxEDA(y, delta, tau0=2., tau1=0.7, delta_knot=10., alpha=8e-4, gamma=1e-2,\n",
    "           solver=None, options={'reltol':1e-9}):\n",
    "    \"\"\"CVXEDA Convex optimization approach to electrodermal activity processing\n",
    "    This function implements the cvxEDA algorithm described in \"cvxEDA: a\n",
    "    Convex Optimization Approach to Electrodermal Activity Processing\"\n",
    "    (http://dx.doi.org/10.1109/TBME.2015.2474131, also available from the\n",
    "    authors' homepages).\n",
    "    Arguments:\n",
    "       y: observed EDA signal (we recommend normalizing it: y = zscore(y))\n",
    "       delta: sampling interval (in seconds) of y\n",
    "       tau0: slow time constant of the Bateman function\n",
    "       tau1: fast time constant of the Bateman function\n",
    "       delta_knot: time between knots of the tonic spline function\n",
    "       alpha: penalization for the sparse SMNA driver\n",
    "       gamma: penalization for the tonic spline coefficients\n",
    "       solver: sparse QP solver to be used, see cvxopt.solvers.qp\n",
    "       options: solver options, see:\n",
    "                http://cvxopt.org/userguide/coneprog.html#algorithm-parameters\n",
    "    Returns (see paper for details):\n",
    "       r: phasic component\n",
    "       p: sparse SMNA driver of phasic component\n",
    "       t: tonic component\n",
    "       l: coefficients of tonic spline\n",
    "       d: offset and slope of the linear drift term\n",
    "       e: model residuals\n",
    "       obj: value of objective function being minimized (eq 15 of paper)\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(y)\n",
    "    y = cv.matrix(y)\n",
    "\n",
    "    # bateman ARMA model\n",
    "    a1 = 1./min(tau1, tau0) # a1 > a0\n",
    "    a0 = 1./max(tau1, tau0)\n",
    "    ar = np.array([(a1*delta + 2.) * (a0*delta + 2.), 2.*a1*a0*delta**2 - 8.,\n",
    "        (a1*delta - 2.) * (a0*delta - 2.)]) / ((a1 - a0) * delta**2)\n",
    "    ma = np.array([1., 2., 1.])\n",
    "\n",
    "    # matrices for ARMA model\n",
    "    i = np.arange(2, n)\n",
    "    A = cv.spmatrix(np.tile(ar, (n-2,1)), np.c_[i,i,i], np.c_[i,i-1,i-2], (n,n))\n",
    "    M = cv.spmatrix(np.tile(ma, (n-2,1)), np.c_[i,i,i], np.c_[i,i-1,i-2], (n,n))\n",
    "\n",
    "    # spline\n",
    "    delta_knot_s = int(round(delta_knot / delta))\n",
    "    spl = np.r_[np.arange(1.,delta_knot_s), np.arange(delta_knot_s, 0., -1.)] # order 1\n",
    "    spl = np.convolve(spl, spl, 'full')\n",
    "    spl /= max(spl)\n",
    "    # matrix of spline regressors\n",
    "    i = np.c_[np.arange(-(len(spl)//2), (len(spl)+1)//2)] + np.r_[np.arange(0, n, delta_knot_s)]\n",
    "    nB = i.shape[1]\n",
    "    j = np.tile(np.arange(nB), (len(spl),1))\n",
    "    p = np.tile(spl, (nB,1)).T\n",
    "    valid = (i >= 0) & (i < n)\n",
    "    B = cv.spmatrix(p[valid], i[valid], j[valid])\n",
    "\n",
    "    # trend\n",
    "    C = cv.matrix(np.c_[np.ones(n), np.arange(1., n+1.)/n])\n",
    "    nC = C.size[1]\n",
    "\n",
    "    # Solve the problem:\n",
    "    # .5*(M*q + B*l + C*d - y)^2 + alpha*sum(A,1)*p + .5*gamma*l'*l\n",
    "    # s.t. A*q >= 0\n",
    "\n",
    "    old_options = cv.solvers.options.copy()\n",
    "    cv.solvers.options.clear()\n",
    "    cv.solvers.options.update(options)\n",
    "    if solver == 'conelp':\n",
    "        # Use conelp\n",
    "        z = lambda m,n: cv.spmatrix([],[],[],(m,n))\n",
    "        G = cv.sparse([[-A,z(2,n),M,z(nB+2,n)],[z(n+2,nC),C,z(nB+2,nC)],\n",
    "                    [z(n,1),-1,1,z(n+nB+2,1)],[z(2*n+2,1),-1,1,z(nB,1)],\n",
    "                    [z(n+2,nB),B,z(2,nB),cv.spmatrix(1.0, range(nB), range(nB))]])\n",
    "        h = cv.matrix([z(n,1),.5,.5,y,.5,.5,z(nB,1)])\n",
    "        c = cv.matrix([(cv.matrix(alpha, (1,n)) * A).T,z(nC,1),1,gamma,z(nB,1)])\n",
    "        res = cv.solvers.conelp(c, G, h, dims={'l':n,'q':[n+2,nB+2],'s':[]})\n",
    "        obj = res['primal objective']\n",
    "    else:\n",
    "        # Use qp\n",
    "        Mt, Ct, Bt = M.T, C.T, B.T\n",
    "        H = cv.sparse([[Mt*M, Ct*M, Bt*M], [Mt*C, Ct*C, Bt*C], \n",
    "                    [Mt*B, Ct*B, Bt*B+gamma*cv.spmatrix(1.0, range(nB), range(nB))]])\n",
    "        f = cv.matrix([(cv.matrix(alpha, (1,n)) * A).T - Mt*y,  -(Ct*y), -(Bt*y)])\n",
    "        res = cv.solvers.qp(H, f, cv.spmatrix(-A.V, A.I, A.J, (n,len(f))),\n",
    "                            cv.matrix(0., (n,1)), solver=solver)\n",
    "        obj = res['primal objective'] + .5 * (y.T * y)\n",
    "    \n",
    "    \n",
    "    cv.solvers.options.clear()\n",
    "    cv.solvers.options.update(old_options)\n",
    "\n",
    "    l = res['x'][-nB:]\n",
    "    d = res['x'][n:n+nC]\n",
    "    t = B*l + C*d\n",
    "    q = res['x'][:n]\n",
    "    p = A * q\n",
    "    r = M * q\n",
    "    e = y - r - t\n",
    "\n",
    "    return (np.array(a).ravel() for a in (r, p, t, l, d, e, obj))\n",
    "    \"\"\"\n",
    "    BELOW COMMENTED OUT TO ENABLE OPTIONS\n",
    "    ????? Double check it later\n",
    "    ????? seems to be clearing and reseting if in the loop?\n",
    "    ...new approach pass dictionary of solver parameters as an option...in test \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b235543c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def settingUpPaths():\n",
    "    os.chdir(os.path.abspath(os.curdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d03dba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "settingUpPaths()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724b3296",
   "metadata": {},
   "source": [
    "# Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd41995",
   "metadata": {},
   "source": [
    "## 1 : Environment (simulation) data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbb6a5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullEnvironmentData(ID, GR): #participantID,participantGR\n",
    "    #First_VR_Experiment_Files/\n",
    "    pathToEnvironmentData = 'RawData/Environment Parameters/'\n",
    "    environmentDataPath = os.path.join(os.getcwd(), pathToEnvironmentData)\n",
    "    filePath = 'Sample_Number-' + str(ID) + '_Group_Number-' + str(GR) + '.csv'    \n",
    "    pathToEnvironmentFile = os.path.join(environmentDataPath, filePath)\n",
    "    environment_data = pd.read_csv(pathToEnvironmentFile , delimiter=',', dtype=None, parse_dates = True, skiprows = 1, infer_datetime_format = False, index_col=None, header = None,low_memory = False)\n",
    "    environmentColNames = [\"timestamp\",\"Ticks\",\"Frame_count\",\"Phase\",\"Time_in_Phase\",\"Real_Time_in_Phase\",\"Trial_Number\",\"Trial_Time\",\"Period_Type\",\"Time_in_Period\",\"Fidelity\",\"Model\",\"FOV\",\"Saturation\",\"Resolution\",\"Pixel_count\",\"Thread_sleep\",\"FPS_real\",\"Audio_clip\",\"STRANGE\"]\n",
    "    environment_data.columns = environmentColNames\n",
    "    \n",
    "    environment_data['timestamp'] = pd.to_datetime(environment_data['timestamp'], format=\"%d/%m/%Y %H:%M:%S.%f\", errors='coerce')\n",
    "    environment_data.set_index('timestamp')\n",
    "   \n",
    "    del environment_data['STRANGE']\n",
    "\n",
    "    return environment_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0ad505",
   "metadata": {},
   "source": [
    "## 2 : Physiological (participant) data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d730908",
   "metadata": {},
   "source": [
    "### 2.1 : pull physiological data from file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92410b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullPhysioData(ID, GR):#participantID,participantGR\n",
    "    #aadSize = pd.Timedelta(seconds=4)\n",
    "    #sliceStart = experimentStart - pd.Timedelta(seconds=4) - padSize\n",
    "    #sliceEnd = experimentEnd + padSize\n",
    "    \n",
    "    #First_VR_Experiment_Files/\n",
    "    pathToPhysiologicalData = 'RawData/Participant biofeedback/'\n",
    "    physiologicaDataPath = os.path.join(os.getcwd(), pathToPhysiologicalData)\n",
    "    #print(\"physiologicaDataPath\" + physiologicaDataPath)\n",
    "    filePath = str(ID) + '-' + str(GR) + '.csv'    \n",
    "    pathToPhysiologicaFile = os.path.join(physiologicaDataPath, filePath)\n",
    "    #print(\"pathToPhysiologicaFile\" + pathToPhysiologicaFile)\n",
    "    \n",
    "    data = pd.read_csv(pathToPhysiologicaFile,\n",
    "                       delimiter='\\t', \n",
    "                       dtype=None, \n",
    "                       parse_dates = True,\n",
    "                       skiprows = 3, \n",
    "                       infer_datetime_format = False, \n",
    "                       index_col=None, \n",
    "                       header = None, \n",
    "                       low_memory = False)\n",
    "    \n",
    "    participantsColNames = [\"timestamp\",\n",
    "                            \"A15\",\n",
    "                            \"A6\",\n",
    "                            \"A7\",\n",
    "                            \"GSR_Range\",\n",
    "                            \"Skin_Conductance\",\n",
    "                            \"Skin_Resistance\",\n",
    "                            \"PPG_A13\",\n",
    "                            \"PPG_IBI\",\n",
    "                            \"PPG_to_HR_CONSENSYS\",\n",
    "                            \"Pressure\",\n",
    "                            \"Temperature\",\n",
    "                            \"STRANGE\"]\n",
    "    \n",
    "    data.columns = participantsColNames\n",
    "    data.set_index('timestamp')\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'], infer_datetime_format  = True)\n",
    "    data[\"TIME\"] = data['timestamp']\n",
    "    del data['STRANGE']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29933c51",
   "metadata": {},
   "source": [
    "#### 2.1.1 Test HR signal quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e527487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qualityCheck(data):\n",
    "    dataLen = len(data['PPG_to_HR_CONSENSYS'])\n",
    "    ambigousMeasuresCount = 0\n",
    "    for point in data['PPG_to_HR_CONSENSYS']:\n",
    "        if point < 0:\n",
    "            ambigousMeasuresCount += 1\n",
    "        \n",
    "    ambigousMeasuresCount = (1-((dataLen - ambigousMeasuresCount)/dataLen)) * 100\n",
    "    if testModeGlobal:\n",
    "        print(f\"Percentage of ambigous HR data in the sample = {ambigousMeasuresCount} %\")\n",
    "    return ambigousMeasuresCount"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f251f3e",
   "metadata": {},
   "source": [
    "### 2.2 : interpolate HR = -1 (NA) values -> append HR_CONSENSYS_interpolated column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1415b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolateConsensysHRna(data, testMode = False):\n",
    "    \n",
    "    participant_data_HR_Interpolated = data['PPG_to_HR_CONSENSYS'].replace(-1, np.nan)\n",
    "    \n",
    "    if(np.isnan(participant_data_HR_Interpolated[0])):\n",
    "        if(testMode):\n",
    "            print(\"Consensys_HR : BACKFILL\")\n",
    "        firstValueIndex = pd.notnull(participant_data_HR_Interpolated).argmax()\n",
    "        participant_data_HR_Interpolated[0] = participant_data_HR_Interpolated[firstValueIndex]\n",
    "    \n",
    "    participant_data_HR_Interpolated.interpolate(inplace=True)\n",
    "    data['HR_CONSENSYS_interpolated'] = participant_data_HR_Interpolated\n",
    "    \n",
    "    \"\"\"OLD\n",
    "    participant_data_HR_Interpolated.interpolate(inplace=True)\n",
    "    location = data.columns.get_loc(\"PPG_to_HR_CONSENSYS\") + 1\n",
    "    data.insert(loc=location, column='HR_CONSENSYS_interpolated', value=participant_data_HR_Interpolated)\n",
    "    \"\"\"\n",
    "    \n",
    "    participant_data_IBI_Interpolated = data['PPG_IBI'].replace(-1, np.nan)\n",
    "        \n",
    "    if(np.isnan(participant_data_IBI_Interpolated[0])):\n",
    "        if(testMode):\n",
    "            print(\"Consensys_IBI : BACKFILL\")\n",
    "        firstValueIndex = pd.notnull(participant_data_IBI_Interpolated).argmax()\n",
    "        participant_data_IBI_Interpolated[0] = participant_data_IBI_Interpolated[firstValueIndex]\n",
    "    \n",
    "    participant_data_IBI_Interpolated.interpolate(inplace=True)\n",
    "    data['IBI_CONSENSYS_interpolated'] = participant_data_IBI_Interpolated\n",
    "    \n",
    "    \"\"\"OLD        \n",
    "    participant_data_IBI_Interpolated.interpolate(inplace=True)\n",
    "    location = data.columns.get_loc(\"PPG_IBI\") + 1\n",
    "    data.insert(loc=location, column='IBI_CONSENSYS_interpolated', value = participant_data_IBI_Interpolated)\n",
    "    \"\"\"\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e29464f",
   "metadata": {},
   "source": [
    "### 2.3 : add HRV and continous HR measures using HeartPy (hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21368018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_windows(data, sample_rate, windowsize=120, overlap=0, min_size=20):\n",
    "    '''make_windows function comes from the heartpy module but in 1.6 it throws access error hence moved to local:\n",
    "    \n",
    "    function that slices data into windows for concurrent analysis.\n",
    "    \n",
    "    keyword arguments:\n",
    "    - data: 1-dimensional numpy array containing heart rate sensor data\n",
    "    - sample_rate: sample rate of the data stream in 'data'\n",
    "    - windowsize: size of the window that is sliced\n",
    "    - overlap: overlap between two adjacent windows: 0 <= float < 1.0\n",
    "    - min_size: the minimum size for the last (partial) window to be included. Very short windows\n",
    "                might not stable for peak fitting, especially when significant noise is present. \n",
    "                Slightly longer windows are likely stable but don't make much sense from a \n",
    "                signal analysis perspective.\n",
    "    \n",
    "    returns index tuples of windows\n",
    "    '''\n",
    "    ln = len(data)\n",
    "    window = windowsize * sample_rate\n",
    "    stepsize = (1 - overlap) * window\n",
    "    start = 0\n",
    "    end = window\n",
    "    \n",
    "    slices = []\n",
    "    while end < len(data):\n",
    "        slices.append((start, end))\n",
    "        start += stepsize\n",
    "        end += stepsize\n",
    "    if (ln - start) / sample_rate >= min_size:\n",
    "        slices.append((start, ln))\n",
    "        \n",
    "    return np.array(slices, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39c15fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPeriod(data, sampleRate, winSize, overLap):\n",
    "    slicesArray = make_windows(data, sample_rate = sampleRate, windowsize = winSize, overlap = overLap)\n",
    "    # if using make_windows from heartpy make sure to change to hp.meak_windows()\n",
    "    period = slicesArray[1,0]\n",
    "    return period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dba56556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContDataFromHP(data, sampleRate, winSize, overLap, scale, period, testMode = False):\n",
    "    \n",
    "    if testMode:\n",
    "        startTime0 = time.time()\n",
    "        print(\"================TEST getContDataFromHP ===============\" )\n",
    "        print(\"type(data) : {}\".format(type(data)))\n",
    "        print(\"len(data) : {}\".format(len(data)))\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        working_data, measures = hp.process(data, \n",
    "                                            sample_rate = sampleRate,\n",
    "                                            #segment_width = winSize,\n",
    "                                            windowsize = winSize,\n",
    "                                            #segment_overlap =overLap, \n",
    "                                            # mode = \"full\", \n",
    "                                            # segment_min_size = -1, \n",
    "                                            # clipping_scale = scale, \n",
    "                                            # hampel_correct = True\n",
    "                                            calc_freq = True,\n",
    "                                            high_precision = True,\n",
    "                                            clean_rr = True,# (bool) - if true, the RR_list is further cleaned with an outlier rejection pass default : False\n",
    "                                            )\n",
    "    except Exception as e:\n",
    "        print(\"error_3 : HP.process()\")\n",
    "        print(e)\n",
    "    \n",
    "    #print(\"measures.get('bpm')\")\n",
    "    #print(measures.get('bpm'))\n",
    "    \n",
    "    #print(\"measures.items()\")\n",
    "    #print(measures.items())\n",
    "    \"\"\"    \n",
    "    \n",
    "    try:\n",
    "        segmented_working_data, segmented_measures = hp.process_segmentwise(data, \n",
    "                                                                            sample_rate = sampleRate,\n",
    "                                                                            segment_width = winSize,\n",
    "                                                                            segment_overlap = overLap, \n",
    "                                                                            mode = \"full\", \n",
    "                                                                            segment_min_size = -1, \n",
    "                                                                            clipping_scale = scale, \n",
    "                                                                            hampel_correct = True)#, \n",
    "                                                                        #reject_segmentwise = True, bpmmin = 1, bpmmax = 5000)#, hampel_correct = True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"error_3 : HP -> process_segmentwise\")\n",
    "        print(e)\n",
    "    \n",
    "    hp_data = pd.DataFrame()\n",
    "    \n",
    "    for key, value in segmented_measures.items():   \n",
    "        hp_data[key] = value\n",
    "        \n",
    "        if testMode:\n",
    "            print(key)\n",
    "            print(type(value))\n",
    "            print(len(value))\n",
    "    \n",
    "\n",
    "    if testMode:\n",
    "        print(\"--------hp_data after for key, value in segmented_measures.items()-------\")\n",
    "        print(hp_data.head(2))\n",
    "        print(hp_data.tail(2))\n",
    "\n",
    "        print(\"----------\")\n",
    "        print(f\"type(hp_data['segment_indices'].iloc[0]) : {type(hp_data['segment_indices'].iloc[0])}\")\n",
    "        print(\"----------\")\n",
    "        print(\"hp_data['segment_indices'].iloc[0:3]\")\n",
    "        print(hp_data['segment_indices'].iloc[0:3])\n",
    "        print(\"hp_data['segment_indices'].iloc[-3:]\")\n",
    "        print(hp_data['segment_indices'].iloc[-3:])\n",
    "        print(\"----------\")\n",
    "\n",
    "        print(\"----------CONVERSION---------------\")\n",
    "    \n",
    "        startTime99 = time.time()\n",
    "    \n",
    "    hp_data['segment_indices'] = [sum(indices) // 2 for indices in hp_data['segment_indices']]\n",
    "    \n",
    "    if testMode:\n",
    "        print(f\"Time to average out : {time.time() - startTime99}\")\n",
    "\n",
    "        print(f\"type(hp_data['segment_indices'].iloc[0]) : {type(hp_data['segment_indices'].iloc[0])}\")\n",
    "        print(\"----------\")\n",
    "        print(\"hp_data['segment_indices'].iloc[0:3]\")\n",
    "        print(hp_data['segment_indices'].iloc[0:3])\n",
    "        print(\"hp_data['segment_indices'].iloc[-3:]\")\n",
    "        print(hp_data['segment_indices'].iloc[-3:])\n",
    "        print(\"----------\")\n",
    "    \n",
    "    NewDF = pd.DataFrame(np.nan , index =  range(0, len(data)) , columns = hp_data.columns)\n",
    "    \n",
    "    lastInd = len(hp_data)-1\n",
    "    \n",
    "    if testMode:\n",
    "        startTime = time.time()\n",
    "        print(NewDF.head(2))\n",
    "        print(NewDF.tail(2))\n",
    "        \n",
    "        print(f\"Last index : {lastInd}\")\n",
    "    \n",
    "        timeForInterpolation = []\n",
    "    \n",
    "    for column in hp_data:\n",
    "        if testMode:\n",
    "            print(\"column : {}\".format(column))\n",
    "        \n",
    "        #setting up first and last index is not nescessery, will be better to interpolate?\n",
    "        \"\"\"\n",
    "        print(\"hp_data[column].iloc[0] : {}\".format(hp_data[column].iloc[0]))\n",
    "        NewDF[column].iloc[0] = hp_data[column].iloc[0]\n",
    "        print(\"NewDF[column].iloc[0] : {}\".format(NewDF[column].iloc[0]))\n",
    "        \n",
    "        print(\"hp_data[column].iloc[-1] : {}\".format(hp_data[column].iloc[-1]))\n",
    "        NewDF[column].iloc[len(hp_data)-1] = hp_data[column].iloc[-1]\n",
    "        print(\"NewDF[column].iloc[] : {}\".format(hp_data[column].iloc[-1]))\n",
    "        \"\"\"\n",
    "        \n",
    "        i = 0        \n",
    "        for item in hp_data[column]:\n",
    "            if testMode:\n",
    "                print(\"item : {}\".format(item))\n",
    "            \n",
    "            NewDF[column].iloc[hp_data['segment_indices'].iloc[i]] = item\n",
    "            i += 1\n",
    "            \n",
    "        if testMode:\n",
    "            startTime = time.time()\n",
    "        \n",
    "        NewDF[column].interpolate(method='cubic', order=2, inplace = True)#(method='polynomial', order=2, inplace = True)\n",
    "        #NewDF[column].interpolate(method='linear', inplace = True)\n",
    "        \n",
    "        if testMode:\n",
    "            timeForInterpolation.append(time.time() - startTime)\n",
    "    \n",
    "        if testMode:\n",
    "            print(f\"interpolations took : {sum(timeForInterpolation)/len(timeForInterpolation)}\")\n",
    "    \n",
    "    firstInd = hp_data['segment_indices'].iloc[0]\n",
    "    lastInd = hp_data['segment_indices'].iloc[-1]\n",
    "    \n",
    "    if testMode:\n",
    "        print(\"---------interpolated-----------\")\n",
    "        print(\"---------------------NewDF.head(2)\")\n",
    "        print(NewDF.head(2))\n",
    "        print(\"NewDF[firstInd - 3 : firstInd + 3]\")\n",
    "        print(NewDF[firstInd - 3 : firstInd + 3])\n",
    "        print(\"NewDF[lastInd - 3 : lastInd + 3]\")\n",
    "        print(NewDF[lastInd - 3 : lastInd + 3])\n",
    "        print(\"---------------------NewDF.tail(2)\")\n",
    "        print(NewDF.tail(2))\n",
    "        print(\"---------interpolated END-----------\")\n",
    "    \n",
    "    hp_data = NewDF\n",
    "    \n",
    "    if testMode:\n",
    "        print(\"---------------------hp_data.head(2)\")\n",
    "        print(hp_data.head(2))\n",
    "        print(\"---------------------hp_data.tail(2)\")\n",
    "        print(hp_data.tail(2))\n",
    "\n",
    "        print(\"PRESTEST TOOK : {}\".format(time.time() - startTime))\n",
    "        print(\"=======PRETEST END=========\")\n",
    "    \n",
    "    # Interpolate NA:\n",
    "    \n",
    "    # HR:\n",
    "    hp_HR_Interpolated = hp_data['bpm'].replace(-1, np.nan)\n",
    "    hp_HR_Interpolated[hp_HR_Interpolated>200] = np.nan\n",
    "    hp_HR_Interpolated[hp_HR_Interpolated<30] = np.nan\n",
    "    \n",
    "    if(np.isnan(hp_HR_Interpolated[0])):\n",
    "        if testMode:\n",
    "            print(\"hp_HR_Interpolated : BACKFILL\")\n",
    "        firstValueIndex = pd.notnull(hp_HR_Interpolated).argmax()\n",
    "        hp_HR_Interpolated[0] = hp_HR_Interpolated[firstValueIndex]\n",
    "\n",
    "    hp_HR_Interpolated.interpolate(inplace=True)\n",
    "    hp_data['HR_HeartPy_interpolated'] = hp_HR_Interpolated\n",
    "    \n",
    "    # IBI:\n",
    "    hp_IBI_Interpolated = hp_data['ibi'].replace(-1, np.nan)\n",
    "    \n",
    "    if(np.isnan(hp_IBI_Interpolated[0])):\n",
    "        if testMode:\n",
    "            print(\"hp_IBI_Interpolated : BACKFILL\")\n",
    "        firstValueIndex = pd.notnull(hp_IBI_Interpolated).argmax()\n",
    "        hp_IBI_Interpolated[0] = hp_IBI_Interpolated[firstValueIndex]\n",
    "\n",
    "    hp_IBI_Interpolated.interpolate(inplace=True)\n",
    "    hp_data['IBI_HeartPy_interpolated'] = hp_IBI_Interpolated\n",
    "    \n",
    "    # sdnn:\n",
    "    hp_SDNN_Interpolated = hp_data['sdnn'].replace(-1, np.nan)\n",
    "    \n",
    "    if(np.isnan(hp_SDNN_Interpolated[0])):\n",
    "        if testMode:\n",
    "            print(\"hp_SDNN_Interpolated : BACKFILL\")\n",
    "        firstValueIndex = pd.notnull(hp_SDNN_Interpolated).argmax()\n",
    "        hp_SDNN_Interpolated[0] = hp_SDNN_Interpolated[firstValueIndex]\n",
    "    \n",
    "    hp_IBI_Interpolated.interpolate(inplace=True)\n",
    "    hp_data['SDNN_HeartPy_interpolated'] = hp_IBI_Interpolated\n",
    "    \n",
    "    # sdsd:\n",
    "    hp_SDSD_Interpolated = hp_data['sdsd'].replace(-1, np.nan)\n",
    "    \n",
    "    if(np.isnan(hp_SDSD_Interpolated[0])):\n",
    "        if testMode:\n",
    "            print(\"hp_SDSD_Interpolated : BACKFILL\")\n",
    "        firstValueIndex = pd.notnull(hp_SDSD_Interpolated).argmax()\n",
    "        hp_SDSD_Interpolated[0] = hp_SDSD_Interpolated[firstValueIndex]\n",
    "    \n",
    "    hp_SDSD_Interpolated.interpolate(inplace=True)\n",
    "    hp_data['SDSD_HeartPy_interpolated'] = hp_SDSD_Interpolated\n",
    "    \n",
    "    # rmssd:\n",
    "    hp_RMSSD_Interpolated = hp_data['rmssd'].replace(-1, np.nan)\n",
    "    \n",
    "    if(np.isnan(hp_RMSSD_Interpolated[0])):\n",
    "        if testMode:\n",
    "            print(\"hp_RMSSD_Interpolated : BACKFILL\")\n",
    "        firstValueIndex = pd.notnull(hp_RMSSD_Interpolated).argmax()\n",
    "        hp_RMSSD_Interpolated[0] = hp_RMSSD_Interpolated[firstValueIndex]\n",
    "    \n",
    "    hp_RMSSD_Interpolated.interpolate(inplace=True)\n",
    "    hp_data['RMSSD_HeartPy_interpolated'] = hp_RMSSD_Interpolated\n",
    "    \n",
    "    # PNN-20:\n",
    "    \n",
    "    hp_PNN20_Interpolated = hp_data['pnn20'].replace(-1, np.nan)\n",
    "    \n",
    "    if(np.isnan(hp_PNN20_Interpolated[0])):\n",
    "        if testMode:\n",
    "            print(\"hp_PNN20_Interpolated : BACKFILL\")\n",
    "        firstValueIndex = pd.notnull(hp_PNN20_Interpolated).argmax()\n",
    "        hp_PNN20_Interpolated[0] = hp_PNN20_Interpolated[firstValueIndex]\n",
    "    \n",
    "    hp_PNN20_Interpolated.interpolate(inplace=True)\n",
    "    hp_data['pNN20_HeartPy_interpolated'] = hp_PNN20_Interpolated\n",
    "    \n",
    "    # PNN-50:\n",
    "    hp_PNN50_Interpolated = hp_data['pnn50'].replace(-1, np.nan)\n",
    "    \n",
    "    if(np.isnan(hp_PNN50_Interpolated[0])):\n",
    "        if testMode:\n",
    "            print(\"hp_PNN20_Interpolated : BACKFILL\")\n",
    "        firstValueIndex = pd.notnull(hp_PNN50_Interpolated).argmax()\n",
    "        hp_PNN50_Interpolated[0] = hp_PNN50_Interpolated[firstValueIndex]\n",
    "    \n",
    "    hp_PNN50_Interpolated.interpolate(inplace=True)\n",
    "    hp_data['pNN50_HeartPy_interpolated'] = hp_PNN50_Interpolated\n",
    "    \n",
    "    if testMode:\n",
    "        print(\"TEST END getContDataFromHP ========{}\".format(time.time() - startTime0))\n",
    "\n",
    "        print(\"getContDataFromHP after all interpolations\")\n",
    "        print(hp_data.head())\n",
    "        print(\"hp_data[firstInd - 3 : firstInd + 3]\")\n",
    "        print(hp_data[firstInd - 3 : firstInd + 3])\n",
    "        print(\"hp_data[lastInd - 3 : lastInd + 3]\")\n",
    "        print(hp_data[lastInd - 3 : lastInd + 3])\n",
    "        print(hp_data.tail())\n",
    "\n",
    "    return hp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06e17ac",
   "metadata": {},
   "source": [
    "#### 2.3.2 : Save peak analysis plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75e9e0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePlot(data, sampleRate, winSize, scale):\n",
    "    working_data, measures = hp.process(data, sample_rate= sampleRate, windowsize= winSize, clipping_scale = scale, reject_segmentwise = False, bpmmin = 1, bpmmax = 5000)#, hampel_correct = True)\n",
    "    plot_object = hp.plotter(working_data, measures, show = False)\n",
    "    graphPath = \"Graphs/\"+ participantID + \"_\" + str(participantGR) + \"-HP_peaks.svg\";\n",
    "    plot_object.savefig(os.path.join(os.getcwd(),graphPath), dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2fa16e",
   "metadata": {},
   "source": [
    "#### 2.3.3 : Save HR plot figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c49ae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveHRplot(ID, GR, HP_data, Consensys_data):   \n",
    "    graphPath = \"Graphs\\/\"+ str(ID) + \"_\" + str(GR) + \"-HR.html\";\n",
    "    output_file(graphPath)\n",
    "    \n",
    "    objectToPlot = figure(\n",
    "        #y_range = (0,1),\n",
    "        #x_range = (0,1),\n",
    "        title = 'Heart Rate data - ID: ' + str(ID) + ' GR: ' + str(GR),\n",
    "        x_axis_label = 'Index[i]',\n",
    "        y_axis_label = 'HR[BPM]',\n",
    "        plot_width = 1600,\n",
    "        #plot_height = 900,\n",
    "        sizing_mode='scale_width'\n",
    "    )\n",
    "    \n",
    "    objectToPlot.line(HP_data.index.tolist(), HP_data['HR_HeartPy_interpolated'], \n",
    "                      legend_label='HP_HR-interpolated', line_color=\"red\", line_width = 1)\n",
    "    objectToPlot.line(HP_data.index.tolist(), HP_data['bpm'], \n",
    "                      legend_label='HeartPy_HR', line_color=\"blue\", line_width = 4,\n",
    "                      line_dash = 'dotted', line_alpha = .7)\n",
    "    objectToPlot.line(Consensys_data.index.tolist(), Consensys_data['PPG_to_HR_CONSENSYS'], \n",
    "                      legend_label='Consensys_HR', line_color=\"green\", line_width = 4,\n",
    "                      line_dash = 'dotted', line_alpha = .7)\n",
    "    objectToPlot.line(Consensys_data.index.tolist(), Consensys_data['HR_CONSENSYS_interpolated'], \n",
    "                      legend_label='Consensys_HR-interpolated', line_color=\"green\", line_width = 1)\n",
    "    \n",
    "    objectToPlot.background_fill_color = \"beige\"\n",
    "    objectToPlot.background_fill_alpha = 0.5\n",
    "    save(objectToPlot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aa08e7",
   "metadata": {},
   "source": [
    "### 2.4 : ***Merge Consensys and HeartPy data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d6e75f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeData(data1, data2):\n",
    "    mergedData = pd.concat([data1, data2], axis= 1)\n",
    "    return mergedData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94ad09b",
   "metadata": {},
   "source": [
    "### 2.5 : ***standardize data (Z-score)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8934c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardizeData(data):\n",
    "    data['Skin_Conductance_STD'] = (data['Skin_Conductance'] - data['Skin_Conductance'].mean())/data['Skin_Conductance'].std()\n",
    "    data['Skin_Resistance_STD'] = (data['Skin_Resistance'] - data['Skin_Resistance'].mean())/data['Skin_Resistance'].std()\n",
    "    data['HR_CONSENSYS_STD'] = (data['HR_CONSENSYS_interpolated'] - data['HR_CONSENSYS_interpolated'].mean())/data['HR_CONSENSYS_interpolated'].std()\n",
    "    data['HR_HP_STD'] = (data['HR_HeartPy_interpolated'] - data['HR_HeartPy_interpolated'].mean())/data['HR_HeartPy_interpolated'].std()\n",
    "    \n",
    "    \"\"\"OLD\n",
    "    standardizedData_SC = (data['Skin_Conductance'] - data['Skin_Conductance'].mean())/data['Skin_Conductance'].std()\n",
    "    location = data.columns.get_loc(\"Skin_Conductance\")\n",
    "    data.insert(loc=location, column='Skin_Conductance_STD', value = standardizedData_SC)\n",
    "    \n",
    "    standardizedData_SR = (data['Skin_Resistance'] - data['Skin_Resistance'].mean())/data['Skin_Resistance'].std()\n",
    "    location = data.columns.get_loc(\"Skin_Resistance\")\n",
    "    data.insert(loc=location, column='Skin_Resistance_STD', value = standardizedData_SR)\n",
    "    \n",
    "    standardizedData_CONS_HR = (data['HR_CONSENSYS_interpolated'] - data['HR_CONSENSYS_interpolated'].mean())/data['HR_CONSENSYS_interpolated'].std()\n",
    "    location = data.columns.get_loc(\"HR_CONSENSYS_interpolated\")\n",
    "    data.insert(loc=location, column='HR_CONSENSYS_STD', value = standardizedData_CONS_HR)\n",
    "    \n",
    "    standardizedData_HP_HR = (data['HR_HeartPy_interpolated'] - data['HR_HeartPy_interpolated'].mean())/data['HR_HeartPy_interpolated'].std()\n",
    "    location = data.columns.get_loc(\"HR_HeartPy_interpolated\")\n",
    "    data.insert(loc=location, column='HR_HP_STD', value = standardizedData_HP_HR)\n",
    "    \"\"\"\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0796e6fa",
   "metadata": {},
   "source": [
    "### 2.6 : ***normalize data (0-1)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20d505bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeData(data):\n",
    "    \n",
    "    data['Skin_Conductance_NORM'] = (data['Skin_Conductance'] - data['Skin_Conductance'].min())/(data['Skin_Conductance'].max()-data['Skin_Conductance'].min())\n",
    "    data['Skin_Resistance_NORM'] = (data['Skin_Resistance'] - data['Skin_Resistance'].min())/(data['Skin_Resistance'].max()-data['Skin_Resistance'].min())\n",
    "    data['HR_CONSENSYS_NORM'] = (data['HR_CONSENSYS_interpolated'] - data['HR_CONSENSYS_interpolated'].min())/(data['HR_CONSENSYS_interpolated'].max()-data['HR_CONSENSYS_interpolated'].min())\n",
    "    data['HR_HP_NORM'] = (data['HR_HeartPy_interpolated'] - data['HR_HeartPy_interpolated'].min())/(data['HR_HeartPy_interpolated'].max()-data['HR_HeartPy_interpolated'].min())\n",
    "    \n",
    "    \"\"\"OLD\n",
    "    normalizedData_SC = (data['Skin_Conductance'] - data['Skin_Conductance'].min())/(data['Skin_Conductance'].max()-data['Skin_Conductance'].min())    \n",
    "    location = data.columns.get_loc(\"Skin_Conductance_STD\")\n",
    "    data.insert(loc=location, column='Skin_Conductance_NORM', value = normalizedData_SC)\n",
    "    \n",
    "    normalizedData_SR = (data['Skin_Resistance'] - data['Skin_Resistance'].min())/(data['Skin_Resistance'].max()-data['Skin_Resistance'].min())\n",
    "    location = data.columns.get_loc(\"Skin_Resistance_STD\")\n",
    "    data.insert(loc=location, column='Skin_Resistance_NORM', value = normalizedData_SR)\n",
    "    \n",
    "    normalizedData_CONS_HR = (data['HR_CONSENSYS_interpolated'] - data['HR_CONSENSYS_interpolated'].min())/(data['HR_CONSENSYS_interpolated'].max()-data['HR_CONSENSYS_interpolated'].min())\n",
    "    location = data.columns.get_loc(\"HR_CONSENSYS_STD\")\n",
    "    data.insert(loc=location, column='HR_CONSENSYS_NORM', value = normalizedData_CONS_HR)\n",
    "    \n",
    "    normalizedData_HP_HR = (data['HR_HeartPy_interpolated'] - data['HR_HeartPy_interpolated'].min())/(data['HR_HeartPy_interpolated'].max()-data['HR_HeartPy_interpolated'].min())\n",
    "    location = data.columns.get_loc(\"HR_HP_STD\")\n",
    "    data.insert(loc=location, column='HR_HP_NORM', value = normalizedData_HP_HR)\n",
    "    \"\"\"    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0259313",
   "metadata": {},
   "source": [
    "### 2.7 process EDA using cvxEDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0597d2f4",
   "metadata": {},
   "source": [
    "I'm not sure why I set solver options to these features... no recollection of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6230e438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processEDA(data, testMode = False):\n",
    "    from cvxopt import solvers\n",
    "    solvers.options['show_progress'] = True\n",
    "    solvers.options['abstol'] = 1e-02\n",
    "    solvers.options['feastol'] = 1e-02\n",
    "    solvers.options['reltol']=1e-02\n",
    "    # had to comment out line 107 in cvxEDA.py : cv.solvers.options.clear() to enable seting options on cvxopt \n",
    "    \n",
    "    freq = 1.0 / 128.0\n",
    "    \n",
    "    \"\"\"\n",
    "    cvxEDA(y, delta, tau0=2., tau1=0.7, delta_knot=10., alpha=8e-4, gamma=1e-2,\n",
    "           solver=None, options={'reltol':1e-9}):\n",
    "    CVXEDA Convex optimization approach to electrodermal activity processing\n",
    "    This function implements the cvxEDA algorithm described in \"cvxEDA: a\n",
    "    Convex Optimization Approach to Electrodermal Activity Processing\"\n",
    "    (http://dx.doi.org/10.1109/TBME.2015.2474131, also available from the\n",
    "    authors' homepages).\n",
    "    Arguments:\n",
    "       y: observed EDA signal (we recommend normalizing it: y = zscore(y))\n",
    "       delta: sampling interval (in seconds) of y\n",
    "       tau0: slow time constant of the Bateman function\n",
    "       tau1: fast time constant of the Bateman function\n",
    "       delta_knot: time between knots of the tonic spline function\n",
    "       alpha: penalization for the sparse SMNA driver\n",
    "       gamma: penalization for the tonic spline coefficients\n",
    "       solver: sparse QP solver to be used, see cvxopt.solvers.qp\n",
    "       options: solver options, see:\n",
    "                http://cvxopt.org/userguide/coneprog.html#algorithm-parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    [phasicComponentData, p, tonicComponentData, l, d, e, obj] = cvxEDA(data['Skin_Conductance_STD'], freq, options = solvers.options) \n",
    "    # new way: , options = solvers.options\n",
    "    \"\"\"\n",
    "       r: phasic component\n",
    "       p: sparse SMNA driver of phasic component\n",
    "       t: tonic component\n",
    "       l: coefficients of tonic spline\n",
    "       d: offset and slope of the linear drift term\n",
    "       e: model residuals\n",
    "       obj: value of objective function being minimized (eq 15 of paper)\n",
    "    \"\"\"\n",
    "    if testMode:\n",
    "        print(\"TEST1 processEDA ===============\")\n",
    "        print(data.head(2))\n",
    "        print(\"TEST1 processEDA END============\")\n",
    "    \n",
    "    location = data.columns.get_loc(\"Skin_Conductance_STD\")\n",
    "    \n",
    "    #data.insert(loc=location, column='Skin_Conductance_Tonic_CVX', value = tonicComponentData)\n",
    "    \n",
    "    #print(\"TEST2 processEDA ===============\")\n",
    "    #print(data.head(2))\n",
    "    #print(\"TEST2 processEDA END============\")\n",
    "    if testMode:\n",
    "        print(\"TEST3 processEDA =============== concat test\")\n",
    "    \n",
    "    #print(type(phasicComponentData))\n",
    "    #phasicComponentData = pd.DataFrame(phasicComponentData)\n",
    "    #print(type(phasicComponentData))\n",
    "    #pd.concat([data, phasicComponentData]) # .insert(loc=location, column='Skin_Conductance_Phasic_CVX', value = phasicComponentData)\n",
    "    #cannot concatenate object of type '<class 'numpy.ndarray'>'; only Series and DataFrame objs are valid\n",
    "    \n",
    "    data['Skin_Conductance_Phasic_CVX'] = phasicComponentData[:]\n",
    "    data['Skin_Conductance_Tonic_CVX'] = tonicComponentData[:]\n",
    "    \n",
    "    if testMode:\n",
    "        print(data.head(2))\n",
    "        print(\"TEST3 processEDA END============\")\n",
    "    \n",
    "    #location = data.columns.get_loc(\"Skin_Conductance_STD\")\n",
    "    #data.insert(loc=location, column='Skin_Conductance_Phasic_CVX', value = phasicComponentData)\n",
    "\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef81031",
   "metadata": {},
   "source": [
    "### 2.8 smooth EDA and HR using centered SMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b266b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothHRandSC(data):\n",
    "    window1 = 128 # 1s\n",
    "    window2 = 3*128 # 3s\n",
    "    window3 = 60*128 # 1m\n",
    "    windows = [window1,window2,window3]\n",
    "    \n",
    "    listOfParametersToSmooth = ['Skin_Conductance',\n",
    "                                \"Skin_Conductance_NORM\",\n",
    "                                'Skin_Conductance_STD', \n",
    "                                \"Skin_Conductance_Phasic_CVX\",\n",
    "                                'Skin_Resistance_NORM',\n",
    "                                'Skin_Resistance_STD', \n",
    "                                'Skin_Resistance',\n",
    "                                'PPG_to_HR_CONSENSYS', \n",
    "                                'HR_CONSENSYS_NORM', \n",
    "                                'HR_CONSENSYS_STD', \n",
    "                                'HR_CONSENSYS_interpolated',\n",
    "                                'bpm',\n",
    "                                'HR_HeartPy_interpolated',\n",
    "                                'HR_HP_NORM',\n",
    "                                'HR_HP_STD']\n",
    "    \n",
    "    for parameter in listOfParametersToSmooth:\n",
    "        for window in windows:\n",
    "            sma = pd.Series(data[parameter]).rolling(window = window, center = True).mean()\n",
    "            columnName = parameter + \"_SMA(\" + str(window/128) + \"s)\"\n",
    "            data[columnName] = sma\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2a054af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothHRandSCewma(data, testMode = False):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    added NewDF due to:\n",
    "    <ipython-input-20-666b3cc13adb>:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
    "  data[columnName] = data[parameter].ewm(alpha = alpha).mean()\n",
    "    \"\"\"\n",
    "    NewDF = data.copy()\n",
    "    \n",
    "    alpha1 = 0.0001 # alpha proportional to the lenght of TS\n",
    "    alpha2 = 0.001 # mid range\n",
    "    alpha3 = 0.01  # near sample\n",
    "    alphas = [alpha1,alpha2,alpha3]\n",
    "    listOfParametersToSmooth = ['Skin_Conductance',\n",
    "                                \"Skin_Conductance_NORM\",\n",
    "                                'Skin_Conductance_STD', \n",
    "                                \"Skin_Conductance_Phasic_CVX\",\n",
    "                                'Skin_Resistance_NORM',\n",
    "                                'Skin_Resistance_STD', \n",
    "                                'Skin_Resistance',\n",
    "                                'PPG_to_HR_CONSENSYS', \n",
    "                                'HR_CONSENSYS_NORM', \n",
    "                                'HR_CONSENSYS_STD', \n",
    "                                'HR_CONSENSYS_interpolated',\n",
    "                                'bpm',\n",
    "                                'HR_HeartPy_interpolated',\n",
    "                                'HR_HP_NORM',\n",
    "                                'HR_HP_STD']\n",
    "    \n",
    "    for parameter in listOfParametersToSmooth:\n",
    "        for alpha in alphas:\n",
    "            columnName = \"{}_EWMA({})\".format(parameter , alpha)\n",
    "            if testMode:\n",
    "                print(columnName)\n",
    "            NewDF[columnName] = data[parameter].ewm(alpha = alpha).mean()\n",
    "        \n",
    "    return NewDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7d95bd",
   "metadata": {},
   "source": [
    "## 2.8 Add 1st order differentiation results column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b6a2b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff1st(data):\n",
    "    # could use a validated method from np or pd here np.diff:\n",
    "    \n",
    "    parametersToDifferentiate = [\"HR_CONSENSYS_interpolated\",\"HR_CONSENSYS_STD\",\"HR_CONSENSYS_NORM\",\n",
    "                                 \"HR_HeartPy_interpolated\",\"HR_HP_STD\",\"HR_HP_NORM\",\n",
    "                                 \"Skin_Conductance\",\"Skin_Conductance_STD\",\"Skin_Conductance_NORM\",\n",
    "                                 \"Skin_Resistance\",\"Skin_Resistance_STD\",\"Skin_Resistance_NORM\"]\n",
    "    \n",
    "    for parameter in parametersToDifferentiate:\n",
    "        \"\"\"\n",
    "        print(\"TEST ============== .diff()\")\n",
    "        print(parameter) \n",
    "        print(type(data[parameter]))\n",
    "        print(data[parameter][1])\n",
    "        print(type(data[parameter][1]))\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            diffData = data[parameter].diff() # 1st order\n",
    "        except Exception as e: \n",
    "            print(\"error_3__1 : diff()\")\n",
    "            print(e) \n",
    "            print(parameter) \n",
    "        \"\"\"\n",
    "        print(diffData.head(2))\n",
    "        print(\"TEST END ============== .diff()\")\n",
    "        \"\"\"\n",
    "        columnName = parameter + \"_DIFF(1)\"\n",
    "        data[columnName] = diffData\n",
    "        \n",
    "    \"\"\"\n",
    "    dataLength = len(data[\"HR_CONSENSYS_interpolated\"])\n",
    "    HR_CONSENSYS_DIFF1 = np.zeros(dataLength)\n",
    "    HR_HP_DIFF1 = np.zeros(dataLength)\n",
    "    SC_DIFF1 = np.zeros(dataLength)\n",
    "    SR_DIFF1 = np.zeros(dataLength)\n",
    "        \n",
    "    for i in range(0,dataLength):\n",
    "        if(i==0):\n",
    "            HR_CONSENSYS_DIFF1[i] = 0\n",
    "            HR_HP_DIFF1[i] = 0\n",
    "            SC_DIFF1[i] = 0\n",
    "            SR_DIFF1[i] = 0\n",
    "        else:\n",
    "            HR_CONSENSYS_DIFF1[i] = data[\"HR_CONSENSYS_interpolated\"][i] - data[\"HR_CONSENSYS_interpolated\"][i-1]\n",
    "            HR_HP_DIFF1[i] = data[\"HR_HeartPy_interpolated\"][i] - data[\"HR_HeartPy_interpolated\"][i-1]\n",
    "            SC_DIFF1[i] = data[\"Skin_Conductance\"][i] - data[\"Skin_Conductance\"][i-1]\n",
    "            SR_DIFF1[i] = data[\"Skin_Resistance\"][i] - data[\"Skin_Resistance\"][i-1]  \n",
    "    \n",
    "    location = data.columns.get_loc(\"HR_CONSENSYS_interpolated\") + 2\n",
    "    data.insert(loc=location, column='HR_CONSENSYS_DIFF1', value = HR_CONSENSYS_DIFF1)\n",
    "    location = data.columns.get_loc(\"HR_HeartPy_interpolated\") + 2\n",
    "    data.insert(loc=location, column='HR_HP_DIFF1', value = HR_HP_DIFF1)\n",
    "    location = data.columns.get_loc(\"Skin_Conductance\") + 2\n",
    "    data.insert(loc=location, column='Skin_Conductance_DIFF1', value = SC_DIFF1)\n",
    "    location = data.columns.get_loc(\"Skin_Resistance\") + 2\n",
    "    data.insert(loc=location, column='Skin_Resistance_DIFF1', value = SR_DIFF1)\n",
    "    \"\"\"    \n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfda750",
   "metadata": {},
   "source": [
    "## 3 : Merge Participant and Environment data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da496969",
   "metadata": {},
   "source": [
    "### 3.1 : merge_asof on nearest timestamp (tolerance = 10ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d72e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeAll(data1, data2):\n",
    "    timeS = data1['timestamp'][1] - data2['timestamp'][1]\n",
    "    \n",
    "    all_data = pd.merge_asof(data1, data2, on=\"timestamp\", direction='nearest', tolerance = pd.Timedelta('50ms'))\n",
    "    #10ms produces NA as there is not enough overlap to account for lags in the environment data which goes to 40ms at times\n",
    "\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a362deb",
   "metadata": {},
   "source": [
    "### 3.2 : Add absolute experiment time:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b50239",
   "metadata": {},
   "source": [
    "function deprecated as the deltatime object was converted to a float in the spreadsheet (i.e.: 0.0000000810185185185185) \n",
    "I could format it as a string or dedicate more time for converting it to a timestamp but it was only an additional feature\n",
    "So I have abandoned it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff61a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def absoluteTime(data):\n",
    "    startTime = data[\"timestamp\"].iloc[0]\n",
    "    #print(data[\"timestamp\"].head())\n",
    "    #print(type(data[\"timestamp\"]))\n",
    "    #print(data['timestamp'] - startTime)\n",
    "    #print(type(data['timestamp'] - startTime))\n",
    "    \n",
    "    try:\n",
    "        #data[\"TEMP_Time\"] = pd.to_datetime(data['timestamp'] - startTime) ###### rewrote to retain datetime data\n",
    "        #... delta time is not time stamp\n",
    "        # if I really really need it:\n",
    "        # data data[\"TEMP_Time\"] = data['timestamp'] + data[\"TEMP_Time\"]\n",
    "        #???\n",
    "        data[\"TEMP_Time\"] = data['timestamp'].iloc[:] - startTime\n",
    "    except Exception as e: \n",
    "        print(\"error_3__1 : converting time\")\n",
    "        print(e)\n",
    "        \n",
    "    print(\"TEST1===============================================\")\n",
    "    print(data[\"TEMP_Time\"].iloc[-1])\n",
    "    print(type(data[\"TEMP_Time\"].iloc[-1]))\n",
    "    print(\"TEST1===END=========================================\")\n",
    "    \n",
    "    try:\n",
    "        data[\"Experiment_duration\"] = data[\"TEMP_Time\"]\n",
    "        #data[\"Experiment_duration\"] = data[\"TEMP_Time\"].dt.strftime('%M:%S.%f')\n",
    "        #error 'TimedeltaProperties' object has no attribute 'strftime'\n",
    "        #data[\"Experiment_duration\"] = pd.to_datetime(data['timestamp'] - startTime).dt.strftime('%M:%S.%f')\n",
    "        # problem is that I want it nicely formated but its a delta time object\n",
    "        \"\"\"\n",
    "        I could write a function to process that with microsseconds similar to divmod below:\n",
    "        s = 13420\n",
    "        hours, remainder = divmod(s, 3600)\n",
    "        minutes, seconds = divmod(remainder, 60)\n",
    "        print '{:02}:{:02}:{:02}'.format(int(hours), int(minutes), int(seconds))\n",
    "        # result: 03:43:40\"\"\"\n",
    "    except Exception as e: \n",
    "        print(\"error_3__2 : converting time\")\n",
    "        print(e)        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f286c764",
   "metadata": {},
   "source": [
    "## 4 : Pull demographic and questionaire data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85f6b6c",
   "metadata": {},
   "source": [
    "***!!! This code runs to early should be the last one to be merged if merged at all? maybe use a second DF and store as a second sheet??***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6103ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullDemoQdata(ID,GR):\n",
    "    filePath = 'RawData/Qualitative data and participant demographics/' + 'id_' + str(ID) + '-Gr_' + str(GR) + '.xlsx'\n",
    "    dataPath = os.path.join(os.getcwd(), filePath)\n",
    "    #EXPERIMENT_1_DATA\\MSSQandIPQ\n",
    "    data = pd.read_excel(dataPath, sheet_name='Basic', header = None)\n",
    "    allDemoData = {\n",
    "        'ID' : data.iloc[0,1],\n",
    "        'Group' : data.iloc[1,1],\n",
    "        'Gender' : data.iloc[2,1],\n",
    "        'Age' : data.iloc[3,1],\n",
    "        'MSSQ' : data.iloc[5,1],\n",
    "        'IPQ-Avrg' : data.iloc[8,1],\n",
    "        'IPQ-G1' : data.iloc[9,1],\n",
    "        'IPQ-SP' : data.iloc[10,1],\n",
    "        'IPQ-INV' : data.iloc[11,1],\n",
    "        'IPQ-REAL' : data.iloc[12,1]\n",
    "    }\n",
    "    \n",
    "    demoData = pd.DataFrame.from_dict(allDemoData, orient='index')\n",
    "    \n",
    "    return demoData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccc73df",
   "metadata": {},
   "source": [
    "## 5 : Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6b450e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveAllData(all_data, ID, GR, demographicData, save = True):\n",
    "    save = saveGlobal\n",
    "    # print(all_data.columns)\n",
    "    \n",
    "    data_to_save = pd.DataFrame()\n",
    "\n",
    "    #data_to_save[\"Raw_Time[HH:MM:ss.us]\"] = all_data[\"Experiment_duration\"]\n",
    "    # LOOKS LIKE .copy() didn't fixed the problem\n",
    "    data_to_save[\"Frame_count\"] = all_data[\"Frame_count\"].copy()\n",
    "\n",
    "    data_to_save[\"Phase\"] = all_data[\"Phase\"].copy()\n",
    "    data_to_save[\"Time_in_Phase[s]\"] = all_data[\"Time_in_Phase\"].copy()\n",
    "    data_to_save[\"Real_Time_in_Phase[s]\"] = all_data[\"Real_Time_in_Phase\"].copy()\n",
    "    data_to_save[\"Trial_Number[int]\"] = all_data[\"Trial_Number\"].copy()\n",
    "    data_to_save[\"Trial_Time[s]\"] = all_data[\"Trial_Time\"].copy()\n",
    "    data_to_save[\"Period_Type\"] = all_data[\"Period_Type\"].copy()\n",
    "    data_to_save[\"Time_in_Period[s]\"] = all_data[\"Time_in_Period\"].copy()\n",
    "    data_to_save[\"Fidelity\"] = all_data[\"Fidelity\"].copy()\n",
    "    data_to_save[\"Model\"] = all_data[\"Model\"].copy()\n",
    "    data_to_save[\"Saturation\"] = all_data[\"Saturation\"].copy()\n",
    "    data_to_save[\"FOV[deg]\"] = all_data[\"FOV\"].copy()\n",
    "    data_to_save[\"Resolution[px]\"] = all_data[\"Resolution\"].copy()\n",
    "    data_to_save[\"Pixel_count[px]\"] = all_data[\"Pixel_count\"].copy()\n",
    "    data_to_save[\"Thread_sleep[ms]\"] = all_data[\"Thread_sleep\"].copy()\n",
    "    data_to_save[\"FPS\"] = all_data[\"FPS_real\"].copy()\n",
    "    data_to_save[\"Audio_clip\"] = all_data[\"Audio_clip\"].copy()\n",
    "    '''    \n",
    "    data_to_save[\"Audio_clip\"] = all_data[\"Audio_clip\"]\n",
    "    audio clip data droped as it didn't contain any interesting information\n",
    "    audio clip Audio_1 was played during the break phase which is removed from the analysed dataset \n",
    "    audio clip Audio_2 was played at the end of the experiment after data capture was finished\n",
    "    ...\n",
    "    WAS ADDED BACK IN!!!!!!!!!!!\n",
    "    '''\n",
    "\n",
    "    data_to_save[\"Skin_Conductance[uS]\"] = all_data[\"Skin_Conductance\"].copy()\n",
    "    data_to_save[\"Skin_Conductance_DIFF(1)\"] = all_data[\"Skin_Conductance_DIFF(1)\"].copy()\n",
    "    \n",
    "    data_to_save['Skin_Conductance_SMA(1s)'] = all_data['Skin_Conductance_SMA(1.0s)'].copy()\n",
    "    data_to_save['Skin_Conductance_SMA(3s)'] = all_data['Skin_Conductance_SMA(3.0s)'].copy()\n",
    "    data_to_save['Skin_Conductance_SMA(60s)'] = all_data['Skin_Conductance_SMA(60.0s)'].copy()\n",
    "    data_to_save['Skin_Conductance_EWMA(0.01)'] = all_data['Skin_Conductance_EWMA(0.01)'].copy()\n",
    "    data_to_save['Skin_Conductance_EWMA(0.001)'] = all_data['Skin_Conductance_EWMA(0.001)'].copy()\n",
    "    data_to_save['Skin_Conductance_EWMA(0.0001)'] = all_data['Skin_Conductance_EWMA(0.0001)'].copy()\n",
    "\n",
    "    data_to_save[\"Skin_Conductance_STD\"] = all_data[\"Skin_Conductance_STD\"].copy()\n",
    "    data_to_save[\"Skin_Conductance_STD_DIFF(1)\"] = all_data[\"Skin_Conductance_STD_DIFF(1)\"].copy()\n",
    "    \n",
    "    data_to_save['Skin_Conductance_STD_SMA(1s)'] = all_data['Skin_Conductance_STD_SMA(1.0s)'].copy()\n",
    "    data_to_save['Skin_Conductance_STD_SMA(3s)'] = all_data['Skin_Conductance_STD_SMA(3.0s)'].copy()\n",
    "    data_to_save['Skin_Conductance_STD_SMA(60s)'] = all_data['Skin_Conductance_STD_SMA(60.0s)'].copy()\n",
    "    data_to_save['Skin_Conductance_STD_EWMA(0.01)'] = all_data['Skin_Conductance_STD_EWMA(0.01)'].copy()\n",
    "    data_to_save['Skin_Conductance_STD_EWMA(0.001)'] = all_data['Skin_Conductance_STD_EWMA(0.001)'].copy()\n",
    "    data_to_save['Skin_Conductance_STD_EWMA(0.0001)'] = all_data['Skin_Conductance_STD_EWMA(0.0001)'].copy()\n",
    "    \n",
    "    data_to_save[\"Skin_Conductance_NORM\"] = all_data[\"Skin_Conductance_NORM\"].copy()\n",
    "    data_to_save[\"Skin_Conductance_NORM_DIFF(1)\"] = all_data[\"Skin_Conductance_NORM_DIFF(1)\"].copy()\n",
    "    \n",
    "    data_to_save[\"Skin_Conductance_NORM_SMA(1s)\"] = all_data[\"Skin_Conductance_NORM_SMA(1.0s)\"].copy()\n",
    "    data_to_save[\"Skin_Conductance_NORM_SMA(3s)\"] = all_data[\"Skin_Conductance_NORM_SMA(3.0s)\"].copy()\n",
    "    data_to_save['Skin_Conductance_NORM_SMA(60s)'] = all_data['Skin_Conductance_NORM_SMA(60.0s)'].copy()\n",
    "    data_to_save['Skin_Conductance_NORM_EWMA(0.01)'] = all_data['Skin_Conductance_NORM_EWMA(0.01)'].copy()\n",
    "    data_to_save['Skin_Conductance_NORM_EWMA(0.001)'] = all_data['Skin_Conductance_NORM_EWMA(0.001)'].copy()\n",
    "    data_to_save['Skin_Conductance_NORM_EWMA(0.0001)'] = all_data['Skin_Conductance_NORM_EWMA(0.0001)'].copy()\n",
    "    \n",
    "    data_to_save[\"Skin_Resistance[kOhms]\"] = all_data[\"Skin_Resistance\"].copy()\n",
    "    data_to_save[\"Skin_Resistance_DIFF(1)\"] = all_data[\"Skin_Resistance_DIFF(1)\"].copy()\n",
    "    \n",
    "    data_to_save[\"Skin_Resistance_SMA(1s)\"] = all_data[\"Skin_Resistance_SMA(1.0s)\"].copy()\n",
    "    data_to_save[\"Skin_Resistance_SMA(3s)\"] = all_data[\"Skin_Resistance_SMA(3.0s)\"].copy()\n",
    "    data_to_save[\"Skin_Resistance_SMA(60s)\"] = all_data[\"Skin_Resistance_SMA(60.0s)\"].copy()\n",
    "    data_to_save['Skin_Resistance_EWMA(0.01)'] = all_data['Skin_Resistance_EWMA(0.01)'].copy()\n",
    "    data_to_save['Skin_Resistance_EWMA(0.001)'] = all_data['Skin_Resistance_EWMA(0.001)'].copy()\n",
    "    data_to_save['Skin_Resistance_EWMA(0.0001)'] = all_data['Skin_Resistance_EWMA(0.0001)'].copy()\n",
    "    \n",
    "    data_to_save[\"Skin_Resistance_STD\"] = all_data[\"Skin_Resistance_STD\"].copy()\n",
    "    data_to_save[\"Skin_Resistance_STD_DIFF(1)\"] = all_data[\"Skin_Resistance_STD_DIFF(1)\"].copy()\n",
    "    \n",
    "    data_to_save[\"Skin_Resistance_STD_SMA(1s)\"] = all_data[\"Skin_Resistance_STD_SMA(1.0s)\"].copy()   \n",
    "    data_to_save[\"Skin_Resistance_STD_SMA(3s)\"] = all_data[\"Skin_Resistance_STD_SMA(3.0s)\"].copy()  \n",
    "    data_to_save[\"Skin_Resistance_STD_SMA(60s)\"] = all_data[\"Skin_Resistance_STD_SMA(60.0s)\"].copy()\n",
    "    data_to_save['Skin_Resistance_STD_EWMA(0.01)'] = all_data['Skin_Resistance_STD_EWMA(0.01)'].copy()\n",
    "    data_to_save['Skin_Resistance_STD_EWMA(0.001)'] = all_data['Skin_Resistance_STD_EWMA(0.001)'].copy()\n",
    "    data_to_save['Skin_Resistance_STD_EWMA(0.0001)'] = all_data['Skin_Resistance_STD_EWMA(0.0001)'].copy()\n",
    "    \n",
    "    data_to_save[\"Skin_Resistance_NORM\"] = all_data[\"Skin_Resistance_NORM\"].copy()\n",
    "    data_to_save[\"Skin_Resistance_NORM_DIFF(1)\"] = all_data[\"Skin_Resistance_NORM_DIFF(1)\"].copy()\n",
    "    \n",
    "    data_to_save[\"Skin_Resistance_NORM_SMA(1s)\"] = all_data[\"Skin_Resistance_NORM_SMA(1.0s)\"].copy()\n",
    "    data_to_save[\"Skin_Resistance_NORM_SMA(3s)\"] = all_data[\"Skin_Resistance_NORM_SMA(3.0s)\"].copy()\n",
    "    data_to_save[\"Skin_Resistance_NORM_SMA(60s)\"] = all_data[\"Skin_Resistance_NORM_SMA(60.0s)\"].copy()\n",
    "    data_to_save['Skin_Resistance_NORM_EWMA(0.01)'] = all_data['Skin_Resistance_NORM_EWMA(0.01)'].copy()\n",
    "    data_to_save['Skin_Resistance_NORM_EWMA(0.001)'] = all_data['Skin_Resistance_NORM_EWMA(0.001)'].copy()\n",
    "    data_to_save['Skin_Resistance_NORM_EWMA(0.0001)'] = all_data['Skin_Resistance_NORM_EWMA(0.0001)'].copy()\n",
    "    \n",
    "    data_to_save[\"Skin_Conductance_Phasic_CVX\"] = all_data[\"Skin_Conductance_Phasic_CVX\"].copy()\n",
    "    data_to_save[\"Skin_Conductance_Phasic_CVX_SMA(1s)\"] = all_data[\"Skin_Conductance_Phasic_CVX_SMA(1.0s)\"].copy()\n",
    "    data_to_save[\"Skin_Conductance_Phasic_CVX_SMA(3s)\"] = all_data[\"Skin_Conductance_Phasic_CVX_SMA(3.0s)\"].copy()\n",
    "    data_to_save[\"Skin_Conductance_Phasic_CVX_SMA(60s)\"] = all_data[\"Skin_Conductance_Phasic_CVX_SMA(60.0s)\"].copy()\n",
    "    data_to_save['Skin_Conductance_Phasic_CVX_EWMA(0.01)'] = all_data['Skin_Conductance_Phasic_CVX_EWMA(0.01)'].copy()\n",
    "    data_to_save['Skin_Conductance_Phasic_CVX_EWMA(0.001)'] = all_data['Skin_Conductance_Phasic_CVX_EWMA(0.001)'].copy()\n",
    "    data_to_save['Skin_Conductance_Phasic_CVX_EWMA(0.0001)'] = all_data['Skin_Conductance_Phasic_CVX_EWMA(0.0001)'].copy()\n",
    "    \n",
    "    data_to_save[\"Skin_Conductance_Tonic_CVX\"] = all_data[\"Skin_Conductance_Tonic_CVX\"].copy()\n",
    "    \n",
    "    data_to_save[\"HR_CONSENSYS[BPM]\"] = all_data[\"PPG_to_HR_CONSENSYS\"].copy()\n",
    "    \n",
    "    data_to_save[\"HR_CONSENSYS_SMA(1s)\"] = all_data[\"PPG_to_HR_CONSENSYS_SMA(1.0s)\"].copy()\n",
    "    data_to_save[\"HR_CONSENSYS_SMA(3s)\"] = all_data[\"PPG_to_HR_CONSENSYS_SMA(3.0s)\"].copy()\n",
    "    data_to_save[\"HR_CONSENSYS_SMA(60s)\"] = all_data[\"PPG_to_HR_CONSENSYS_SMA(60.0s)\"].copy()\n",
    "    data_to_save['HR_CONSENSYS_EWMA(0.01)'] = all_data['PPG_to_HR_CONSENSYS_EWMA(0.01)'].copy()\n",
    "    data_to_save['HR_CONSENSYS_EWMA(0.001)'] = all_data['PPG_to_HR_CONSENSYS_EWMA(0.001)'].copy()\n",
    "    data_to_save['HR_CONSENSYS_EWMA(0.0001)'] = all_data['PPG_to_HR_CONSENSYS_EWMA(0.0001)'].copy()\n",
    "    \n",
    "    \n",
    "    data_to_save[\"HR_CONSENSYS_interpolated[BPM]\"] = all_data[\"HR_CONSENSYS_interpolated\"].copy()\n",
    "    data_to_save[\"HR_CONSENSYS_interpolated_DIFF(1)\"] = all_data[\"HR_CONSENSYS_interpolated_DIFF(1)\"].copy()\n",
    "    \n",
    "    data_to_save[\"HR_CONSENSYS_interpolated_SMA(1s)\"] = all_data[\"HR_CONSENSYS_interpolated_SMA(1.0s)\"].copy()\n",
    "    data_to_save[\"HR_CONSENSYS_interpolated_SMA(3s)\"] = all_data[\"HR_CONSENSYS_interpolated_SMA(3.0s)\"].copy()\n",
    "    data_to_save[\"HR_CONSENSYS_interpolated_SMA(60s)\"] = all_data[\"HR_CONSENSYS_interpolated_SMA(60.0s)\"].copy()\n",
    "    data_to_save['HR_CONSENSYS_interpolated_EWMA(0.01)'] = all_data['HR_CONSENSYS_interpolated_EWMA(0.01)'].copy()\n",
    "    data_to_save['HR_CONSENSYS_interpolated_EWMA(0.001)'] = all_data['HR_CONSENSYS_interpolated_EWMA(0.001)'].copy()\n",
    "    data_to_save['HR_CONSENSYS_interpolated_EWMA(0.0001)'] = all_data['HR_CONSENSYS_interpolated_EWMA(0.0001)'].copy()\n",
    "    \n",
    "    data_to_save[\"HR_CONSENSYS_interpolated_STD\"] = all_data[\"HR_CONSENSYS_STD\"].copy()\n",
    "    data_to_save[\"HR_CONSENSYS_interpolated_STD_DIFF(1)\"] = all_data[\"HR_CONSENSYS_STD_DIFF(1)\"].copy()\n",
    "    \n",
    "    data_to_save[\"HR_CONSENSYS_interpolated_STD_SMA(1s)\"] = all_data[\"HR_CONSENSYS_STD_SMA(1.0s)\"].copy()\n",
    "    data_to_save[\"HR_CONSENSYS_interpolated_STD_SMA(3s)\"] = all_data[\"HR_CONSENSYS_STD_SMA(3.0s)\"].copy()\n",
    "    data_to_save[\"HR_CONSENSYS_interpolated_STD_SMA(60s)\"] = all_data[\"HR_CONSENSYS_STD_SMA(60.0s)\"].copy()\n",
    "    data_to_save['HR_CONSENSYS_interpolated_STD_EWMA(0.01)'] = all_data['HR_CONSENSYS_STD_EWMA(0.01)'].copy()\n",
    "    data_to_save['HR_CONSENSYS_interpolated_STD_EWMA(0.001)'] = all_data['HR_CONSENSYS_STD_EWMA(0.001)'].copy()\n",
    "    data_to_save['HR_CONSENSYS_interpolated_STD_EWMA(0.0001)'] = all_data['HR_CONSENSYS_STD_EWMA(0.0001)'].copy()\n",
    "    \n",
    "    data_to_save[\"HR_CONSENSYS_interpolated_NORM\"] = all_data[\"HR_CONSENSYS_NORM\"].copy()\n",
    "    data_to_save[\"HR_CONSENSYS_interpolated_NORM_DIFF(1)\"] = all_data[\"HR_CONSENSYS_NORM_DIFF(1)\"].copy()\n",
    "    \n",
    "    data_to_save[\"HR_CONSENSYS_interpolated_NORM_SMA(1s)\"] = all_data[\"HR_CONSENSYS_NORM_SMA(1.0s)\"].copy()\n",
    "    data_to_save[\"HR_CONSENSYS_interpolated_NORM_SMA(3s)\"] = all_data[\"HR_CONSENSYS_NORM_SMA(3.0s)\"].copy()\n",
    "    data_to_save[\"HR_CONSENSYS_interpolated_NORM_SMA(60s)\"] = all_data[\"HR_CONSENSYS_NORM_SMA(60.0s)\"].copy() \n",
    "    data_to_save['HR_CONSENSYS_interpolated_NORM_EWMA(0.01)'] = all_data['HR_CONSENSYS_NORM_EWMA(0.01)'].copy()\n",
    "    data_to_save['HR_CONSENSYS_interpolated_NORM_EWMA(0.001)'] = all_data['HR_CONSENSYS_NORM_EWMA(0.001)'].copy()\n",
    "    data_to_save['HR_CONSENSYS_interpolated_NORM_EWMA(0.0001)'] = all_data['HR_CONSENSYS_NORM_EWMA(0.0001)'].copy()\n",
    "    \n",
    "    data_to_save[\"IBI_CONSENSYS[ms]\"] = all_data['PPG_IBI'].copy()\n",
    "    data_to_save[\"IBI_CONSENSYS_interpolated[ms]\"] = all_data['IBI_CONSENSYS_interpolated'].copy()\n",
    "\n",
    "    data_to_save[\"HR_HeartPy[BPM]\"] = all_data[\"bpm\"].copy()\n",
    "    data_to_save[\"HR_HeartPy_SMA(1s)\"] = all_data['bpm_SMA(1.0s)'].copy()\n",
    "    data_to_save[\"HR_HeartPy_SMA(3s)\"] = all_data[\"bpm_SMA(3.0s)\"].copy()\n",
    "    data_to_save[\"HR_HeartPy_SMA(60s)\"] = all_data['bpm_SMA(60.0s)'].copy()\n",
    "    data_to_save['HR_HeartPy_EWMA(0.01)'] = all_data['bpm_EWMA(0.01)'].copy()\n",
    "    data_to_save['HR_HeartPy_EWMA(0.001)'] = all_data['bpm_EWMA(0.001)'].copy()\n",
    "    data_to_save['HR_HeartPy_EWMA(0.0001)'] = all_data['bpm_EWMA(0.0001)'].copy()\n",
    "    \n",
    "    data_to_save[\"HR_HeartPy_interpolated[BPM]\"] = all_data[\"HR_HeartPy_interpolated\"].copy()\n",
    "    data_to_save[\"HR_HeartPy_interpolated_DIFF(1)\"] = all_data[\"HR_HeartPy_interpolated_DIFF(1)\"].copy()\n",
    "    \n",
    "    data_to_save[\"HR_HeartPy_interpolated_SMA(1s)\"] = all_data[\"HR_HeartPy_interpolated_SMA(1.0s)\"].copy()\n",
    "    data_to_save[\"HR_HeartPy_interpolated_SMA(3s)\"] = all_data[\"HR_HeartPy_interpolated_SMA(3.0s)\"].copy()\n",
    "    data_to_save[\"HR_HeartPy_interpolated_SMA(60s)\"] = all_data['HR_HeartPy_interpolated_SMA(60.0s)'].copy()\n",
    "    data_to_save['HR_HeartPy_interpolated_EWMA(0.01)'] = all_data['HR_HeartPy_interpolated_EWMA(0.01)'].copy()\n",
    "    data_to_save['HR_HeartPy_interpolated_EWMA(0.001)'] = all_data['HR_HeartPy_interpolated_EWMA(0.001)'].copy()\n",
    "    data_to_save['HR_HeartPy_interpolated_EWMA(0.0001)'] = all_data['HR_HeartPy_interpolated_EWMA(0.0001)'].copy()\n",
    "    \n",
    "    data_to_save[\"HR_HeartPy_interpolated_STD\"] = all_data[\"HR_HP_STD\"].copy()\n",
    "    data_to_save[\"HR_HeartPy_interpolated_STD_DIFF(1)\"] = all_data[\"HR_HP_STD_DIFF(1)\"].copy()\n",
    "    \n",
    "    data_to_save[\"HR_HeartPy_interpolated_STD_SMA(1s)\"] = all_data[\"HR_HP_STD_SMA(1.0s)\"].copy()\n",
    "    data_to_save[\"HR_HeartPy_interpolated_STD_SMA(3s)\"] = all_data[\"HR_HP_STD_SMA(3.0s)\"].copy()\n",
    "    data_to_save[\"HR_HeartPy_interpolated_STD_SMA(60s)\"] = all_data['HR_HP_STD_SMA(60.0s)'].copy()\n",
    "    data_to_save['HR_HeartPy_interpolated_STD_EWMA(0.01)'] = all_data['HR_HP_STD_EWMA(0.01)'].copy()\n",
    "    data_to_save['HR_HeartPy_interpolated_STD_EWMA(0.001)'] = all_data['HR_HP_STD_EWMA(0.001)'].copy()\n",
    "    data_to_save['HR_HeartPy_interpolated_STD_EWMA(0.0001)'] = all_data['HR_HP_STD_EWMA(0.0001)'].copy()\n",
    "    \n",
    "    data_to_save[\"HR_HeartPy_interpolated_NORM\"] = all_data[\"HR_HP_NORM\"].copy()\n",
    "    data_to_save[\"HR_HeartPy_interpolated_NORM_DIFF(1)\"] = all_data[\"HR_HP_NORM_DIFF(1)\"].copy()\n",
    "    \n",
    "    data_to_save[\"HR_HeartPy_interpolated_NORM_SMA(1s)\"] = all_data[\"HR_HP_NORM_SMA(1.0s)\"].copy()\n",
    "    data_to_save[\"HR_HeartPy_interpolated_NORM_SMA(3s)\"] = all_data[\"HR_HP_NORM_SMA(3.0s)\"].copy()\n",
    "    data_to_save[\"HR_HeartPy_interpolated_NORM_SMA(60s)\"] = all_data['HR_HP_NORM_SMA(60.0s)'].copy()\n",
    "    data_to_save['HR_HeartPy_interpolated_NORM_EWMA(0.01)'] = all_data['HR_HP_NORM_EWMA(0.01)'].copy()\n",
    "    data_to_save['HR_HeartPy_interpolated_NORM_EWMA(0.001)'] = all_data['HR_HP_NORM_EWMA(0.001)'].copy()\n",
    "    data_to_save['HR_HeartPy_interpolated_NORM_EWMA(0.0001)'] = all_data['HR_HP_NORM_EWMA(0.0001)'].copy()  \n",
    "    \n",
    "    data_to_save[\"IBI_HeartPy[ms]\"] = all_data[\"ibi\"].copy()\n",
    "    data_to_save[\"IBI_HeartPy_interpolated[ms]\"] = all_data[\"IBI_HeartPy_interpolated\"].copy()\n",
    "    \n",
    "    data_to_save[\"SDNN_HeartPy[ms]\"] = all_data[\"sdnn\"].copy()\n",
    "    data_to_save[\"SDNN_HeartPy_interpolated[ms]\"] = all_data[\"SDNN_HeartPy_interpolated\"].copy()\n",
    "    \n",
    "    data_to_save[\"SDSD_HeartPy[ms]\"] = all_data[\"sdsd\"].copy()\n",
    "    data_to_save[\"SDSD_HeartPy_interpolated[ms]\"] = all_data[\"SDSD_HeartPy_interpolated\"].copy()\n",
    "    \n",
    "    data_to_save[\"RMSSD_HeartPy[ms]\"] = all_data[\"rmssd\"]\n",
    "    data_to_save[\"RMSSD_HeartPy_interpolated[ms]\"] = all_data[\"RMSSD_HeartPy_interpolated\"]\n",
    "    \n",
    "    data_to_save[\"pNN20_HeartPy[%]\"] = all_data[\"pnn20\"].copy()\n",
    "    data_to_save[\"pNN20_HeartPy_interpolated[%]\"] = all_data[\"pNN20_HeartPy_interpolated\"].copy()\n",
    "    \n",
    "    data_to_save[\"pNN50_HeartPy[%]\"] = all_data[\"pnn50\"].copy()\n",
    "    data_to_save[\"pNN50_HeartPy_interpolated[%]\"] = all_data[\"pNN50_HeartPy_interpolated\"].copy()\n",
    "    \n",
    "    data_to_save[\"Pressure[kPa]\"] = all_data[\"Pressure\"].copy()\n",
    "    data_to_save[\"Temperature[C]\"] = all_data[\"Temperature\"].copy()\n",
    "    \n",
    "    \"\"\"OLD\n",
    "    #data_to_save[\"Raw_Time[HH:MM:ss.us]\"] = all_data[\"Experiment_duration\"]\n",
    "    data_to_save[\"Frame_count\"] = all_data[\"Frame_count\"]\n",
    "\n",
    "    data_to_save[\"Phase\"] = all_data[\"Phase\"] \n",
    "    data_to_save[\"Time_in_Phase[s]\"] = all_data[\"Time_in_Phase\"]\n",
    "    data_to_save[\"Real_Time_in_Phase[s]\"] = all_data[\"Real_Time_in_Phase\"]\n",
    "    data_to_save[\"Trial_Number[int]\"] = all_data[\"Trial_Number\"]\n",
    "    data_to_save[\"Trial_Time[s]\"] = all_data[\"Trial_Time\"]\n",
    "    data_to_save[\"Period_Type\"] = all_data[\"Period_Type\"]\n",
    "    data_to_save[\"Time_in_Period[s]\"] = all_data[\"Time_in_Period\"]\n",
    "    data_to_save[\"Fidelity\"] = all_data[\"Fidelity\"]\n",
    "    data_to_save[\"Model\"] = all_data[\"Model\"]\n",
    "    data_to_save[\"Saturation\"] = all_data[\"Saturation\"]\n",
    "    data_to_save[\"FOV[deg]\"] = all_data[\"FOV\"]\n",
    "    data_to_save[\"Resolution[px]\"] = all_data[\"Resolution\"]\n",
    "    data_to_save[\"Pixel_count[px]\"] = all_data[\"Pixel_count\"]\n",
    "    data_to_save[\"Thread_sleep[ms]\"] = all_data[\"Thread_sleep\"]\n",
    "    data_to_save[\"FPS\"] = all_data[\"FPS_real\"]\n",
    "    data_to_save[\"Audio_clip\"] = all_data[\"Audio_clip\"]\n",
    "    '''    \n",
    "    data_to_save[\"Audio_clip\"] = all_data[\"Audio_clip\"]\n",
    "    audio clip data droped as it didn't contain any interesting information\n",
    "    audio clip Audio_1 was played during the break phase which is removed from the analysed dataset \n",
    "    audio clip Audio_2 was played at the end of the experiment after data capture was finished\n",
    "    WAS ADDED BACK IN!!!!!!!!!!!\n",
    "    '''\n",
    "\n",
    "    data_to_save[\"Skin_Conductance[uS]\"] = all_data[\"Skin_Conductance\"]\n",
    "    data_to_save[\"Skin_Conductance_DIFF(1)\"] = all_data[\"Skin_Conductance_DIFF(1)\"]\n",
    "    \n",
    "    data_to_save['Skin_Conductance_SMA(1s)'] = all_data['Skin_Conductance_SMA(1.0s)']\n",
    "    data_to_save['Skin_Conductance_SMA(3s)'] = all_data['Skin_Conductance_SMA(3.0s)']\n",
    "    data_to_save['Skin_Conductance_SMA(60s)'] = all_data['Skin_Conductance_SMA(60.0s)']\n",
    "    data_to_save['Skin_Conductance_EWMA(0.01)'] = all_data['Skin_Conductance_EWMA(0.01)']\n",
    "    data_to_save['Skin_Conductance_EWMA(0.001)'] = all_data['Skin_Conductance_EWMA(0.001)']\n",
    "    data_to_save['Skin_Conductance_EWMA(0.0001)'] = all_data['Skin_Conductance_EWMA(0.0001)']\n",
    "\n",
    "    data_to_save[\"Skin_Conductance_STD\"] = all_data[\"Skin_Conductance_STD\"]\n",
    "    data_to_save[\"Skin_Conductance_STD_DIFF(1)\"] = all_data[\"Skin_Conductance_STD_DIFF(1)\"]\n",
    "    \n",
    "    data_to_save['Skin_Conductance_STD_SMA(1s)'] = all_data['Skin_Conductance_STD_SMA(1.0s)']\n",
    "    data_to_save['Skin_Conductance_STD_SMA(3s)'] = all_data['Skin_Conductance_STD_SMA(3.0s)']\n",
    "    data_to_save['Skin_Conductance_STD_SMA(60s)'] = all_data['Skin_Conductance_STD_SMA(60.0s)']\n",
    "    data_to_save['Skin_Conductance_STD_EWMA(0.01)'] = all_data['Skin_Conductance_STD_EWMA(0.01)']\n",
    "    data_to_save['Skin_Conductance_STD_EWMA(0.001)'] = all_data['Skin_Conductance_STD_EWMA(0.001)']\n",
    "    data_to_save['Skin_Conductance_STD_EWMA(0.0001)'] = all_data['Skin_Conductance_STD_EWMA(0.0001)']\n",
    "    \n",
    "    data_to_save[\"Skin_Conductance_NORM\"] = all_data[\"Skin_Conductance_NORM\"]\n",
    "    data_to_save[\"Skin_Conductance_NORM_DIFF(1)\"] = all_data[\"Skin_Conductance_NORM_DIFF(1)\"]\n",
    "    \n",
    "    data_to_save[\"Skin_Conductance_NORM_SMA(1s)\"] = all_data[\"Skin_Conductance_NORM_SMA(1.0s)\"]\n",
    "    data_to_save[\"Skin_Conductance_NORM_SMA(3s)\"] = all_data[\"Skin_Conductance_NORM_SMA(3.0s)\"]\n",
    "    data_to_save['Skin_Conductance_NORM_SMA(60s)'] = all_data['Skin_Conductance_NORM_SMA(60.0s)']\n",
    "    data_to_save['Skin_Conductance_NORM_EWMA(0.01)'] = all_data['Skin_Conductance_NORM_EWMA(0.01)']\n",
    "    data_to_save['Skin_Conductance_NORM_EWMA(0.001)'] = all_data['Skin_Conductance_NORM_EWMA(0.001)']\n",
    "    data_to_save['Skin_Conductance_NORM_EWMA(0.0001)'] = all_data['Skin_Conductance_NORM_EWMA(0.0001)']\n",
    "    \n",
    "    data_to_save[\"Skin_Resistance[kOhms]\"] = all_data[\"Skin_Resistance\"]\n",
    "    data_to_save[\"Skin_Resistance_DIFF(1)\"] = all_data[\"Skin_Resistance_DIFF(1)\"]\n",
    "    \n",
    "    data_to_save[\"Skin_Resistance_SMA(1s)\"] = all_data[\"Skin_Resistance_SMA(1.0s)\"]\n",
    "    data_to_save[\"Skin_Resistance_SMA(3s)\"] = all_data[\"Skin_Resistance_SMA(3.0s)\"]\n",
    "    data_to_save[\"Skin_Resistance_SMA(60s)\"] = all_data[\"Skin_Resistance_SMA(60.0s)\"]\n",
    "    data_to_save['Skin_Resistance_EWMA(0.01)'] = all_data['Skin_Resistance_EWMA(0.01)']\n",
    "    data_to_save['Skin_Resistance_EWMA(0.001)'] = all_data['Skin_Resistance_EWMA(0.001)']\n",
    "    data_to_save['Skin_Resistance_EWMA(0.0001)'] = all_data['Skin_Resistance_EWMA(0.0001)']\n",
    "    \n",
    "    data_to_save[\"Skin_Resistance_STD\"] = all_data[\"Skin_Resistance_STD\"]\n",
    "    data_to_save[\"Skin_Resistance_STD_DIFF(1)\"] = all_data[\"Skin_Resistance_STD_DIFF(1)\"]\n",
    "    \n",
    "    data_to_save[\"Skin_Resistance_STD_SMA(1s)\"] = all_data[\"Skin_Resistance_STD_SMA(1.0s)\"]    \n",
    "    data_to_save[\"Skin_Resistance_STD_SMA(3s)\"] = all_data[\"Skin_Resistance_STD_SMA(3.0s)\"]   \n",
    "    data_to_save[\"Skin_Resistance_STD_SMA(60s)\"] = all_data[\"Skin_Resistance_STD_SMA(60.0s)\"]\n",
    "    data_to_save['Skin_Resistance_STD_EWMA(0.01)'] = all_data['Skin_Resistance_STD_EWMA(0.01)']\n",
    "    data_to_save['Skin_Resistance_STD_EWMA(0.001)'] = all_data['Skin_Resistance_STD_EWMA(0.001)']\n",
    "    data_to_save['Skin_Resistance_STD_EWMA(0.0001)'] = all_data['Skin_Resistance_STD_EWMA(0.0001)']\n",
    "    \n",
    "    data_to_save[\"Skin_Resistance_NORM\"] = all_data[\"Skin_Resistance_NORM\"]\n",
    "    data_to_save[\"Skin_Resistance_NORM_DIFF(1)\"] = all_data[\"Skin_Resistance_NORM_DIFF(1)\"]\n",
    "    \n",
    "    data_to_save[\"Skin_Resistance_NORM_SMA(1s)\"] = all_data[\"Skin_Resistance_NORM_SMA(1.0s)\"]    \n",
    "    data_to_save[\"Skin_Resistance_NORM_SMA(3s)\"] = all_data[\"Skin_Resistance_NORM_SMA(3.0s)\"]    \n",
    "    data_to_save[\"Skin_Resistance_NORM_SMA(60s)\"] = all_data[\"Skin_Resistance_NORM_SMA(60.0s)\"]\n",
    "    data_to_save['Skin_Resistance_NORM_EWMA(0.01)'] = all_data['Skin_Resistance_NORM_EWMA(0.01)']\n",
    "    data_to_save['Skin_Resistance_NORM_EWMA(0.001)'] = all_data['Skin_Resistance_NORM_EWMA(0.001)']\n",
    "    data_to_save['Skin_Resistance_NORM_EWMA(0.0001)'] = all_data['Skin_Resistance_NORM_EWMA(0.0001)']\n",
    "    \n",
    "    data_to_save[\"Skin_Conductance_Phasic_CVX\"] = all_data[\"Skin_Conductance_Phasic_CVX\"]\n",
    "    data_to_save[\"Skin_Conductance_Phasic_CVX_SMA(1s)\"] = all_data[\"Skin_Conductance_Phasic_CVX_SMA(1.0s)\"]\n",
    "    data_to_save[\"Skin_Conductance_Phasic_CVX_SMA(3s)\"] = all_data[\"Skin_Conductance_Phasic_CVX_SMA(3.0s)\"]\n",
    "    data_to_save[\"Skin_Conductance_Phasic_CVX_SMA(60s)\"] = all_data[\"Skin_Conductance_Phasic_CVX_SMA(60.0s)\"]\n",
    "    data_to_save['Skin_Conductance_Phasic_CVX_EWMA(0.01)'] = all_data['Skin_Conductance_Phasic_CVX_EWMA(0.01)']\n",
    "    data_to_save['Skin_Conductance_Phasic_CVX_EWMA(0.001)'] = all_data['Skin_Conductance_Phasic_CVX_EWMA(0.001)']\n",
    "    data_to_save['Skin_Conductance_Phasic_CVX_EWMA(0.0001)'] = all_data['Skin_Conductance_Phasic_CVX_EWMA(0.0001)']\n",
    "    \n",
    "    data_to_save[\"Skin_Conductance_Tonic_CVX\"] = all_data[\"Skin_Conductance_Tonic_CVX\"]\n",
    "    \n",
    "    data_to_save[\"HR_CONSENSYS[BPM]\"] = all_data[\"PPG_to_HR_CONSENSYS\"]\n",
    "    \n",
    "    data_to_save[\"HR_CONSENSYS_SMA(1s)\"] = all_data[\"PPG_to_HR_CONSENSYS_SMA(1.0s)\"]\n",
    "    data_to_save[\"HR_CONSENSYS_SMA(3s)\"] = all_data[\"PPG_to_HR_CONSENSYS_SMA(3.0s)\"]\n",
    "    data_to_save[\"HR_CONSENSYS_SMA(60s)\"] = all_data[\"PPG_to_HR_CONSENSYS_SMA(60.0s)\"]\n",
    "    data_to_save['HR_CONSENSYS_EWMA(0.01)'] = all_data['PPG_to_HR_CONSENSYS_EWMA(0.01)']\n",
    "    data_to_save['HR_CONSENSYS_EWMA(0.001)'] = all_data['PPG_to_HR_CONSENSYS_EWMA(0.001)']\n",
    "    data_to_save['HR_CONSENSYS_EWMA(0.0001)'] = all_data['PPG_to_HR_CONSENSYS_EWMA(0.0001)']\n",
    "    \n",
    "    \n",
    "    data_to_save[\"HR_CONSENSYS_interpolated[BPM]\"] = all_data[\"HR_CONSENSYS_interpolated\"]\n",
    "    data_to_save[\"HR_CONSENSYS_interpolated_DIFF(1)\"] = all_data[\"HR_CONSENSYS_interpolated_DIFF(1)\"]\n",
    "    \n",
    "    data_to_save[\"HR_CONSENSYS_interpolated_SMA(1s)\"] = all_data[\"HR_CONSENSYS_interpolated_SMA(1.0s)\"]\n",
    "    data_to_save[\"HR_CONSENSYS_interpolated_SMA(3s)\"] = all_data[\"HR_CONSENSYS_interpolated_SMA(3.0s)\"]\n",
    "    data_to_save[\"HR_CONSENSYS_interpolated_SMA(60s)\"] = all_data[\"HR_CONSENSYS_interpolated_SMA(60.0s)\"]\n",
    "    data_to_save['HR_CONSENSYS_interpolated_EWMA(0.01)'] = all_data['HR_CONSENSYS_interpolated_EWMA(0.01)']\n",
    "    data_to_save['HR_CONSENSYS_interpolated_EWMA(0.001)'] = all_data['HR_CONSENSYS_interpolated_EWMA(0.001)']\n",
    "    data_to_save['HR_CONSENSYS_interpolated_EWMA(0.0001)'] = all_data['HR_CONSENSYS_interpolated_EWMA(0.0001)']\n",
    "    \n",
    "    data_to_save[\"HR_CONSENSYS_interpolated_STD\"] = all_data[\"HR_CONSENSYS_STD\"]\n",
    "    data_to_save[\"HR_CONSENSYS_interpolated_STD_DIFF(1)\"] = all_data[\"HR_CONSENSYS_STD_DIFF(1)\"]\n",
    "    \n",
    "    data_to_save[\"HR_CONSENSYS_interpolated_STD_SMA(1s)\"] = all_data[\"HR_CONSENSYS_STD_SMA(1.0s)\"]\n",
    "    data_to_save[\"HR_CONSENSYS_interpolated_STD_SMA(3s)\"] = all_data[\"HR_CONSENSYS_STD_SMA(3.0s)\"]\n",
    "    data_to_save[\"HR_CONSENSYS_interpolated_STD_SMA(60s)\"] = all_data[\"HR_CONSENSYS_STD_SMA(60.0s)\"]\n",
    "    data_to_save['HR_CONSENSYS_interpolated_STD_EWMA(0.01)'] = all_data['HR_CONSENSYS_STD_EWMA(0.01)']\n",
    "    data_to_save['HR_CONSENSYS_interpolated_STD_EWMA(0.001)'] = all_data['HR_CONSENSYS_STD_EWMA(0.001)']\n",
    "    data_to_save['HR_CONSENSYS_interpolated_STD_EWMA(0.0001)'] = all_data['HR_CONSENSYS_STD_EWMA(0.0001)']\n",
    "    \n",
    "    data_to_save[\"HR_CONSENSYS_interpolated_NORM\"] = all_data[\"HR_CONSENSYS_NORM\"]\n",
    "    data_to_save[\"HR_CONSENSYS_interpolated_NORM_DIFF(1)\"] = all_data[\"HR_CONSENSYS_NORM_DIFF(1)\"]\n",
    "    \n",
    "    data_to_save[\"HR_CONSENSYS_interpolated_NORM_SMA(1s)\"] = all_data[\"HR_CONSENSYS_NORM_SMA(1.0s)\"]\n",
    "    data_to_save[\"HR_CONSENSYS_interpolated_NORM_SMA(3s)\"] = all_data[\"HR_CONSENSYS_NORM_SMA(3.0s)\"]   \n",
    "    data_to_save[\"HR_CONSENSYS_interpolated_NORM_SMA(60s)\"] = all_data[\"HR_CONSENSYS_NORM_SMA(60.0s)\"] \n",
    "    data_to_save['HR_CONSENSYS_interpolated_NORM_EWMA(0.01)'] = all_data['HR_CONSENSYS_NORM_EWMA(0.01)']\n",
    "    data_to_save['HR_CONSENSYS_interpolated_NORM_EWMA(0.001)'] = all_data['HR_CONSENSYS_NORM_EWMA(0.001)']\n",
    "    data_to_save['HR_CONSENSYS_interpolated_NORM_EWMA(0.0001)'] = all_data['HR_CONSENSYS_NORM_EWMA(0.0001)']\n",
    "    \n",
    "    data_to_save[\"IBI_CONSENSYS[ms]\"] = all_data['PPG_IBI']\n",
    "    data_to_save[\"IBI_CONSENSYS_interpolated[ms]\"] = all_data['IBI_CONSENSYS_interpolated']\n",
    "\n",
    "    data_to_save[\"HR_HeartPy[BPM]\"] = all_data[\"bpm\"]\n",
    "    data_to_save[\"HR_HeartPy_SMA(1s)\"] = all_data['bpm_SMA(1.0s)']\n",
    "    data_to_save[\"HR_HeartPy_SMA(3s)\"] = all_data[\"bpm_SMA(3.0s)\"]\n",
    "    data_to_save[\"HR_HeartPy_SMA(60s)\"] = all_data['bpm_SMA(60.0s)']\n",
    "    data_to_save['HR_HeartPy_EWMA(0.01)'] = all_data['bpm_EWMA(0.01)']\n",
    "    data_to_save['HR_HeartPy_EWMA(0.001)'] = all_data['bpm_EWMA(0.001)']\n",
    "    data_to_save['HR_HeartPy_EWMA(0.0001)'] = all_data['bpm_EWMA(0.0001)']\n",
    "    \n",
    "    data_to_save[\"HR_HeartPy_interpolated[BPM]\"] = all_data[\"HR_HeartPy_interpolated\"]\n",
    "    data_to_save[\"HR_HeartPy_interpolated_DIFF(1)\"] = all_data[\"HR_HeartPy_interpolated_DIFF(1)\"]\n",
    "    \n",
    "    data_to_save[\"HR_HeartPy_interpolated_SMA(1s)\"] = all_data[\"HR_HeartPy_interpolated_SMA(1.0s)\"]\n",
    "    data_to_save[\"HR_HeartPy_interpolated_SMA(3s)\"] = all_data[\"HR_HeartPy_interpolated_SMA(3.0s)\"]\n",
    "    data_to_save[\"HR_HeartPy_interpolated_SMA(60s)\"] = all_data['HR_HeartPy_interpolated_SMA(60.0s)']\n",
    "    data_to_save['HR_HeartPy_interpolated_EWMA(0.01)'] = all_data['HR_HeartPy_interpolated_EWMA(0.01)']\n",
    "    data_to_save['HR_HeartPy_interpolated_EWMA(0.001)'] = all_data['HR_HeartPy_interpolated_EWMA(0.001)']\n",
    "    data_to_save['HR_HeartPy_interpolated_EWMA(0.0001)'] = all_data['HR_HeartPy_interpolated_EWMA(0.0001)']\n",
    "    \n",
    "    data_to_save[\"HR_HeartPy_interpolated_STD\"] = all_data[\"HR_HP_STD\"]\n",
    "    data_to_save[\"HR_HeartPy_interpolated_STD_DIFF(1)\"] = all_data[\"HR_HP_STD_DIFF(1)\"]\n",
    "    \n",
    "    data_to_save[\"HR_HeartPy_interpolated_STD_SMA(1s)\"] = all_data[\"HR_HP_STD_SMA(1.0s)\"]\n",
    "    data_to_save[\"HR_HeartPy_interpolated_STD_SMA(3s)\"] = all_data[\"HR_HP_STD_SMA(3.0s)\"]\n",
    "    data_to_save[\"HR_HeartPy_interpolated_STD_SMA(60s)\"] = all_data['HR_HP_STD_SMA(60.0s)']\n",
    "    data_to_save['HR_HeartPy_interpolated_STD_EWMA(0.01)'] = all_data['HR_HP_STD_EWMA(0.01)']\n",
    "    data_to_save['HR_HeartPy_interpolated_STD_EWMA(0.001)'] = all_data['HR_HP_STD_EWMA(0.001)']\n",
    "    data_to_save['HR_HeartPy_interpolated_STD_EWMA(0.0001)'] = all_data['HR_HP_STD_EWMA(0.0001)']\n",
    "    \n",
    "    data_to_save[\"HR_HeartPy_interpolated_NORM\"] = all_data[\"HR_HP_NORM\"]\n",
    "    data_to_save[\"HR_HeartPy_interpolated_NORM_DIFF(1)\"] = all_data[\"HR_HP_NORM_DIFF(1)\"]\n",
    "    \n",
    "    data_to_save[\"HR_HeartPy_interpolated_NORM_SMA(1s)\"] = all_data[\"HR_HP_NORM_SMA(1.0s)\"]\n",
    "    data_to_save[\"HR_HeartPy_interpolated_NORM_SMA(3s)\"] = all_data[\"HR_HP_NORM_SMA(3.0s)\"]\n",
    "    data_to_save[\"HR_HeartPy_interpolated_NORM_SMA(60s)\"] = all_data['HR_HP_NORM_SMA(60.0s)']\n",
    "    data_to_save['HR_HeartPy_interpolated_NORM_EWMA(0.01)'] = all_data['HR_HP_NORM_EWMA(0.01)']\n",
    "    data_to_save['HR_HeartPy_interpolated_NORM_EWMA(0.001)'] = all_data['HR_HP_NORM_EWMA(0.001)']\n",
    "    data_to_save['HR_HeartPy_interpolated_NORM_EWMA(0.0001)'] = all_data['HR_HP_NORM_EWMA(0.0001)']    \n",
    "    \n",
    "    data_to_save[\"IBI_HeartPy[ms]\"] = all_data[\"ibi\"]\n",
    "    data_to_save[\"IBI_HeartPy_interpolated[ms]\"] = all_data[\"IBI_HeartPy_interpolated\"]\n",
    "    \n",
    "    data_to_save[\"SDNN_HeartPy[ms]\"] = all_data[\"sdnn\"]\n",
    "    data_to_save[\"SDNN_HeartPy_interpolated[ms]\"] = all_data[\"SDNN_HeartPy_interpolated\"]\n",
    "    \n",
    "    data_to_save[\"SDSD_HeartPy[ms]\"] = all_data[\"sdsd\"]\n",
    "    data_to_save[\"SDSD_HeartPy_interpolated[ms]\"] = all_data[\"SDSD_HeartPy_interpolated\"]\n",
    "    \n",
    "    data_to_save[\"RMSSD_HeartPy[ms]\"] = all_data[\"rmssd\"]\n",
    "    data_to_save[\"RMSSD_HeartPy_interpolated[ms]\"] = all_data[\"RMSSD_HeartPy_interpolated\"]\n",
    "    \n",
    "    data_to_save[\"pNN20_HeartPy[%]\"] = all_data[\"pnn20\"]\n",
    "    data_to_save[\"pNN20_HeartPy_interpolated[%]\"] = all_data[\"pNN20_HeartPy_interpolated\"]\n",
    "    \n",
    "    data_to_save[\"pNN50_HeartPy[%]\"] = all_data[\"pnn50\"]\n",
    "    data_to_save[\"pNN50_HeartPy_interpolated[%]\"] = all_data[\"pNN50_HeartPy_interpolated\"]\n",
    "    \n",
    "    data_to_save[\"Pressure[kPa]\"] = all_data[\"Pressure\"]\n",
    "    data_to_save[\"Temperature[C]\"] = all_data[\"Temperature\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    # data_to_save[\"breathing-rate_HeartPy[?]\"] = all_data[\"breathingrate\"] # all values = 0 ?\n",
    "    \n",
    "    \n",
    "    # export to .xlsx takes over twice as long as .csv but reduces the file size by over half: 35MB -> 14MB (partially due to 2nd sheet containing demographic data: approx 4MB)\n",
    "\n",
    "    data_to_save.index.names = ['INDEX_in_all_data']\n",
    "    demographicData.index.names = ['PARAMETER']\n",
    "    demographicData.rename(columns = {0:'VALUE'}, inplace = True)\n",
    "    if(save):\n",
    "        try:\n",
    "            filePath = \"MERGED_DATA\\merged_\" + str(ID) + \"-\" + str(GR) + \".xlsx\"\n",
    "            with pd.ExcelWriter(os.path.join(os.getcwd(),filePath)) as writer:\n",
    "                demographicData.to_excel(writer, sheet_name='Participant_Data')\n",
    "                data_to_save.to_excel(writer, sheet_name='Experiment_Data')\n",
    "        except Exception as e:\n",
    "            print(\"error_2__1 : exporting raw file path?\")\n",
    "            print(e)\n",
    "\n",
    "    # added temp time to avoid string parsing to datetime \n",
    "    \n",
    "    # data_to_save['TEMP_Time'] = all_data['TEMP_Time']\n",
    "    \n",
    "    return data_to_save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2227b058",
   "metadata": {},
   "source": [
    "## *** Clean up ***##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8deca731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeTails(data, GR, testMode = False):\n",
    "    if testMode:\n",
    "        print(\"START removeTails----------------\")\n",
    "        print(f\"len(data) : {len(data)}\")\n",
    "        \n",
    "    phase = \"\"\n",
    "    endIndex = 0\n",
    "    \n",
    "    if(GR == 1):\n",
    "        phase = 'control'\n",
    "    else:\n",
    "        phase = 'intervention'\n",
    "    \n",
    "    endIndex = data['Phase'][data['Phase'] == phase].index[-1]\n",
    "    \n",
    "    if testMode:\n",
    "        print(f\"------------endIndex : {endIndex} ----------\")\n",
    "        print(data['Phase'].iloc[endIndex -1 : endIndex + 2])    \n",
    "        print(data.iloc[endIndex -1 : endIndex + 2])\n",
    "        \n",
    "    data = data[:endIndex]\n",
    "    \n",
    "    if testMode:\n",
    "        print(\"new tail======\")\n",
    "        print(data.tail(3))\n",
    "        print(f\"len(data) : {len(data)}\")\n",
    "    \n",
    "    # front clean up moved to end to avoid indexing issues: \n",
    "    \n",
    "    startIndex = data['Phase'][data['Phase'] == 'adaptation PE'].index[0]\n",
    "    \n",
    "    if testMode:\n",
    "        print(f\"------------endIndex : {startIndex} ----------\")\n",
    "        print(data['Phase'].iloc[startIndex -2 : startIndex + 3])\n",
    "    \n",
    "    data = data[startIndex:]\n",
    "    \n",
    "    if testMode:\n",
    "        print(\"new head======\")\n",
    "        print(data.head(3))\n",
    "        print(f\"len(data) : {len(data)}\")\n",
    "    \n",
    "    data.rename(columns = {'index':'index_in_all_data'}, inplace = True)    \n",
    "    data = data.reset_index()\n",
    "    \n",
    "    if testMode:\n",
    "        print(\"Final product returned : \")\n",
    "        print(data)\n",
    "        print(\"END----------------\")\n",
    "    \n",
    "    \"\"\"\n",
    "    removed abs time from the data set\n",
    "    \n",
    "    timeToSubstract = data[\"TEMP_Time\"][0]   \n",
    "    absolute_time = data['TEMP_Time'] - timeToSubstract\n",
    "\n",
    "    location = data.columns.get_loc(\"Raw_Time[HH:MM:ss.us]\")\n",
    "    data.insert(loc=location, column='Absolute_time[HH:MM:ss.us]', value = absolute_time)\n",
    "    \n",
    "    data.drop(columns = 'TEMP_Time', inplace = True)\n",
    "    \"\"\"\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fcd73c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeBreak(data):\n",
    "    \"\"\"\n",
    "    was used to remove the break\n",
    "    left for now to use it for processing break stage later\n",
    "    \"\"\"\n",
    "    \n",
    "    breakIndexes = np.argwhere(data['Phase'] == 'break')\n",
    "    firstBreakInd = breakIndexes[0]\n",
    "    lastBreakInd = breakIndexes[-1]\n",
    "    \n",
    "    firstBreakTimeStamp = data['Absolute_time[HH:MM:ss.us]'].iloc[firstBreakInd].values\n",
    "    timeStampFirst = pd.to_datetime(firstBreakTimeStamp) \n",
    "\n",
    "    lastBreakTimeStamp = data['Absolute_time[HH:MM:ss.us]'].iloc[lastBreakInd].values\n",
    "    timeStampLast = pd.to_datetime(lastBreakTimeStamp)\n",
    "    \n",
    "    breakDuration = pd.Timedelta(timeStampLast[0] - timeStampFirst[0])\n",
    "\n",
    "    start = lastBreakInd + 1 \n",
    "    end = len(data)\n",
    "    TIME = data['Absolute_time[HH:MM:ss.us]']\n",
    "    indexesToCorrect =  np.arange(start, end)\n",
    "    \n",
    "    TIME[indexesToCorrect] = TIME[indexesToCorrect] - breakDuration \n",
    "\n",
    "    data['Absolute_time[HH:MM:ss.us]'] = TIME\n",
    "    \n",
    "    data.drop(data.index[breakIndexes], inplace = True)\n",
    "\n",
    "    # data.rename(columns = {'index':'index_no_tails'}, inplace = True) \n",
    "    data = data.reset_index()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea10f98",
   "metadata": {},
   "source": [
    "Resample data to 100Hz and merge asof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80c07fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardizeTime(data):\n",
    "    \"\"\"\n",
    "    Deprecated since break phase was uneven length\n",
    "    \"\"\"\n",
    "    \n",
    "    timeDF = pd.DataFrame()\n",
    "    \n",
    "    TotalTime = 2*90 + 16*12*2 # adaptationPE = 90s; adaptationVE = 90s; intervention = 12s * 16; control = intervention\n",
    "    numberOfSamples = TotalTime * 100 # to 100Hz\n",
    "    \n",
    "    #timeDF['TIME'] = pd.timedelta_range(0, periods = numberOfSamples, freq=\"10L\")\n",
    "    #timeDF['Absolute_time[HH:MM:ss.us]'] = timeDF['TIME'].values.to_datetime()\n",
    "    \n",
    "    timeDF['TIME[HH:MM:ss.us]'] = pd.timedelta_range(0, periods = numberOfSamples, freq=\"10L\")        \n",
    "    timeDF['TIME[HH:MM:ss.us]'] = pd.to_datetime(timeDF['TIME[HH:MM:ss.us]'])\n",
    "\n",
    "    data['TIME[HH:MM:ss.us]'] = pd.to_datetime(data['Absolute_time[HH:MM:ss.us]'])\n",
    "    \n",
    "    all_data = pd.merge_asof(timeDF, data, on='TIME[HH:MM:ss.us]', direction='nearest', tolerance = pd.Timedelta('50ms'))\n",
    "    \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfb4a375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveCleanedData(ID, GR, data, demographicData, testMode = False, save = False):\n",
    "    save = saveGlobal\n",
    "    #belowed removed as abs time deprecated\n",
    "    #data.drop(columns = 'Absolute_time[HH:MM:ss.us]', inplace = True)\n",
    "    data.drop(columns = 'index', inplace = True)\n",
    "    \n",
    "    #timeDF['TIME'] = pd.timedelta_range(0, periods = numberOfSamples, freq=\"10L\")\n",
    "    #timeDF['Absolute_time[HH:MM:ss.us]'] = timeDF['TIME'].values.to_datetime()\n",
    "    \n",
    "    if(testMode):\n",
    "        print(\"===================data['TIME']===============\")\n",
    "        print(data['TIME'])\n",
    "    \n",
    "    #I'm not even sure if its needed\n",
    "    #data['TIME[HH:MM:ss.us]'] = data['TIME'].values.to_datetime().dt.strftime('%H:%M:%S.%f')\n",
    "    \n",
    "    data['TIME[HH:MM:ss.us]'] = pd.to_datetime(data['TIME'],format = '%H:%M:%S.%f')\n",
    "    \n",
    "    if(save):\n",
    "        filePath = \"MERGED_DATA/Cleaned/\" + str(ID) + \"-\" + str(GR) + \".xlsx\"\n",
    "        with pd.ExcelWriter(filePath) as writer:\n",
    "            demographicData.to_excel(writer, sheet_name='Participant_Data')\n",
    "            data.to_excel(writer, sheet_name='Experiment_Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e62a545",
   "metadata": {},
   "source": [
    "## 5? try to go through all functions in a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d12935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def executeAllFunctions(item, save = True, testMode = False):\n",
    "    testMode = testModeGlobal\n",
    "    save = saveGlobal\n",
    "    #print(f\"testMode {testMode}\")\n",
    "    \n",
    "    if testMode:\n",
    "        print(\"executeAllFunctions======\")\n",
    "        print(item)\n",
    "        print(type(item))\n",
    "        print(item[0])\n",
    "        print(type(item[0]))\n",
    "        print(item[1])\n",
    "        print(type(item[1]))\n",
    "        \n",
    "    ID = item[0]\n",
    "    GR = item[1]\n",
    "    \n",
    "    print(f\"ID = {ID} | GR = {GR}\")\n",
    "    \n",
    "    #global participantID\n",
    "    #participantID = ID\n",
    "    #global participantGR\n",
    "    #participantGR = GR\n",
    "    \n",
    "    goTime = time.time()\n",
    "    \n",
    "    try:\n",
    "        environment_data = pullEnvironmentData(ID, GR)\n",
    "        #print(environment_data.head(1))\n",
    "    except Exception as e:\n",
    "        print(\"error_2 : env data - import <- pullEnvironmentData()\")\n",
    "        print(e)    \n",
    "    if testMode:\n",
    "        print(\"pullEnvironmentData() - completed\")\n",
    "    \n",
    "    try:\n",
    "        participant_data = pullPhysioData(ID, GR)\n",
    "        #print(participant_data.head(1))\n",
    "    except Exception as e: \n",
    "        print(\"error_2 : participant data <- pullPhysioData()\")\n",
    "        print(e)\n",
    "    if testMode:\n",
    "        print(\"pullPhysioData - completed\") \n",
    "    \n",
    "    try:    \n",
    "        qualityScore = qualityCheck(participant_data)\n",
    "    except Exception as e: \n",
    "        print(\"error_2 : participant data - testing quality\")\n",
    "        print(e)\n",
    "    if testMode:\n",
    "        print(\"qualityCheck - completed\")  \n",
    "            \n",
    "    try:\n",
    "        participant_data = interpolateConsensysHRna(participant_data)\n",
    "    except Exception as e: \n",
    "        print(\"error_2 : participant data - interpolate NA\")\n",
    "        print(e)\n",
    "    if testMode:\n",
    "        print(\"interpolateConsensysHRna - completed\")  \n",
    "\n",
    "    TS = [timeStamp.strftime('%Y-%m-%d %H:%M:%S.%f') for timeStamp in participant_data['timestamp']]\n",
    "    sampleRate = hp.get_samplerate_datetime(TS, timeformat = '%Y-%m-%d %H:%M:%S.%f')\n",
    "    \n",
    "    #ERROR BELOW\n",
    "    \"\"\"\n",
    "    try:\n",
    "        savePlot(participant_data[\"PPG_A13\"].values, sampleRate, winSize, scale)\n",
    "    except Exception as e: \n",
    "        print(\"error_2 : HP peak graph\")\n",
    "        print(e)\n",
    "    if testMode:\n",
    "        print(\"savePlot - completed\")  \n",
    "    \"\"\"\n",
    "    try:\n",
    "        period = getPeriod(participant_data[\"PPG_A13\"].values, sampleRate, winSize, overLap)\n",
    "    except Exception as e:\n",
    "        print(\"error_2 : HP period\")\n",
    "        print(e)  \n",
    "    if testMode:\n",
    "        print(\"getPeriod - completed\")  \n",
    "\n",
    "    try:\n",
    "        hp_data = getContDataFromHP(participant_data[\"PPG_A13\"].values, sampleRate, winSize, overLap, scale, period)\n",
    "    except Exception as e:\n",
    "        print(\"error_2 : HP continous\")\n",
    "        print(e)\n",
    "    if testMode:\n",
    "        print(\"getContDataFromHP - completed\")  \n",
    "    \n",
    "    \"\"\"\n",
    "    #some bool ?? cannot be executed here, some error in saveHRplot \n",
    "    try:\n",
    "        saveHRplot(ID, GR, hp_data, participant_data)\n",
    "    except Exception as e:\n",
    "        print(\"error_2 : HP HR graph\")\n",
    "        print(e)\n",
    "    if testMode:\n",
    "        print(\"saveHRplot - completed\")  \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        demographicData = pullDemoQdata(ID,GR)\n",
    "    except Exception as e:\n",
    "        print(\"error_2 : demographic data - import\")\n",
    "        print(e)\n",
    "    if testMode:\n",
    "        print(\"pullDemoQdata - completed\")\n",
    "    \n",
    "    try:\n",
    "        try:\n",
    "            participant_data = mergeData(participant_data, hp_data)\n",
    "        except Exception as e:\n",
    "            print(\"error_2_1 : merging\")\n",
    "            print(e)\n",
    "        if testMode:\n",
    "            print(\"mergeData - completed\") \n",
    "        \n",
    "        try:\n",
    "            all_data = mergeAll(participant_data, environment_data)\n",
    "        except Exception as e:\n",
    "            print(\"error_2_2 : merging\")\n",
    "            print(e)\n",
    "        if testMode:\n",
    "            print(\"mergeAll - completed\") \n",
    "        \n",
    "        \"\"\"\n",
    "        absoluteTime function deprecated\n",
    "        \n",
    "        try:\n",
    "            all_data = absoluteTime(all_data)\n",
    "        except Exception as e:\n",
    "            print(\"error_2_3 : merging\")\n",
    "            print(e)\n",
    "        print(\"absoluteTime - completed\")\n",
    "        \"\"\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"error_2 : merging\")\n",
    "        print(e)\n",
    "    if testMode:\n",
    "        print(\"merging - completed\")  \n",
    "    \n",
    "    try: \n",
    "        all_data = standardizeData(all_data)\n",
    "    except Exception as e:\n",
    "        print(\"error_2 : standardizing data\")\n",
    "        print(e)\n",
    "    if testMode:\n",
    "        print(\"standardizeData - completed\")  \n",
    "    \n",
    "    try: \n",
    "        all_data = normalizeData(all_data)\n",
    "    except Exception as e:\n",
    "        print(\"error_2 : normalizing data\")\n",
    "        print(e)\n",
    "    if testMode:\n",
    "        print(\"normalizeData - completed\")  \n",
    "        \n",
    "    try:\n",
    "        all_data = processEDA(all_data)\n",
    "    except Exception as e:\n",
    "        print(\"error_2 : processing EDA\")\n",
    "        print(e)\n",
    "    if testMode:\n",
    "        print(\"processEDA - completed\")  \n",
    "    \n",
    "    try:\n",
    "        all_data = smoothHRandSC(all_data)\n",
    "    except Exception as e:\n",
    "        print(\"error_2 : smoothin MA\")\n",
    "        print(e)\n",
    "    if testMode:\n",
    "        print(\"smoothHRandSC - completed\")  \n",
    "        \n",
    "    try:\n",
    "        all_data = smoothHRandSCewma(all_data)\n",
    "    except Exception as e:    \n",
    "        print(\"error_2 : smoothin EWMA\")\n",
    "        print(e)\n",
    "    if testMode:\n",
    "        print(\"smoothHRandSCewma - completed\")  \n",
    "        \n",
    "    try:\n",
    "        all_data = diff1st(all_data)\n",
    "    except Exception as e:\n",
    "        print(\"error_2 : differentiating Data\")\n",
    "        print(e)\n",
    "    if testMode:\n",
    "        print(\"diff1st - completed\")  \n",
    "    \n",
    "    \n",
    "    #\"\"\"\n",
    "    savingStartTime = time.time()\n",
    "    try:\n",
    "        savedData = saveAllData(all_data, ID, GR, demographicData, save=save)\n",
    "    except Exception as e:\n",
    "        print(\"error_2 : exporting raw\")\n",
    "        print(e)\n",
    "    if testMode:\n",
    "        print(f\"saveAllData - completed in {time.time() - savingStartTime}\")  \n",
    "    \n",
    "    #\"\"\"\n",
    "    \n",
    "    try:\n",
    "        noTailsData = removeTails(all_data, GR = GR, testMode = testMode)#savedData)\n",
    "    except Exception as e:\n",
    "        print(\"error_2 : failed to remove tails\")\n",
    "        print(e)\n",
    "    if testMode:\n",
    "        print(\"removeTails - completed\")  \n",
    "    \n",
    "    #below removed to keep the break data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cleanData = removeBreak(noTailsData)\n",
    "    except Exception as e:\n",
    "        print(\"error_2 : failed to remove break\")\n",
    "        print(e)\n",
    "    print(\"removeBreak - completed\")  \n",
    "    \"\"\"\n",
    "    \n",
    "    #belowed removed as break phase lenght was uneven\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cleanData = standardizeTime(noTailsData) #standardizeTime(cleanData)\n",
    "    except Exception as e:\n",
    "        print(\"error_2 : failed to standardized time data\")\n",
    "        print(e)\n",
    "    print(\"standardizeTime - completed\")  \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        saveCleanedData(ID, GR, noTailsData, demographicData, save = save)#cleanData, demographicData)\n",
    "    except Exception as e:\n",
    "        print(\"error_2 : exporting clean\")\n",
    "        print(e)\n",
    "    if testMode:\n",
    "        print(\"saveCleanedData - completed\") \n",
    "    \n",
    "    goTime = time.time() - goTime\n",
    "    print(f\"*********allFunctions - completed ********* runtime = {goTime} **********\")\n",
    "    \n",
    "    qualityScores.append(qualityScore)\n",
    "    noTailsDataSet.append(noTailsData)\n",
    "    demographicDataSet.append(demographicData)\n",
    "    runTimeSet.append(goTime)\n",
    "    \n",
    "    return qualityScore, noTailsData, demographicData, goTime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804c4f11",
   "metadata": {},
   "source": [
    "## ***PROCESS ALL***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ebbdd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processAll(dictionary, save=True, multi = True, testMode = False, advancedMode = False):\n",
    "    testMode = testModeGlobal\n",
    "    save = saveGlobal\n",
    "    advancedMode = advancedModeGlobal\n",
    "    \n",
    "    procedureStartTime = pd.Timestamp.now()\n",
    "    \n",
    "    #qualityScores = []\n",
    "    #noTailsDataSet = []\n",
    "    #demographicDataSet = []\n",
    "    #runTimeSet = []\n",
    "        \n",
    "    \n",
    "    def thredExecuteAllFunctions(item, item2):\n",
    "        try:\n",
    "            print(f'{item[0]} : {item[1]}')\n",
    "            print(item2)\n",
    "            #qualityScore, noTailsData, demographicData, goTime = executeAllFunctions(item, save = save, testMode = testMode)\n",
    "\n",
    "            \n",
    "            #??? return qualityScore, noTailsData, demographicData, goTime\n",
    "            #qualityScore, noTailsData, demographicData, goTime = executeAllFunctions(item, save = save, testMode = testMode)\n",
    "            executeAllFunctions(item, save = save, testMode = testMode)\n",
    "            \n",
    "            # trying changing scope first\n",
    "            #qualityScores.append(qualityScore)\n",
    "            #noTailsDataSet.append(noTailsData)\n",
    "            #demographicDataSet.append(demographicData)\n",
    "            #runTimeSet.append(goTime)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"fatal error_1 : thredExecuteAllFunctions() in {item}\")\n",
    "            print(e)\n",
    "            \n",
    "    def thredExecuteAllFunctionsADVANCE(item):\n",
    "        try:\n",
    "            print(f'{item[0]} : {item[1]}')\n",
    "            executeAllFunctions(item, save = save, testMode = testMode)\n",
    "            \n",
    "            #qualityScore, noTailsData, demographicData, goTime = executeAllFunctions(item, save = save, testMode = testMode)\n",
    "            \"\"\"\n",
    "            ---------------------------------------------------------------------------\n",
    "            TypeError                                 Traceback (most recent call last)\n",
    "            <ipython-input-38-6b9cfbaea136> in <module>\n",
    "            ----> 1 qualityScores, noTailsDataSet, demographicDataSet, runTimeSet = processAll(AllParticipantsGr, save = False, testMode = True)\n",
    "\n",
    "            <ipython-input-32-26328fb9ac4d> in processAll(dictionary, save, multi, testMode)\n",
    "                 44 \n",
    "                 45             for process in concurrent.futures.as_completed(results):\n",
    "            ---> 46                 qualityScores.append(process.result()[0])\n",
    "                 47                 noTailsDataSet.append(process.result()[1])\n",
    "                 48                 demographicDataSet.append(process.result()[2])\n",
    "\n",
    "            TypeError: 'NoneType' object is not subscriptable\n",
    "            \"\"\"\n",
    "            \n",
    "            #??? return qualityScore, noTailsData, demographicData, goTime\n",
    "            \"\"\"\n",
    "            qualityScore, noTailsData, demographicData, goTime = executeAllFunctions(item, save = save, testMode = testMode)\n",
    "            \n",
    "            # trying changing scope first\n",
    "            qualityScores.append(qualityScore)\n",
    "            noTailsDataSet.append(noTailsData)\n",
    "            demographicDataSet.append(demographicData)\n",
    "            runTimeSet.append(goTime)\n",
    "            \"\"\"\n",
    "        except Exception as e:\n",
    "            print(f\"fatal error_1 : thredExecuteAllFunctionsADVANCE() in {item}\")\n",
    "            print(e)\n",
    "    \n",
    "    \n",
    "    #multiprocessing\n",
    "    if multi and advancedMode:\n",
    "        print(f\"advanced mode : {advancedMode} | multiprocessing : {multi}\")\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            results = [executor.submit(thredExecuteAllFunctionsADVANCE, i) for i in list(dictionary.items())]\n",
    "            \n",
    "            for process in concurrent.futures.as_completed(results):\n",
    "                print(process)\n",
    "    \n",
    "    elif multi:\n",
    "        print(f\"advanced mode : {advancedMode} | multiprocessing : {multi}\")\n",
    "        try:\n",
    "            threads = []\n",
    "\n",
    "            for i in list(dictionary.items()):\n",
    "                print(i)\n",
    "                print(type(i))\n",
    "                t = threading.Thread(target = executeAllFunctions, args = [i])#args = [1])\n",
    "                t.start()\n",
    "                threads.append(t)\n",
    "\n",
    "            for t in threads:\n",
    "                t.join()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(\"fatal error_1 : executeAllFunctions()\")\n",
    "            print(e)\n",
    "        \n",
    "    else:\n",
    "        print(f\"multiprocessing : {multi}\")\n",
    "        try:\n",
    "            for item in list(dictionary.items()):\n",
    "                print(f'{item[0]} : {item[1]}')\n",
    "                executeAllFunctions(item, save = save, testMode = False)\n",
    "                \n",
    "                #qualityScore, noTailsData, demographicData, goTime = executeAllFunctions(item, save = save, testMode = False)\n",
    "                #qualityScores.append(qualityScore)\n",
    "                #noTailsDataSet.append(noTailsData)\n",
    "                #demographicDataSet.append(demographicData)\n",
    "                #runTimeSet.append(goTime)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"fatal error_1 : executeAllFunctions()\")\n",
    "            print(e)\n",
    "    \n",
    "    print(f'Total time to process : {pd.Timestamp.now() - procedureStartTime}')\n",
    "    \n",
    "    return qualityScores, noTailsDataSet, demographicDataSet, runTimeSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b70147",
   "metadata": {},
   "source": [
    "# Run:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8308c311",
   "metadata": {},
   "source": [
    "# 1 : Parameters:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "701cc087",
   "metadata": {},
   "outputs": [],
   "source": [
    "global qualityScores\n",
    "qualityScores = []\n",
    "global noTailsDataSet\n",
    "noTailsDataSet = []\n",
    "global demographicDataSet\n",
    "demographicDataSet = []\n",
    "global runTimeSet\n",
    "runTimeSet = []\n",
    "\n",
    "global testModeGlobal\n",
    "testModeGlobal = False\n",
    "global saveGlobal\n",
    "saveGlobal = True\n",
    "global multiGlobal\n",
    "multiGlobal = True\n",
    "global advancedModeGlobal\n",
    "advancedModeGlobal = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4a17c5",
   "metadata": {},
   "source": [
    "### 1.1 : Participants list (dictionary key: id | value: group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "adefd685",
   "metadata": {},
   "outputs": [],
   "source": [
    "AllParticipantsGr = {'0002':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf25de2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AllParticipantsGr = {'0048':1,'0053':2,'0061':2,'0071':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "52984650",
   "metadata": {},
   "outputs": [],
   "source": [
    "AllParticipantsGr = {'0002':2,'0003':1,'0004':2,'0005':2,'0006':1,'0007':1,'0008':1,'0009':2,'0010':2,\n",
    "                     '0011':2,'0012':2,'0013':2,'0014':1,'0015':2,'0016':1,'0017':2,'0018':2,'0019':2,'0020':1,\n",
    "                     '0021':2,'0022':1,'0023':1,'0024':1,'0025':1,'0026':1,'0027':1,'0028':1,'0029':2,'0030':1,\n",
    "                     '0032':2,'0033':2,'0034':2,'0035':1,'0036':2,'0037':1,'0038':1,'0039':2,'0040':2,\n",
    "                     '0041':2,'0042':2,'0043':1,'0044':2,'0045':1,'0046':1,'0047':1,'0048':1,'0049':2,'0050':1,\n",
    "                     '0051':1,'0052':1,'0053':2,'0054':2,'0055':2,'0056':1,'0057':1,'0058':1,'0059':1,'0060':2,\n",
    "                     '0061':2,'0062':2,'0063':1,\n",
    "                     '0071':2,'0072':1,'0073':2\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40ec41ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "print(len(AllParticipantsGr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d14ab",
   "metadata": {},
   "source": [
    "### 1.2 : Run parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64a2453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "winSize = 6 # in seconds\n",
    "overLap = 0.5 # % overlap\n",
    "sampleRate = 128.0\n",
    "scale = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605845e6",
   "metadata": {},
   "source": [
    "### 1.3 : Process all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f304f2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advanced mode : True | multiprocessing : True\n",
      "0002 : 20003 : 1\n",
      "ID = 0002 | GR = 2\n",
      "\n",
      "ID = 0003 | GR = 1\n",
      "0004 : 2\n",
      "ID = 0004 | GR = 2\n",
      "0005 : 2\n",
      "ID = 0005 | GR = 2\n",
      "0006 : 1\n",
      "ID = 0006 | GR = 1\n",
      "0007 : 1\n",
      "ID = 0007 | GR = 1\n",
      "0008 : 1\n",
      "ID = 0008 | GR = 1\n",
      "0009 : 2\n",
      "ID = 0009 | GR = 2\n",
      "0010 : 2\n",
      "ID = 0010 | GR = 20011 : 2\n",
      "\n",
      "ID = 0011 | GR = 2\n",
      "0012 : 2\n",
      "ID = 0012 | GR = 2\n",
      "0013 : 2\n",
      "ID = 0013 | GR = 20014 : 1\n",
      "\n",
      "ID = 0014 | GR = 10015 : 2\n",
      "\n",
      "ID = 0015 | GR = 2\n",
      "0016 : 1\n",
      "0017 : 2\n",
      "ID = 0017 | GR = 2\n",
      "ID = 0016 | GR = 10018 : 2\n",
      "\n",
      "ID = 0018 | GR = 20019 : 2\n",
      "ID = 0019 | GR = 2\n",
      "\n",
      "0020 : 1\n",
      "ID = 0020 | GR = 1\n",
      "0021 : 2\n",
      "ID = 0021 | GR = 2\n",
      "0022 : 1\n",
      "ID = 0022 | GR = 1\n",
      "0023 : 1\n",
      "0024 : 1ID = 0023 | GR = 1\n",
      "\n",
      "0025 : 1ID = 0024 | GR = 1\n",
      "\n",
      "ID = 0025 | GR = 1\n",
      "0026 : 1\n",
      "ID = 0026 | GR = 1\n",
      "0027 : 1\n",
      "ID = 0027 | GR = 1\n",
      "0028 : 1\n",
      "ID = 0028 | GR = 1\n",
      "0029 : 2\n",
      "0030 : 1\n",
      "ID = 0030 | GR = 1\n",
      "0032 : 2\n",
      "ID = 0032 | GR = 2\n",
      "ID = 0029 | GR = 20033 : 2\n",
      "\n",
      "ID = 0033 | GR = 20034 : 2\n",
      "\n",
      "ID = 0034 | GR = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Krzyniu\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Krzyniu\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\Krzyniu\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:261: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "C:\\Users\\Krzyniu\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:221: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "C:\\Users\\Krzyniu\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:253: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\Krzyniu\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:180: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = um.true_divide(\n",
      "C:\\Users\\Krzyniu\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3702: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "C:\\Users\\Krzyniu\\anaconda3\\lib\\site-packages\\numpy\\ma\\core.py:5239: RuntimeWarning: Mean of empty slice.\n",
      "  result = super(MaskedArray, self).mean(axis=axis,\n",
      "C:\\Users\\Krzyniu\\anaconda3\\lib\\site-packages\\heartpy\\analysis.py:784: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  measures['sd1/sd2'] = sd1 / sd2\n",
      "C:\\Users\\Krzyniu\\anaconda3\\lib\\site-packages\\heartpy\\analysis.py:784: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  measures['sd1/sd2'] = sd1 / sd2\n",
      "C:\\Users\\Krzyniu\\anaconda3\\lib\\site-packages\\scipy\\interpolate\\fitpack2.py:279: UserWarning: \n",
      "The maximal number of iterations maxit (set to 20 by the program)\n",
      "allowed for finding a smoothing spline with fp=s has been reached: s\n",
      "too small.\n",
      "There is an approximation returned but the corresponding weighted sum\n",
      "of squared residuals does not satisfy the condition abs(fp-s)/s < tol.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\Krzyniu\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\internals\\blocks.py:954: UserWarning: Warning: converting a masked element to nan.\n",
      "  values[indexer] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      "     pcost       dcost       gap    pres   dres     pcost       dcost       gap    pres   dres\n",
      "\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.9260e+04 -3.8983e+04  1e+05  4e+02  9e+00 0: -4.0100e+04 -4.0017e+04  1e+05  3e+02  6e+00\n",
      "\n",
      " 0: -3.5660e+04 -3.3138e+04  4e+05  6e+02  2e+01 0: -3.9857e+04 -3.9512e+04  1e+05  3e+02  7e+00\n",
      "\n",
      " 1: -4.0041e+04 -4.6913e+04  7e+03  2e+01  5e-01 1: -3.9318e+04 -5.4033e+04  2e+04  5e+01  1e+00\n",
      "\n",
      " 1: -3.7386e+04 -1.1748e+05  1e+05  2e+02  4e+00\n",
      " 1: -3.9965e+04 -5.3541e+04  2e+04  4e+01  9e-01\n",
      " 2: -4.0038e+04 -4.1423e+04  1e+03  4e+00  7e-02\n",
      " 2: -3.9385e+04 -4.2474e+04  3e+03  7e+00  2e-01\n",
      " 2: -3.8525e+04 -9.4364e+04  6e+04  7e+01  2e+00\n",
      " 2: -4.0048e+04 -4.2119e+04  2e+03  5e+00  1e-01\n",
      " 3: -4.0044e+04 -4.0313e+04  3e+02  6e-01  1e-02\n",
      " 3: -3.9392e+04 -3.9979e+04  6e+02  1e+00  3e-02\n",
      " 3: -3.8807e+04 -5.9348e+04  2e+04  2e+01  6e-01\n",
      " 4: -4.0052e+04 -4.0148e+04  1e+02  8e-02  2e-03 3: -4.0060e+04 -4.0495e+04  4e+02  8e-01  2e-02\n",
      "\n",
      " 4: -3.9386e+04 -3.9554e+04  2e+02  1e-01  3e-03\n",
      " 4: -3.8918e+04 -5.4436e+04  2e+04  1e+01  4e-01\n",
      " 4: -4.0069e+04 -4.0209e+04  1e+02  1e-01  3e-03 5: -4.0099e+04 -4.0124e+04  3e+01  9e-03  2e-04\n",
      "\n",
      " 5: -3.9451e+04 -3.9498e+04  5e+01  3e-02  8e-04\n",
      " 5: -3.8967e+04 -4.7694e+04  9e+03  6e+00  1e-01\n",
      " 5: -4.0120e+04 -4.0152e+04  3e+01  2e-02  4e-04\n",
      " 6: -4.0102e+04 -4.0124e+04  2e+01  7e-03  1e-04\n",
      " 6: -3.9462e+04 -3.9494e+04  3e+01  2e-02  5e-04\n",
      " 6: -3.8947e+04 -4.4055e+04  5e+03  2e+00  6e-02\n",
      " 6: -4.0131e+04 -4.0147e+04  2e+01  7e-03  1e-04\n",
      " 7: -4.0112e+04 -4.0123e+04  1e+01  3e-03  6e-05\n",
      " 7: -3.9476e+04 -3.9490e+04  1e+01  8e-03  2e-04\n",
      " 7: -3.8912e+04 -4.1800e+04  3e+03  1e+00  3e-02\n",
      " 7: -4.0139e+04 -4.0145e+04  6e+00  2e-03  5e-05 8: -4.0118e+04 -4.0122e+04  4e+00  8e-04  2e-05\n",
      "\n",
      " 8: -3.9484e+04 -3.9489e+04  5e+00  3e-03  6e-05\n",
      " 8: -3.8856e+04 -4.0723e+04  2e+03  5e-01  1e-02\n",
      " 8: -4.0142e+04 -4.0144e+04  2e+00  4e-04  9e-06\n",
      " 9: -3.9486e+04 -3.9488e+04  2e+00  5e-04  1e-05\n",
      " 9: -4.0120e+04 -4.0122e+04  2e+00  2e-04  5e-06\n",
      " 9: -3.8824e+04 -3.9966e+04  1e+03  2e-01  5e-03\n",
      " 9: -4.0142e+04 -4.0144e+04  1e+00  3e-04  6e-06\n",
      "10: -4.0121e+04 -4.0122e+04  8e-01  5e-05  9e-07\n",
      "10: -3.9487e+04 -3.9488e+04  6e-01  1e-04  3e-06\n",
      "10: -3.9141e+04 -3.9467e+04  3e+02  3e-02  9e-04\n",
      "10: -4.0143e+04 -4.0143e+04  4e-01  6e-05  1e-06\n",
      "11: -3.9487e+04 -3.9488e+04  1e-01  2e-05  4e-07\n",
      "11: -3.9193e+04 -3.9441e+04  2e+02  2e-02  5e-04\n",
      "11: -4.0122e+04 -4.0122e+04  2e-01  2e-06  4e-08\n",
      "11: -4.0143e+04 -4.0143e+04  3e-01  3e-05  7e-07\n",
      "12: -3.9487e+04 -3.9488e+04  1e-01  9e-06  2e-07\n",
      "12: -4.0122e+04 -4.0122e+04  9e-02  6e-07  1e-08\n",
      "12: -3.9310e+04 -3.9415e+04  1e+02  8e-03  2e-04\n",
      "12: -4.0143e+04 -4.0143e+04  1e-01  9e-06  2e-07\n",
      "13: -3.9488e+04 -3.9488e+04  3e-02  2e-06  4e-08\n",
      "13: -3.9343e+04 -3.9408e+04  6e+01  4e-03  1e-04\n",
      "13: -4.0122e+04 -4.0122e+04  3e-02  2e-07  3e-09\n",
      "13: -4.0143e+04 -4.0143e+04  3e-02  1e-06  3e-08\n",
      "14: -3.9488e+04 -3.9488e+04  2e-02  9e-07  2e-08\n",
      "14: -3.9363e+04 -3.9403e+04  4e+01  2e-03  7e-05\n",
      "14: -4.0122e+04 -4.0122e+04  3e-02  1e-07  2e-09\n",
      "14: -4.0143e+04 -4.0143e+04  2e-02  6e-07  1e-08\n",
      "15: -3.9488e+04 -3.9488e+04  6e-03  2e-07  5e-09\n",
      "15: -4.0122e+04 -4.0122e+04  8e-03  2e-08  4e-10\n",
      "Optimal solution found.\n",
      "15: -4.0143e+04 -4.0143e+04  6e-03  1e-07  3e-0915: -3.9375e+04 -3.9400e+04  2e+01  1e-03  4e-05\n",
      "\n",
      "16: -3.9384e+04 -3.9396e+04  1e+01  6e-04  2e-05\n",
      "16: -3.9488e+04 -3.9488e+04  5e-03  1e-07  3e-09\n",
      "16: -4.0143e+04 -4.0143e+04  4e-03  6e-08  1e-09\n",
      "Optimal solution found.\n",
      "17: -3.9388e+04 -3.9394e+04  7e+00  3e-04  7e-06\n",
      "17: -3.9488e+04 -3.9488e+04  2e-03  3e-08  8e-10\n",
      "Optimal solution found.\n",
      "18: -3.9391e+04 -3.9393e+04  2e+00  6e-05  2e-06\n",
      "19: -3.9391e+04 -3.9392e+04  9e-01  2e-05  4e-07\n",
      "20: -3.9392e+04 -3.9392e+04  3e-01  4e-06  1e-07\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.8552e+04 -3.7705e+04  2e+05  4e+02  1e+01\n",
      "21: -3.9392e+04 -3.9392e+04  1e-01  9e-07  2e-08\n",
      " 1: -3.9008e+04 -5.9750e+04  2e+04  6e+01  2e+0022: -3.9392e+04 -3.9392e+04  4e-02  2e-07  6e-09\n",
      "\n",
      " 2: -3.9312e+04 -4.3794e+04  5e+03  9e+00  3e-0123: -3.9392e+04 -3.9392e+04  4e-02  2e-07  5e-09\n",
      "\n",
      "24: -3.9392e+04 -3.9392e+04  1e-02  3e-08  8e-10\n",
      "Optimal solution found.\n",
      " 3: -3.9345e+04 -4.0267e+04  9e+02  2e+00  6e-02\n",
      " 4: -3.9339e+04 -3.9612e+04  3e+02  2e-01  9e-03\n",
      " 5: -3.9373e+04 -3.9470e+04  1e+02  2e-02  9e-04\n",
      " 6: -3.9391e+04 -3.9464e+04  7e+01  2e-02  6e-04\n",
      " 7: -3.9411e+04 -3.9457e+04  5e+01  1e-02  4e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-7d090e579a5c>:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  NewDF[columnName] = data[parameter].ewm(alpha = alpha).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8: -3.9431e+04 -3.9452e+04  2e+01  4e-03  2e-04\n",
      " 9: -3.9440e+04 -3.9449e+04  9e+00  2e-03  6e-05\n",
      "10: -3.9443e+04 -3.9447e+04  4e+00  7e-04  2e-05\n",
      "11: -3.9445e+04 -3.9447e+04  1e+00  2e-04  6e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-faa363d1e944>:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[columnName] = diffData\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12: -3.9446e+04 -3.9446e+04  5e-01  2e-05  9e-07\n",
      "13: -3.9446e+04 -3.9446e+04  1e-01  4e-06  2e-07\n",
      "14: -3.9446e+04 -3.9446e+04  1e-01  3e-06  1e-07\n",
      "15: -3.9446e+04 -3.9446e+04  4e-02  6e-07  2e-08\n",
      "16: -3.9446e+04 -3.9446e+04  3e-02  3e-07  1e-08\n",
      "17: -3.9446e+04 -3.9446e+04  8e-03  8e-08  3e-09\n",
      "Optimal solution found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-56396ebd70d1>:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_CONSENSYS_interpolated_NORM_SMA(60s)\"] = all_data[\"HR_CONSENSYS_NORM_SMA(60.0s)\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:142: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save['HR_CONSENSYS_interpolated_NORM_EWMA(0.01)'] = all_data['HR_CONSENSYS_NORM_EWMA(0.01)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:143: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save['HR_CONSENSYS_interpolated_NORM_EWMA(0.001)'] = all_data['HR_CONSENSYS_NORM_EWMA(0.001)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save['HR_CONSENSYS_interpolated_NORM_EWMA(0.0001)'] = all_data['HR_CONSENSYS_NORM_EWMA(0.0001)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"IBI_CONSENSYS[ms]\"] = all_data['PPG_IBI'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"IBI_CONSENSYS_interpolated[ms]\"] = all_data['IBI_CONSENSYS_interpolated'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy[BPM]\"] = all_data[\"bpm\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:150: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_SMA(1s)\"] = all_data['bpm_SMA(1.0s)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:151: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_SMA(3s)\"] = all_data[\"bpm_SMA(3.0s)\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_SMA(60s)\"] = all_data['bpm_SMA(60.0s)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save['HR_HeartPy_EWMA(0.01)'] = all_data['bpm_EWMA(0.01)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save['HR_HeartPy_EWMA(0.001)'] = all_data['bpm_EWMA(0.001)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save['HR_HeartPy_EWMA(0.0001)'] = all_data['bpm_EWMA(0.0001)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:157: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_interpolated[BPM]\"] = all_data[\"HR_HeartPy_interpolated\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:158: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_interpolated_DIFF(1)\"] = all_data[\"HR_HeartPy_interpolated_DIFF(1)\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:160: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_interpolated_SMA(1s)\"] = all_data[\"HR_HeartPy_interpolated_SMA(1.0s)\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:161: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_interpolated_SMA(3s)\"] = all_data[\"HR_HeartPy_interpolated_SMA(3.0s)\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_interpolated_SMA(60s)\"] = all_data['HR_HeartPy_interpolated_SMA(60.0s)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:163: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save['HR_HeartPy_interpolated_EWMA(0.01)'] = all_data['HR_HeartPy_interpolated_EWMA(0.01)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:164: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save['HR_HeartPy_interpolated_EWMA(0.001)'] = all_data['HR_HeartPy_interpolated_EWMA(0.001)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:165: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save['HR_HeartPy_interpolated_EWMA(0.0001)'] = all_data['HR_HeartPy_interpolated_EWMA(0.0001)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:167: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_interpolated_STD\"] = all_data[\"HR_HP_STD\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:168: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_interpolated_STD_DIFF(1)\"] = all_data[\"HR_HP_STD_DIFF(1)\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:170: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_interpolated_STD_SMA(1s)\"] = all_data[\"HR_HP_STD_SMA(1.0s)\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:171: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_interpolated_STD_SMA(3s)\"] = all_data[\"HR_HP_STD_SMA(3.0s)\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:172: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_interpolated_STD_SMA(60s)\"] = all_data['HR_HP_STD_SMA(60.0s)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:173: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save['HR_HeartPy_interpolated_STD_EWMA(0.01)'] = all_data['HR_HP_STD_EWMA(0.01)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save['HR_HeartPy_interpolated_STD_EWMA(0.001)'] = all_data['HR_HP_STD_EWMA(0.001)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save['HR_HeartPy_interpolated_STD_EWMA(0.0001)'] = all_data['HR_HP_STD_EWMA(0.0001)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:177: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_interpolated_NORM\"] = all_data[\"HR_HP_NORM\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_interpolated_NORM_DIFF(1)\"] = all_data[\"HR_HP_NORM_DIFF(1)\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_interpolated_NORM_SMA(1s)\"] = all_data[\"HR_HP_NORM_SMA(1.0s)\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:181: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_interpolated_NORM_SMA(3s)\"] = all_data[\"HR_HP_NORM_SMA(3.0s)\"].copy()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-56396ebd70d1>:182: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_interpolated_NORM_SMA(60s)\"] = all_data['HR_HP_NORM_SMA(60.0s)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:183: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save['HR_HeartPy_interpolated_NORM_EWMA(0.01)'] = all_data['HR_HP_NORM_EWMA(0.01)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save['HR_HeartPy_interpolated_NORM_EWMA(0.001)'] = all_data['HR_HP_NORM_EWMA(0.001)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:185: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save['HR_HeartPy_interpolated_NORM_EWMA(0.0001)'] = all_data['HR_HP_NORM_EWMA(0.0001)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"IBI_HeartPy[ms]\"] = all_data[\"ibi\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:188: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"IBI_HeartPy_interpolated[ms]\"] = all_data[\"IBI_HeartPy_interpolated\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"SDNN_HeartPy[ms]\"] = all_data[\"sdnn\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"SDNN_HeartPy_interpolated[ms]\"] = all_data[\"SDNN_HeartPy_interpolated\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:193: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"SDSD_HeartPy[ms]\"] = all_data[\"sdsd\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"SDSD_HeartPy_interpolated[ms]\"] = all_data[\"SDSD_HeartPy_interpolated\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:196: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"RMSSD_HeartPy[ms]\"] = all_data[\"rmssd\"]\n",
      "<ipython-input-26-56396ebd70d1>:197: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"RMSSD_HeartPy_interpolated[ms]\"] = all_data[\"RMSSD_HeartPy_interpolated\"]\n",
      "<ipython-input-26-56396ebd70d1>:199: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"pNN20_HeartPy[%]\"] = all_data[\"pnn20\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"pNN20_HeartPy_interpolated[%]\"] = all_data[\"pNN20_HeartPy_interpolated\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"pNN50_HeartPy[%]\"] = all_data[\"pnn50\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:203: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"pNN50_HeartPy_interpolated[%]\"] = all_data[\"pNN50_HeartPy_interpolated\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:205: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"Pressure[kPa]\"] = all_data[\"Pressure\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:206: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"Temperature[C]\"] = all_data[\"Temperature\"].copy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.0201e+04 -4.0031e+04  1e+05  3e+02  7e+00\n",
      " 1: -4.0190e+04 -5.0427e+04  1e+04  3e+01  6e-01\n",
      " 2: -4.0215e+04 -4.1520e+04  1e+03  4e+00  7e-02\n",
      " 3: -4.0215e+04 -4.0569e+04  4e+02  7e-01  1e-02\n",
      " 4: -4.0225e+04 -4.0304e+04  8e+01  7e-03  1e-04\n",
      " 5: -4.0262e+04 -4.0300e+04  4e+01  3e-03  6e-05\n",
      " 6: -4.0275e+04 -4.0299e+04  2e+01  2e-03  3e-05\n",
      " 7: -4.0285e+04 -4.0298e+04  1e+01  8e-04  2e-05\n",
      " 8: -4.0291e+04 -4.0297e+04  7e+00  4e-04  7e-06\n",
      " 9: -4.0293e+04 -4.0297e+04  4e+00  2e-04  3e-06\n",
      "10: -4.0295e+04 -4.0296e+04  2e+00  7e-05  1e-06\n",
      "11: -4.0295e+04 -4.0296e+04  7e-01  2e-05  3e-07\n",
      "12: -4.0296e+04 -4.0296e+04  2e-01  4e-06  7e-08\n",
      "13: -4.0296e+04 -4.0296e+04  7e-02  4e-07  8e-09\n",
      "14: -4.0296e+04 -4.0296e+04  2e-02  5e-08  1e-09\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.8724e+04 -3.8613e+04  1e+05  3e+02  6e+00\n",
      " 1: -3.8684e+04 -4.8877e+04  1e+04  3e+01  7e-01\n",
      " 2: -3.8686e+04 -4.2110e+04  4e+03  9e+00  2e-01\n",
      " 3: -3.8673e+04 -4.0225e+04  2e+03  3e+00  6e-02\n",
      " 4: -3.8654e+04 -3.9316e+04  7e+02  9e-01  2e-02\n",
      " 5: -3.8652e+04 -3.8881e+04  2e+02  2e-01  3e-03\n",
      " 6: -3.8719e+04 -3.8785e+04  7e+01  3e-03  6e-05\n",
      " 7: -3.8756e+04 -3.8783e+04  3e+01  6e-04  1e-05\n",
      " 8: -3.8770e+04 -3.8783e+04  1e+01  2e-04  4e-06\n",
      " 9: -3.8776e+04 -3.8782e+04  6e+00  7e-05  1e-06\n",
      "10: -3.8780e+04 -3.8782e+04  2e+00  2e-05  3e-07\n",
      "11: -3.8781e+04 -3.8782e+04  1e+00  9e-06  2e-07\n",
      "12: -3.8782e+04 -3.8782e+04  4e-01  9e-08  2e-09\n",
      "13: -3.8782e+04 -3.8782e+04  9e-02  1e-08  3e-10\n",
      "14: -3.8782e+04 -3.8782e+04  4e-02  3e-09  8e-11\n",
      "15: -3.8782e+04 -3.8782e+04  1e-02  9e-10  7e-11\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.8920e+04 -3.8406e+04  1e+05  4e+02  1e+01\n",
      " 1: -3.9098e+04 -5.3660e+04  2e+04  4e+01  1e+00\n",
      " 2: -3.9182e+04 -4.2766e+04  4e+03  9e+00  3e-01\n",
      " 3: -3.9175e+04 -4.0179e+04  1e+03  2e+00  5e-02\n",
      " 4: -3.9176e+04 -3.9414e+04  2e+02  2e-01  7e-03\n",
      " 5: -3.9236e+04 -3.9301e+04  6e+01  4e-02  1e-03\n",
      " 6: -3.9253e+04 -3.9293e+04  4e+01  1e-02  4e-04\n",
      " 7: -3.9273e+04 -3.9288e+04  2e+01  5e-03  1e-04\n",
      " 8: -3.9280e+04 -3.9286e+04  6e+00  1e-03  3e-05\n",
      " 9: -3.9283e+04 -3.9285e+04  3e+00  4e-04  1e-05\n",
      "10: -3.9283e+04 -3.9285e+04  2e+00  2e-04  5e-06\n",
      "11: -3.9284e+04 -3.9285e+04  7e-01  7e-05  2e-06\n",
      "12: -3.9284e+04 -3.9285e+04  3e-01  2e-05  7e-07\n",
      "13: -3.9284e+04 -3.9284e+04  9e-02  2e-07  7e-09\n",
      "14: -3.9284e+04 -3.9284e+04  3e-02  7e-08  2e-09\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.9341e+04 -3.9143e+04  1e+05  4e+02  8e+00\n",
      " 1: -3.9345e+04 -5.2287e+04  1e+04  4e+01  9e-01\n",
      " 2: -3.9363e+04 -4.1377e+04  2e+03  5e+00  1e-01\n",
      " 3: -3.9364e+04 -3.9584e+04  2e+02  3e-01  7e-03\n",
      " 4: -3.9414e+04 -3.9456e+04  4e+01  2e-02  5e-04\n",
      " 5: -3.9431e+04 -3.9452e+04  2e+01  8e-03  2e-04\n",
      " 6: -3.9440e+04 -3.9451e+04  1e+01  4e-03  8e-05\n",
      " 7: -3.9445e+04 -3.9450e+04  5e+00  2e-03  3e-05\n",
      " 8: -3.9448e+04 -3.9450e+04  2e+00  6e-04  1e-05\n",
      " 9: -3.9449e+04 -3.9450e+04  8e-01  2e-04  4e-06\n",
      "10: -3.9449e+04 -3.9449e+04  2e-01  3e-05  6e-07\n",
      "11: -3.9449e+04 -3.9449e+04  1e-01  1e-05  3e-07\n",
      "12: -3.9449e+04 -3.9449e+04  4e-02  3e-06  6e-08\n",
      "13: -3.9449e+04 -3.9449e+04  3e-02  1e-06  3e-08\n",
      "14: -3.9449e+04 -3.9449e+04  7e-03  3e-07  7e-09\n",
      "15: -3.9449e+04 -3.9449e+04  6e-03  2e-07  5e-09\n",
      "16: -3.9449e+04 -3.9449e+04  2e-03  4e-08  9e-10\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.9957e+04 -3.9845e+04  1e+05  3e+02  6e+00\n",
      " 1: -3.9917e+04 -4.5965e+04  6e+03  2e+01  4e-01\n",
      " 2: -3.9925e+04 -4.0591e+04  7e+02  2e+00  4e-02\n",
      " 3: -3.9934e+04 -4.0057e+04  1e+02  2e-01  4e-03\n",
      " 4: -3.9959e+04 -3.9998e+04  4e+01  2e-03  4e-05\n",
      " 5: -3.9984e+04 -3.9997e+04  1e+01  6e-04  1e-05\n",
      " 6: -3.9991e+04 -3.9997e+04  5e+00  2e-04  4e-06\n",
      " 7: -3.9994e+04 -3.9997e+04  2e+00  6e-05  1e-06\n",
      " 8: -3.9996e+04 -3.9996e+04  6e-01  1e-05  2e-07\n",
      " 9: -3.9996e+04 -3.9996e+04  4e-01  4e-06  8e-08\n",
      "10: -3.9996e+04 -3.9996e+04  1e-01  9e-07  2e-08\n",
      "11: -3.9996e+04 -3.9996e+04  9e-02  5e-07  1e-08\n",
      "12: -3.9996e+04 -3.9996e+04  3e-02  1e-07  2e-09\n",
      "13: -3.9996e+04 -3.9996e+04  2e-02  8e-08  2e-09\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.9136e+04 -3.8821e+04  2e+05  4e+02  8e+00\n",
      " 1: -3.9217e+04 -6.0770e+04  3e+04  6e+01  1e+00\n",
      " 2: -3.9314e+04 -4.6508e+04  7e+03  1e+01  3e-01\n",
      " 3: -3.9340e+04 -4.2262e+04  3e+03  5e+00  1e-01\n",
      " 4: -3.9333e+04 -4.1378e+04  2e+03  3e+00  5e-02\n",
      " 5: -3.9329e+04 -4.1319e+04  2e+03  2e+00  5e-02\n",
      " 6: -3.9330e+04 -4.0515e+04  1e+03  1e+00  3e-02\n",
      " 7: -3.9324e+04 -4.0469e+04  1e+03  1e+00  2e-02\n",
      " 8: -3.9330e+04 -3.9963e+04  6e+02  6e-01  1e-02\n",
      " 9: -3.9323e+04 -3.9888e+04  6e+02  4e-01  8e-03\n",
      "10: -3.9380e+04 -3.9574e+04  2e+02  1e-01  2e-03\n",
      "11: -3.9405e+04 -3.9536e+04  1e+02  6e-02  1e-03\n",
      "12: -3.9467e+04 -3.9509e+04  4e+01  2e-02  3e-04\n",
      "13: -3.9482e+04 -3.9505e+04  2e+01  6e-03  1e-04\n",
      "14: -3.9493e+04 -3.9503e+04  1e+01  2e-03  3e-05\n",
      "15: -3.9498e+04 -3.9502e+04  4e+00  1e-04  2e-06\n",
      "16: -3.9501e+04 -3.9502e+04  1e+00  2e-05  3e-07\n",
      "17: -3.9501e+04 -3.9502e+04  8e-01  1e-05  2e-07\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.9559e+04 -3.9373e+04  1e+05  4e+02  7e+00\n",
      "18: -3.9501e+04 -3.9502e+04  2e-01  2e-06  3e-08\n",
      " 1: -3.9560e+04 -5.5493e+04  2e+04  5e+01  1e+00\n",
      "19: -3.9502e+04 -3.9502e+04  1e-01  9e-07  2e-08\n",
      " 2: -3.9591e+04 -4.4947e+04  6e+03  1e+01  2e-01\n",
      "20: -3.9502e+04 -3.9502e+04  4e-02  2e-07  4e-09\n",
      "21: -3.9502e+04 -3.9502e+04  4e-02  1e-07  3e-09\n",
      " 3: -3.9584e+04 -4.2066e+04  3e+03  4e+00  9e-02\n",
      "22: -3.9502e+04 -3.9502e+04  1e-02  3e-08  6e-10\n",
      "Optimal solution found.\n",
      " 4: -3.9577e+04 -4.0698e+04  1e+03  2e+00  3e-02\n",
      " 5: -3.9552e+04 -4.0229e+04  7e+02  7e-01  1e-02\n",
      " 6: -3.9548e+04 -3.9930e+04  4e+02  3e-01  5e-03\n",
      " 7: -3.9565e+04 -3.9793e+04  2e+02  1e-01  2e-03\n",
      " 8: -3.9676e+04 -3.9726e+04  5e+01  2e-02  3e-04\n",
      " 9: -3.9691e+04 -3.9723e+04  3e+01  7e-03  1e-04\n",
      "10: -3.9710e+04 -3.9720e+04  1e+01  2e-03  4e-05\n",
      "11: -3.9716e+04 -3.9720e+04  4e+00  5e-04  1e-05\n",
      "     pcost       dcost       gap    pres   dres12: -3.9718e+04 -3.9719e+04  1e+00  1e-04  2e-06\n",
      "\n",
      " 0: -3.9239e+04 -3.8944e+04  1e+05  3e+02  1e+01\n",
      "13: -3.9718e+04 -3.9719e+04  1e+00  1e-04  2e-06\n",
      " 1: -3.9334e+04 -5.1682e+04  1e+04  4e+01  1e+00\n",
      "14: -3.9718e+04 -3.9719e+04  6e-01  4e-05  8e-07\n",
      " 2: -3.9437e+04 -4.2864e+04  4e+03  9e+00  3e-01\n",
      "15: -3.9719e+04 -3.9719e+04  2e-01  9e-06  2e-07\n",
      " 3: -3.9435e+04 -4.0879e+04  1e+03  3e+00  7e-02\n",
      "16: -3.9719e+04 -3.9719e+04  1e-01  5e-06  1e-07\n",
      " 4: -3.9413e+04 -4.0125e+04  7e+02  9e-01  2e-02\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.0067e+04 -3.9985e+04  1e+05  3e+02  6e+00\n",
      "17: -3.9719e+04 -3.9719e+04  4e-02  1e-06  2e-08\n",
      " 1: -4.0009e+04 -4.5139e+04  5e+03  2e+01  3e-01\n",
      " 5: -3.9397e+04 -3.9681e+04  3e+02  2e-01  5e-03\n",
      "18: -3.9719e+04 -3.9719e+04  2e-02  2e-07  4e-09\n",
      " 6: -3.9455e+04 -3.9570e+04  1e+02  3e-02  8e-04\n",
      " 2: -4.0013e+04 -4.0922e+04  9e+02  3e+00  5e-02\n",
      "19: -3.9719e+04 -3.9719e+04  5e-03  4e-08  8e-10\n",
      "Optimal solution found.\n",
      " 7: -3.9479e+04 -3.9560e+04  8e+01  1e-02  4e-04\n",
      " 3: -4.0016e+04 -4.0253e+04  2e+02  5e-01  1e-02\n",
      " 8: -3.9522e+04 -3.9554e+04  3e+01  5e-03  1e-04\n",
      " 4: -4.0025e+04 -4.0099e+04  7e+01  3e-02  6e-04\n",
      " 9: -3.9536e+04 -3.9551e+04  1e+01  1e-03  3e-05\n",
      " 5: -4.0044e+04 -4.0093e+04  5e+01  2e-02  4e-04\n",
      "10: -3.9544e+04 -3.9549e+04  5e+00  4e-04  1e-05 6: -4.0060e+04 -4.0090e+04  3e+01  1e-02  2e-04\n",
      "\n",
      "11: -3.9545e+04 -3.9549e+04  4e+00  2e-04  7e-06 7: -4.0074e+04 -4.0089e+04  1e+01  5e-03  1e-04\n",
      "\n",
      "12: -3.9546e+04 -3.9548e+04  2e+00  9e-05  3e-06\n",
      " 8: -4.0083e+04 -4.0088e+04  5e+00  2e-03  3e-05\n",
      "13: -3.9547e+04 -3.9548e+04  9e-01  4e-05  1e-06\n",
      " 9: -4.0084e+04 -4.0088e+04  3e+00  9e-04  2e-05\n",
      "14: -3.9547e+04 -3.9547e+04  4e-01  1e-05  4e-07\n",
      "10: -4.0086e+04 -4.0087e+04  2e+00  5e-04  1e-05\n",
      "15: -3.9547e+04 -3.9547e+04  2e-01  5e-06  2e-07\n",
      "11: -4.0086e+04 -4.0087e+04  1e+00  3e-04  6e-06\n",
      "16: -3.9547e+04 -3.9547e+04  9e-02  1e-06  4e-08\n",
      "12: -4.0087e+04 -4.0087e+04  7e-01  2e-04  3e-06\n",
      "17: -3.9547e+04 -3.9547e+04  2e-02  3e-07  9e-09\n",
      "13: -4.0087e+04 -4.0087e+04  5e-01  9e-05  2e-0618: -3.9547e+04 -3.9547e+04  2e-02  2e-07  4e-09\n",
      "\n",
      "19: -3.9547e+04 -3.9547e+04  5e-03  4e-08  1e-09\n",
      "Optimal solution found.\n",
      "14: -4.0087e+04 -4.0087e+04  3e-01  4e-05  8e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15: -4.0087e+04 -4.0087e+04  1e-01  2e-05  3e-07\n",
      "16: -4.0087e+04 -4.0087e+04  3e-02  3e-06  6e-08\n",
      "17: -4.0087e+04 -4.0087e+04  3e-02  2e-06  3e-08\n",
      "18: -4.0087e+04 -4.0087e+04  7e-03  3e-07  6e-09\n",
      "19: -4.0087e+04 -4.0087e+04  6e-03  2e-07  3e-09\n",
      "20: -4.0087e+04 -4.0087e+04  2e-03  4e-08  8e-10\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.1836e+04 -4.1556e+04  2e+05  5e+02  7e+00\n",
      " 1: -4.1880e+04 -6.9680e+04  3e+04  9e+01  1e+00\n",
      " 2: -4.2011e+04 -5.7400e+04  2e+04  3e+01  5e-01\n",
      " 3: -4.2060e+04 -4.8688e+04  7e+03  1e+01  2e-01\n",
      " 4: -4.2046e+04 -4.5888e+04  4e+03  5e+00  7e-02\n",
      " 5: -4.2018e+04 -4.3266e+04  1e+03  1e+00  2e-02\n",
      " 6: -4.2016e+04 -4.2711e+04  7e+02  4e-01  6e-03\n",
      " 7: -4.2048e+04 -4.2433e+04  4e+02  2e-01  2e-03\n",
      " 8: -4.2186e+04 -4.2313e+04  1e+02  5e-02  7e-04\n",
      " 9: -4.2247e+04 -4.2295e+04  5e+01  1e-02  2e-04\n",
      "     pcost       dcost       gap    pres   dres10: -4.2265e+04 -4.2292e+04  3e+01  4e-03  6e-05\n",
      "\n",
      " 0: -3.9620e+04 -3.9418e+04  1e+05  4e+02  7e+00\n",
      " 1: -3.9607e+04 -4.8984e+04  1e+04  3e+01  6e-01\n",
      "11: -4.2281e+04 -4.2291e+04  1e+01  1e-03  2e-05\n",
      " 2: -3.9619e+04 -4.1906e+04  2e+03  6e+00  1e-01\n",
      "12: -4.2285e+04 -4.2290e+04  5e+00  4e-04  5e-06\n",
      " 3: -3.9611e+04 -4.0093e+04  5e+02  9e-01  2e-02\n",
      "13: -4.2288e+04 -4.2289e+04  1e+00  8e-05  1e-06\n",
      " 4: -3.9644e+04 -3.9742e+04  1e+02  9e-02  2e-03\n",
      "14: -4.2289e+04 -4.2289e+04  5e-01  1e-05  2e-07\n",
      " 5: -3.9689e+04 -3.9713e+04  2e+01  9e-04  2e-05\n",
      "15: -4.2289e+04 -4.2289e+04  2e-01  4e-06  6e-08\n",
      " 6: -3.9702e+04 -3.9712e+04  1e+01  3e-04  6e-06\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.9883e+04 -3.9673e+04  1e+05  3e+02  7e+00\n",
      "16: -4.2289e+04 -4.2289e+04  1e-01  1e-06  2e-08\n",
      " 7: -3.9708e+04 -3.9712e+04  3e+00  6e-05  1e-06\n",
      " 1: -3.9898e+04 -5.0368e+04  1e+04  3e+01  7e-01\n",
      " 8: -3.9710e+04 -3.9711e+04  1e+00  1e-05  3e-0717: -4.2289e+04 -4.2289e+04  4e-02  3e-07  5e-09\n",
      "\n",
      " 9: -3.9711e+04 -3.9711e+04  4e-01  2e-06  5e-08\n",
      " 2: -3.9926e+04 -4.1255e+04  1e+03  3e+00  7e-02\n",
      "18: -4.2289e+04 -4.2289e+04  3e-02  2e-07  3e-09\n",
      "10: -3.9711e+04 -3.9711e+04  2e-01  9e-07  2e-08\n",
      "19: -4.2289e+04 -4.2289e+04  9e-03  4e-08  7e-10\n",
      "Optimal solution found.\n",
      " 3: -3.9934e+04 -4.0184e+04  3e+02  5e-01  1e-02\n",
      "11: -3.9711e+04 -3.9711e+04  6e-02  2e-07  4e-09\n",
      " 4: -3.9942e+04 -4.0025e+04  8e+01  5e-02  9e-04\n",
      "12: -3.9711e+04 -3.9711e+04  5e-02  1e-07  2e-09\n",
      "13: -3.9711e+04 -3.9711e+04  1e-02  2e-08  5e-10\n",
      "Optimal solution found.\n",
      " 5: -3.9984e+04 -4.0011e+04  3e+01  1e-02  3e-04\n",
      " 6: -3.9993e+04 -4.0008e+04  1e+01  6e-03  1e-04\n",
      " 7: -4.0001e+04 -4.0006e+04  5e+00  2e-03  3e-05\n",
      " 8: -4.0003e+04 -4.0005e+04  2e+00  4e-04  9e-06\n",
      " 9: -4.0004e+04 -4.0005e+04  9e-01  1e-04  3e-06\n",
      "10: -4.0005e+04 -4.0005e+04  4e-01  4e-05  8e-07\n",
      "11: -4.0005e+04 -4.0005e+04  1e-01  1e-05  2e-07\n",
      "12: -4.0005e+04 -4.0005e+04  5e-02  3e-06  5e-08\n",
      "13: -4.0005e+04 -4.0005e+04  2e-02  4e-07  9e-09\n",
      "14: -4.0005e+04 -4.0005e+04  1e-02  3e-07  6e-09\n",
      "15: -4.0005e+04 -4.0005e+04  4e-03  7e-08  1e-09\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.0063e+04 -3.9827e+04  1e+05  4e+02  7e+00\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.9988e+04 -3.9630e+04  1e+05  4e+02  8e+00\n",
      " 1: -4.0092e+04 -5.4760e+04  2e+04  5e+01  9e-01\n",
      " 2: -4.0139e+04 -4.5812e+04  6e+03  2e+01  3e-01\n",
      " 1: -4.0063e+04 -4.9129e+04  1e+04  3e+01  6e-01\n",
      " 2: -4.0090e+04 -4.0948e+04  9e+02  2e+00  5e-02\n",
      " 3: -4.0136e+04 -4.5322e+04  5e+03  1e+01  2e-01\n",
      " 3: -4.0098e+04 -4.0289e+04  2e+02  4e-01  8e-03\n",
      " 4: -4.0148e+04 -4.1470e+04  1e+03  3e+00  5e-02\n",
      " 4: -4.0127e+04 -4.0173e+04  5e+01  4e-03  8e-05\n",
      " 5: -4.0139e+04 -4.0970e+04  8e+02  1e+00  2e-02\n",
      " 5: -4.0156e+04 -4.0170e+04  1e+01  9e-04  2e-05\n",
      " 6: -4.0130e+04 -4.0688e+04  6e+02  7e-01  1e-02\n",
      " 6: -4.0161e+04 -4.0169e+04  8e+00  4e-04  1e-05\n",
      " 7: -4.0152e+04 -4.0435e+04  3e+02  3e-01  5e-03\n",
      " 7: -4.0165e+04 -4.0168e+04  3e+00  1e-04  3e-06\n",
      " 8: -4.0153e+04 -4.0399e+04  2e+02  2e-01  4e-03\n",
      " 8: -4.0166e+04 -4.0168e+04  2e+00  6e-05  1e-06\n",
      " 9: -4.0218e+04 -4.0293e+04  7e+01  4e-02  8e-04\n",
      " 9: -4.0167e+04 -4.0168e+04  8e-01  2e-05  5e-07\n",
      "10: -4.0236e+04 -4.0284e+04  5e+01  2e-02  4e-04\n",
      "10: -4.0167e+04 -4.0167e+04  3e-01  5e-06  1e-0711: -4.0265e+04 -4.0278e+04  1e+01  5e-03  1e-04\n",
      "\n",
      "12: -4.0272e+04 -4.0277e+04  5e+00  1e-03  2e-05\n",
      "11: -4.0167e+04 -4.0167e+04  2e-01  2e-06  4e-08\n",
      "13: -4.0274e+04 -4.0276e+04  2e+00  4e-04  8e-06\n",
      "12: -4.0167e+04 -4.0167e+04  4e-02  2e-07  3e-09\n",
      "13: -4.0167e+04 -4.0167e+04  4e-02  1e-07  3e-09\n",
      "14: -4.0275e+04 -4.0276e+04  1e+00  2e-04  4e-06\n",
      "14: -4.0167e+04 -4.0167e+04  1e-02  2e-08  5e-10\n",
      "Optimal solution found.\n",
      "15: -4.0275e+04 -4.0276e+04  7e-01  1e-04  2e-06\n",
      "16: -4.0276e+04 -4.0276e+04  3e-01  4e-05  8e-07\n",
      "17: -4.0276e+04 -4.0276e+04  1e-01  1e-05  3e-07\n",
      "18: -4.0276e+04 -4.0276e+04  5e-02  4e-06  7e-08\n",
      "19: -4.0276e+04 -4.0276e+04  2e-02  6e-07  1e-08\n",
      "20: -4.0276e+04 -4.0276e+04  8e-03  2e-07  3e-09\n",
      "21: -4.0276e+04 -4.0276e+04  3e-03  4e-08  8e-10\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.1300e+04 -3.9923e+04  4e+05  6e+02  2e+01\n",
      " 1: -4.1889e+04 -1.0688e+05  8e+04  1e+02  4e+00\n",
      " 2: -4.2053e+04 -6.2848e+04  2e+04  3e+01  8e-01\n",
      " 3: -4.2083e+04 -5.1777e+04  1e+04  1e+01  3e-01\n",
      " 4: -4.2068e+04 -4.7417e+04  5e+03  5e+00  1e-01\n",
      " 5: -4.2022e+04 -4.5562e+04  4e+03  2e+00  7e-02\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 6: -4.2061e+04 -4.3000e+04  9e+02  5e-01  2e-02\n",
      " 0: -4.0763e+04 -4.0568e+04  1e+05  3e+02  8e+00\n",
      " 1: -4.0756e+04 -4.9789e+04  1e+04  3e+01  6e-01\n",
      " 7: -4.2058e+04 -4.2645e+04  6e+02  2e-01  7e-03\n",
      " 2: -4.0769e+04 -4.1850e+04  1e+03  3e+00  6e-02\n",
      " 8: -4.2083e+04 -4.2526e+04  4e+02  2e-01  4e-03\n",
      " 3: -4.0771e+04 -4.1006e+04  2e+02  4e-01  1e-02\n",
      " 9: -4.2236e+04 -4.2364e+04  1e+02  4e-02  1e-03\n",
      " 4: -4.0781e+04 -4.0851e+04  7e+01  5e-03  1e-04\n",
      "10: -4.2279e+04 -4.2346e+04  7e+01  2e-02  4e-04\n",
      " 5: -4.0810e+04 -4.0849e+04  4e+01  3e-03  6e-05\n",
      " 6: -4.0820e+04 -4.0848e+04  3e+01  2e-03  4e-05\n",
      "11: -4.2305e+04 -4.2338e+04  3e+01  6e-03  2e-04\n",
      " 7: -4.0834e+04 -4.0847e+04  1e+01  7e-04  2e-05\n",
      "12: -4.2321e+04 -4.2333e+04  1e+01  2e-03  5e-05\n",
      " 8: -4.0841e+04 -4.0846e+04  5e+00  2e-04  4e-06\n",
      "13: -4.2326e+04 -4.2332e+04  6e+00  6e-04  2e-05\n",
      " 9: -4.0843e+04 -4.0845e+04  2e+00  5e-05  1e-06\n",
      "10: -4.0844e+04 -4.0845e+04  7e-01  1e-05  3e-07\n",
      "14: -4.2328e+04 -4.2331e+04  3e+00  3e-04  8e-06\n",
      "11: -4.0844e+04 -4.0845e+04  6e-01  1e-05  2e-07\n",
      "15: -4.2329e+04 -4.2330e+04  2e+00  1e-04  4e-06\n",
      "12: -4.0845e+04 -4.0845e+04  3e-01  3e-06  8e-0816: -4.2329e+04 -4.2330e+04  7e-01  3e-05  8e-07\n",
      "\n",
      "13: -4.0845e+04 -4.0845e+04  1e-01  1e-06  2e-0817: -4.2329e+04 -4.2330e+04  2e-01  6e-06  2e-07\n",
      "\n",
      "14: -4.0845e+04 -4.0845e+04  8e-02  5e-07  1e-08\n",
      "18: -4.2329e+04 -4.2330e+04  1e-01  2e-06  5e-08\n",
      "15: -4.0845e+04 -4.0845e+04  2e-02  1e-07  3e-0919: -4.2330e+04 -4.2330e+04  3e-02  4e-07  1e-08\n",
      "\n",
      "16: -4.0845e+04 -4.0845e+04  2e-02  8e-08  2e-09\n",
      "Optimal solution found.\n",
      "20: -4.2330e+04 -4.2330e+04  3e-02  3e-07  8e-09\n",
      "21: -4.2330e+04 -4.2330e+04  9e-03  5e-08  2e-09\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.0132e+04 -4.0041e+04  1e+05  3e+02  6e+00\n",
      " 1: -4.0076e+04 -4.4457e+04  5e+03  1e+01  3e-01\n",
      " 2: -4.0078e+04 -4.0468e+04  4e+02  1e+00  2e-02\n",
      " 3: -4.0096e+04 -4.0149e+04  5e+01  1e-02  2e-04\n",
      " 4: -4.0134e+04 -4.0147e+04  1e+01  2e-03  4e-05\n",
      " 5: -4.0140e+04 -4.0146e+04  6e+00  9e-04  2e-05\n",
      " 6: -4.0144e+04 -4.0146e+04  2e+00  2e-04  5e-06\n",
      " 7: -4.0145e+04 -4.0146e+04  1e+00  1e-04  2e-06\n",
      " 8: -4.0145e+04 -4.0146e+04  4e-01  3e-05  5e-07\n",
      " 9: -4.0145e+04 -4.0146e+04  2e-01  8e-06  2e-07\n",
      "10: -4.0146e+04 -4.0146e+04  5e-02  2e-06  3e-08\n",
      "11: -4.0146e+04 -4.0146e+04  4e-02  1e-06  2e-08\n",
      "12: -4.0146e+04 -4.0146e+04  1e-02  2e-07  4e-09\n",
      "13: -4.0146e+04 -4.0146e+04  7e-03  9e-08  2e-09\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.0040e+04 -3.9868e+04  1e+05  3e+02  8e+00\n",
      " 1: -4.0033e+04 -4.9902e+04  1e+04  3e+01  7e-01\n",
      " 2: -4.0058e+04 -4.1793e+04  2e+03  5e+00  1e-01\n",
      " 3: -4.0059e+04 -4.0485e+04  4e+02  9e-01  2e-02\n",
      " 4: -4.0059e+04 -4.0186e+04  1e+02  1e-01  2e-03\n",
      " 5: -4.0117e+04 -4.0147e+04  3e+01  2e-02  5e-04\n",
      " 6: -4.0120e+04 -4.0146e+04  3e+01  2e-02  4e-04\n",
      " 7: -4.0129e+04 -4.0143e+04  1e+01  1e-02  2e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8: -4.0137e+04 -4.0142e+04  5e+00  3e-03  7e-05\n",
      " 9: -4.0140e+04 -4.0142e+04  2e+00  7e-04  1e-05\n",
      "10: -4.0141e+04 -4.0141e+04  8e-01  3e-04  6e-06\n",
      "11: -4.0141e+04 -4.0141e+04  4e-01  1e-04  2e-06\n",
      "12: -4.0141e+04 -4.0141e+04  2e-01  5e-05  1e-06\n",
      "13: -4.0141e+04 -4.0141e+04  8e-02  2e-05  3e-07\n",
      "14: -4.0141e+04 -4.0141e+04  3e-02  5e-06  1e-07\n",
      "15: -4.0141e+04 -4.0141e+04  9e-03  5e-07  1e-08\n",
      "16: -4.0141e+04 -4.0141e+04  7e-03  3e-07  7e-09\n",
      "17: -4.0141e+04 -4.0141e+04  2e-03  7e-08  2e-09\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.0998e+04 -4.0714e+04  1e+05  4e+02  8e+00\n",
      " 1: -4.1024e+04 -5.4836e+04  2e+04  4e+01  9e-01\n",
      " 2: -4.1039e+04 -4.3269e+04  2e+03  5e+00  1e-01\n",
      " 3: -4.1036e+04 -4.1695e+04  7e+02  1e+00  3e-02\n",
      " 4: -4.1045e+04 -4.1210e+04  2e+02  2e-01  3e-03\n",
      " 5: -4.1094e+04 -4.1144e+04  5e+01  2e-03  3e-05\n",
      " 6: -4.1119e+04 -4.1143e+04  2e+01  4e-04  1e-05\n",
      " 7: -4.1134e+04 -4.1142e+04  8e+00  1e-04  3e-06\n",
      " 8: -4.1137e+04 -4.1141e+04  4e+00  3e-05  6e-07\n",
      " 9: -4.1139e+04 -4.1141e+04  1e+00  6e-06  1e-07\n",
      "10: -4.1140e+04 -4.1141e+04  1e+00  4e-06  9e-08\n",
      "11: -4.1140e+04 -4.1140e+04  3e-01  7e-07  2e-08\n",
      "12: -4.1140e+04 -4.1140e+04  1e-01  1e-07  3e-09\n",
      "13: -4.1140e+04 -4.1140e+04  3e-02  2e-08  6e-10\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.0308e+04 -3.9598e+04  2e+05  4e+02  1e+01\n",
      " 1: -4.0675e+04 -6.2696e+04  3e+04  6e+01  2e+00\n",
      " 2: -4.0992e+04 -4.8271e+04  8e+03  1e+01  5e-01\n",
      " 3: -4.1036e+04 -4.2617e+04  2e+03  2e+00  9e-02\n",
      " 4: -4.1039e+04 -4.1522e+04  5e+02  6e-01  2e-02\n",
      " 5: -4.1050e+04 -4.1199e+04  1e+02  6e-02  2e-03\n",
      " 6: -4.1112e+04 -4.1165e+04  5e+01  2e-02  7e-04\n",
      " 7: -4.1133e+04 -4.1158e+04  2e+01  7e-03  3e-04\n",
      " 8: -4.1146e+04 -4.1154e+04  7e+00  2e-03  6e-05\n",
      " 9: -4.1148e+04 -4.1153e+04  4e+00  7e-04  3e-05\n",
      "10: -4.1150e+04 -4.1152e+04  2e+00  2e-04  9e-06\n",
      "11: -4.1150e+04 -4.1151e+04  6e-01  8e-05  3e-06\n",
      "12: -4.1151e+04 -4.1151e+04  3e-01  3e-05  1e-06\n",
      "13: -4.1151e+04 -4.1151e+04  1e-01  9e-06  3e-07\n",
      "14: -4.1151e+04 -4.1151e+04  3e-02  1e-06  5e-08\n",
      "15: -4.1151e+04 -4.1151e+04  1e-02  2e-07  8e-09\n",
      "16: -4.1151e+04 -4.1151e+04  7e-03  8e-08  3e-09\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.1697e+04 -4.1578e+04  1e+05  3e+02  6e+00\n",
      " 1: -4.1653e+04 -5.0976e+04  1e+04  3e+01  6e-01\n",
      " 2: -4.1657e+04 -4.3224e+04  2e+03  4e+00  8e-02\n",
      " 3: -4.1645e+04 -4.2095e+04  5e+02  8e-01  1e-02\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.1237e+04 -4.1000e+04  1e+05  4e+02  9e+00\n",
      " 4: -4.1647e+04 -4.1791e+04  1e+02  1e-01  2e-03\n",
      " 1: -4.1279e+04 -6.4768e+04  3e+04  8e+01  2e+00\n",
      " 5: -4.1710e+04 -4.1747e+04  4e+01  7e-03  1e-04\n",
      " 2: -4.1389e+04 -5.3367e+04  1e+04  2e+01  6e-01\n",
      " 3: -4.1428e+04 -4.5454e+04  4e+03  6e+00  2e-01\n",
      " 6: -4.1727e+04 -4.1745e+04  2e+01  2e-03  3e-05\n",
      " 4: -4.1422e+04 -4.3434e+04  2e+03  2e+00  6e-02\n",
      " 7: -4.1740e+04 -4.1744e+04  4e+00  2e-04  4e-06\n",
      " 5: -4.1400e+04 -4.2562e+04  1e+03  1e+00  3e-02\n",
      " 8: -4.1741e+04 -4.1744e+04  3e+00  1e-04  2e-06\n",
      " 6: -4.1379e+04 -4.2215e+04  8e+02  6e-01  1e-02\n",
      " 9: -4.1743e+04 -4.1744e+04  6e-01  2e-05  3e-07\n",
      " 7: -4.1413e+04 -4.1772e+04  4e+02  2e-01  4e-03\n",
      "10: -4.1743e+04 -4.1743e+04  3e-01  2e-06  4e-08\n",
      " 8: -4.1454e+04 -4.1640e+04  2e+02  4e-02  1e-03\n",
      "11: -4.1743e+04 -4.1743e+04  8e-02  4e-07  8e-09\n",
      " 9: -4.1492e+04 -4.1618e+04  1e+02  2e-02  5e-04\n",
      "12: -4.1743e+04 -4.1743e+04  7e-02  3e-07  6e-09\n",
      "10: -4.1566e+04 -4.1604e+04  4e+01  6e-03  1e-04\n",
      "13: -4.1743e+04 -4.1743e+04  3e-02  1e-07  2e-09\n",
      "11: -4.1584e+04 -4.1602e+04  2e+01  2e-03  5e-05\n",
      "14: -4.1743e+04 -4.1743e+04  1e-02  3e-08  5e-10\n",
      "Optimal solution found.\n",
      "12: -4.1593e+04 -4.1601e+04  8e+00  8e-04  2e-05\n",
      "13: -4.1596e+04 -4.1600e+04  4e+00  3e-04  8e-06\n",
      "14: -4.1598e+04 -4.1600e+04  2e+00  1e-04  3e-06\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.1058e+04 -4.0276e+04  2e+05  4e+02  9e+00\n",
      "15: -4.1599e+04 -4.1599e+04  5e-01  1e-05  4e-07\n",
      " 1: -4.1418e+04 -6.2856e+04  3e+04  6e+01  1e+00\n",
      "16: -4.1599e+04 -4.1599e+04  3e-01  8e-06  2e-07\n",
      " 2: -4.1648e+04 -4.6182e+04  5e+03  1e+01  2e-01\n",
      "17: -4.1599e+04 -4.1599e+04  9e-02  2e-06  4e-08\n",
      " 3: -4.1675e+04 -4.2784e+04  1e+03  2e+00  4e-02\n",
      "18: -4.1599e+04 -4.1599e+04  7e-02  9e-07  2e-08\n",
      " 4: -4.1675e+04 -4.1979e+04  3e+02  3e-01  7e-03\n",
      "19: -4.1599e+04 -4.1599e+04  2e-02  2e-07  6e-09\n",
      " 5: -4.1705e+04 -4.1794e+04  9e+01  3e-03  7e-05\n",
      "20: -4.1599e+04 -4.1599e+04  2e-02  1e-07  4e-09\n",
      " 6: -4.1752e+04 -4.1788e+04  4e+01  1e-03  2e-05\n",
      "21: -4.1599e+04 -4.1599e+04  5e-03  3e-08  9e-10\n",
      "Optimal solution found.\n",
      " 7: -4.1766e+04 -4.1784e+04  2e+01  5e-04  1e-05\n",
      " 8: -4.1773e+04 -4.1782e+04  9e+00  2e-04  5e-06\n",
      " 9: -4.1776e+04 -4.1781e+04  5e+00  9e-05  2e-06\n",
      "10: -4.1778e+04 -4.1780e+04  2e+00  3e-05  7e-07\n",
      "11: -4.1779e+04 -4.1780e+04  6e-01  3e-06  7e-08\n",
      "12: -4.1779e+04 -4.1780e+04  2e-01  6e-07  1e-08\n",
      "13: -4.1779e+04 -4.1779e+04  7e-02  1e-07  2e-09\n",
      "14: -4.1779e+04 -4.1779e+04  2e-02  1e-08  4e-10\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.2025e+04 -4.1935e+04  1e+05  3e+02  7e+00\n",
      " 1: -4.1969e+04 -4.9619e+04  8e+03  3e+01  5e-01\n",
      " 2: -4.1964e+04 -4.4222e+04  2e+03  6e+00  1e-01\n",
      " 3: -4.1970e+04 -4.2366e+04  4e+02  8e-01  2e-02\n",
      " 4: -4.1989e+04 -4.2067e+04  8e+01  2e-02  5e-04\n",
      " 5: -4.2037e+04 -4.2060e+04  2e+01  4e-03  9e-05\n",
      " 6: -4.2047e+04 -4.2059e+04  1e+01  2e-03  4e-05\n",
      " 7: -4.2052e+04 -4.2058e+04  6e+00  1e-03  2e-05\n",
      " 8: -4.2055e+04 -4.2058e+04  3e+00  5e-04  9e-06\n",
      " 9: -4.2056e+04 -4.2058e+04  2e+00  2e-04  4e-06\n",
      "10: -4.2057e+04 -4.2058e+04  7e-01  7e-05  2e-06\n",
      "11: -4.2057e+04 -4.2058e+04  3e-01  3e-05  5e-07\n",
      "12: -4.2057e+04 -4.2057e+04  1e-01  8e-06  2e-07\n",
      "13: -4.2057e+04 -4.2057e+04  6e-02  3e-06  6e-08\n",
      "14: -4.2057e+04 -4.2057e+04  2e-02  5e-07  1e-08\n",
      "15: -4.2057e+04 -4.2057e+04  8e-03  1e-07  2e-09\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.1408e+04 -4.0821e+04  1e+05  4e+02  9e+00\n",
      " 1: -4.1666e+04 -5.5093e+04  2e+04  4e+01  1e+00\n",
      " 2: -4.1865e+04 -4.5068e+04  3e+03  7e+00  2e-01\n",
      " 3: -4.1884e+04 -4.3422e+04  2e+03  3e+00  7e-02\n",
      " 4: -4.1876e+04 -4.3132e+04  1e+03  2e+00  4e-02\n",
      " 5: -4.1892e+04 -4.2445e+04  6e+02  7e-01  2e-02\n",
      " 6: -4.1888e+04 -4.2388e+04  5e+02  6e-01  1e-02\n",
      " 7: -4.1920e+04 -4.2085e+04  2e+02  1e-01  3e-03\n",
      " 8: -4.1936e+04 -4.2051e+04  1e+02  7e-02  2e-03\n",
      " 9: -4.1985e+04 -4.2017e+04  3e+01  2e-02  4e-04\n",
      "10: -4.1988e+04 -4.2015e+04  3e+01  1e-02  3e-04\n",
      "11: -4.1996e+04 -4.2011e+04  1e+01  6e-03  2e-04\n",
      "12: -4.2000e+04 -4.2009e+04  9e+00  3e-03  8e-05\n",
      "13: -4.2002e+04 -4.2007e+04  5e+00  1e-03  4e-05\n",
      "14: -4.2004e+04 -4.2006e+04  2e+00  3e-04  8e-06\n",
      "15: -4.2005e+04 -4.2006e+04  6e-01  4e-05  9e-07\n",
      "16: -4.2005e+04 -4.2005e+04  3e-01  1e-05  2e-07\n",
      "17: -4.2005e+04 -4.2005e+04  1e-01  3e-06  7e-08\n",
      "18: -4.2005e+04 -4.2005e+04  9e-02  2e-06  5e-08\n",
      "19: -4.2005e+04 -4.2005e+04  3e-02  5e-07  1e-08\n",
      "20: -4.2005e+04 -4.2005e+04  1e-02  1e-07  3e-09\n",
      "21: -4.2005e+04 -4.2005e+04  4e-03  2e-08  5e-10\n",
      "Optimal solution found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-0383d79aeb3c>:44: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data = data.reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.3031e+04 -4.2956e+04  9e+04  3e+02  5e+00\n",
      " 1: -4.2962e+04 -4.5435e+04  3e+03  8e+00  1e-01\n",
      " 2: -4.2966e+04 -4.3254e+04  3e+02  8e-01  1e-02\n",
      " 3: -4.2981e+04 -4.3037e+04  6e+01  8e-03  1e-04\n",
      " 4: -4.3015e+04 -4.3035e+04  2e+01  3e-03  4e-05\n",
      " 5: -4.3022e+04 -4.3035e+04  1e+01  1e-03  2e-05\n",
      " 6: -4.3029e+04 -4.3035e+04  6e+00  6e-04  9e-06\n",
      " 7: -4.3033e+04 -4.3034e+04  2e+00  1e-04  2e-06\n",
      " 8: -4.3033e+04 -4.3034e+04  1e+00  6e-05  1e-06\n",
      " 9: -4.3034e+04 -4.3034e+04  3e-01  1e-05  2e-07\n",
      "10: -4.3034e+04 -4.3034e+04  2e-01  8e-06  1e-07\n",
      "11: -4.3034e+04 -4.3034e+04  6e-02  2e-06  3e-08\n",
      "12: -4.3034e+04 -4.3034e+04  3e-02  4e-07  7e-09\n",
      "13: -4.3034e+04 -4.3034e+04  7e-03  7e-08  1e-09\n",
      "Optimal solution found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-7d090e579a5c>:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  NewDF[columnName] = data[parameter].ewm(alpha = alpha).mean()\n",
      "<ipython-input-22-faa363d1e944>:29: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[columnName] = diffData\n",
      "<ipython-input-26-56396ebd70d1>:141: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_CONSENSYS_interpolated_NORM_SMA(60s)\"] = all_data[\"HR_CONSENSYS_NORM_SMA(60.0s)\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:142: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save['HR_CONSENSYS_interpolated_NORM_EWMA(0.01)'] = all_data['HR_CONSENSYS_NORM_EWMA(0.01)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:143: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save['HR_CONSENSYS_interpolated_NORM_EWMA(0.001)'] = all_data['HR_CONSENSYS_NORM_EWMA(0.001)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:144: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save['HR_CONSENSYS_interpolated_NORM_EWMA(0.0001)'] = all_data['HR_CONSENSYS_NORM_EWMA(0.0001)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:146: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"IBI_CONSENSYS[ms]\"] = all_data['PPG_IBI'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:147: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"IBI_CONSENSYS_interpolated[ms]\"] = all_data['IBI_CONSENSYS_interpolated'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy[BPM]\"] = all_data[\"bpm\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:150: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_SMA(1s)\"] = all_data['bpm_SMA(1.0s)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:151: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_SMA(3s)\"] = all_data[\"bpm_SMA(3.0s)\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:152: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_SMA(60s)\"] = all_data['bpm_SMA(60.0s)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:153: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save['HR_HeartPy_EWMA(0.01)'] = all_data['bpm_EWMA(0.01)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:154: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save['HR_HeartPy_EWMA(0.001)'] = all_data['bpm_EWMA(0.001)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:155: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save['HR_HeartPy_EWMA(0.0001)'] = all_data['bpm_EWMA(0.0001)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:157: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_interpolated[BPM]\"] = all_data[\"HR_HeartPy_interpolated\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:158: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_interpolated_DIFF(1)\"] = all_data[\"HR_HeartPy_interpolated_DIFF(1)\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:160: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_interpolated_SMA(1s)\"] = all_data[\"HR_HeartPy_interpolated_SMA(1.0s)\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:161: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_interpolated_SMA(3s)\"] = all_data[\"HR_HeartPy_interpolated_SMA(3.0s)\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:162: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_interpolated_SMA(60s)\"] = all_data['HR_HeartPy_interpolated_SMA(60.0s)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:163: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save['HR_HeartPy_interpolated_EWMA(0.01)'] = all_data['HR_HeartPy_interpolated_EWMA(0.01)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:164: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save['HR_HeartPy_interpolated_EWMA(0.001)'] = all_data['HR_HeartPy_interpolated_EWMA(0.001)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:165: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save['HR_HeartPy_interpolated_EWMA(0.0001)'] = all_data['HR_HeartPy_interpolated_EWMA(0.0001)'].copy()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-56396ebd70d1>:167: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_interpolated_STD\"] = all_data[\"HR_HP_STD\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:168: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_interpolated_STD_DIFF(1)\"] = all_data[\"HR_HP_STD_DIFF(1)\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:170: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_interpolated_STD_SMA(1s)\"] = all_data[\"HR_HP_STD_SMA(1.0s)\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:171: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_interpolated_STD_SMA(3s)\"] = all_data[\"HR_HP_STD_SMA(3.0s)\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:172: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_interpolated_STD_SMA(60s)\"] = all_data['HR_HP_STD_SMA(60.0s)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:173: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save['HR_HeartPy_interpolated_STD_EWMA(0.01)'] = all_data['HR_HP_STD_EWMA(0.01)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:174: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save['HR_HeartPy_interpolated_STD_EWMA(0.001)'] = all_data['HR_HP_STD_EWMA(0.001)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save['HR_HeartPy_interpolated_STD_EWMA(0.0001)'] = all_data['HR_HP_STD_EWMA(0.0001)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:177: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_interpolated_NORM\"] = all_data[\"HR_HP_NORM\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_interpolated_NORM_DIFF(1)\"] = all_data[\"HR_HP_NORM_DIFF(1)\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_interpolated_NORM_SMA(1s)\"] = all_data[\"HR_HP_NORM_SMA(1.0s)\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:181: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_interpolated_NORM_SMA(3s)\"] = all_data[\"HR_HP_NORM_SMA(3.0s)\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:182: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"HR_HeartPy_interpolated_NORM_SMA(60s)\"] = all_data['HR_HP_NORM_SMA(60.0s)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:183: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save['HR_HeartPy_interpolated_NORM_EWMA(0.01)'] = all_data['HR_HP_NORM_EWMA(0.01)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save['HR_HeartPy_interpolated_NORM_EWMA(0.001)'] = all_data['HR_HP_NORM_EWMA(0.001)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:185: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save['HR_HeartPy_interpolated_NORM_EWMA(0.0001)'] = all_data['HR_HP_NORM_EWMA(0.0001)'].copy()\n",
      "<ipython-input-26-56396ebd70d1>:187: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"IBI_HeartPy[ms]\"] = all_data[\"ibi\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:188: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"IBI_HeartPy_interpolated[ms]\"] = all_data[\"IBI_HeartPy_interpolated\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:190: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"SDNN_HeartPy[ms]\"] = all_data[\"sdnn\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:191: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"SDNN_HeartPy_interpolated[ms]\"] = all_data[\"SDNN_HeartPy_interpolated\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:193: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"SDSD_HeartPy[ms]\"] = all_data[\"sdsd\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:194: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"SDSD_HeartPy_interpolated[ms]\"] = all_data[\"SDSD_HeartPy_interpolated\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:196: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"RMSSD_HeartPy[ms]\"] = all_data[\"rmssd\"]\n",
      "<ipython-input-26-56396ebd70d1>:197: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"RMSSD_HeartPy_interpolated[ms]\"] = all_data[\"RMSSD_HeartPy_interpolated\"]\n",
      "<ipython-input-26-56396ebd70d1>:199: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"pNN20_HeartPy[%]\"] = all_data[\"pnn20\"].copy()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-56396ebd70d1>:200: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"pNN20_HeartPy_interpolated[%]\"] = all_data[\"pNN20_HeartPy_interpolated\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"pNN50_HeartPy[%]\"] = all_data[\"pnn50\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:203: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"pNN50_HeartPy_interpolated[%]\"] = all_data[\"pNN50_HeartPy_interpolated\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:205: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"Pressure[kPa]\"] = all_data[\"Pressure\"].copy()\n",
      "<ipython-input-26-56396ebd70d1>:206: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_to_save[\"Temperature[C]\"] = all_data[\"Temperature\"].copy()\n"
     ]
    }
   ],
   "source": [
    "qualityScores, noTailsDataSet, demographicDataSet, runTimeSet = processAll(AllParticipantsGr, save = False, testMode = True, multi = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c349849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(qualityScores))\n",
    "print(qualityScores)\n",
    "print(f\"average quality score = {sum(qualityScores)/len(qualityScores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06de5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(qualityScores,64)\n",
    "plt.ylabel('number of samples')\n",
    "plt.xlabel('missing HR data %')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d546d94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentageCalcuator(below, listToCheck):\n",
    "    sumOfBelow = len([x for x in listToCheck if x < below])\n",
    "    totalN = len(listToCheck)\n",
    "    print(f\"{sumOfBelow} samples with errors below {below}% out of {totalN} samples = {sumOfBelow/totalN * 100}% of the sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d407b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0, .5, 1, 3, 5, 10, 20, 25, 50, 75, 80, 90]:\n",
    "    percentageCalcuator(i, qualityScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb1a3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(noTailsDataSet) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa8be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(demographicDataSet))\n",
    "print(demographicDataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61a3057",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(runTimeSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c68960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTest():\n",
    "    participantID = '0002'\n",
    "    participantGR = 2\n",
    "    #Virtual Enviornment Data\n",
    "    environment_data = pullEnvironmentData(participantID,participantGR)\n",
    "    print(environment_data.head())\n",
    "    physiologicaData = pullPhysioData(participantID,participantGR)\n",
    "    print(physiologicaData.head())\n",
    "    #return environment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd7f36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b7afa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#runTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674285b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eae8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"total processing time = {runTimeSet}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02387da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"average processing time = {sum(runTimeSet)/len(runTimeSet)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b05f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"average ambiguous HR data = {sum(qualityScores)/len(qualityScores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418f838b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e30452",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNames = noTailsDataSet[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ec87ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(columnNames[5:22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8804c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "noTailsDataSet[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328341f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e019f10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "breakStarts = [dataFrame['Phase'][dataFrame['Phase'] == 'break'].index[0] for dataFrame in noTailsDataSet]\n",
    "print(len(breakStarts))\n",
    "print(breakStarts)\n",
    "breakEnds = [dataFrame['Phase'][dataFrame['Phase'] == 'break'].index[-1] for dataFrame in noTailsDataSet]\n",
    "print(breakEnds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bebaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "breakLenght = lambda start, finish: (finish - start)/60\n",
    "breakLenghts = [breakLenght(key, value) for key, value in zip(breakStarts, breakEnds)]\n",
    "print(breakLenghts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2630d9ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40372ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36b8d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34c24de",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.normaltest(breakLenghts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45723204",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(breakLenghts,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9012a01b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9037fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Within Subject approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd16399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzeThis(column):\n",
    "    print(f\"<+>==================={column}+++++++++++++++++++++++<<<\")\n",
    "    pre = [noTailsDataSet[i].loc[:breakStarts[i]-1, column] for i in range(0, len(noTailsDataSet))]\n",
    "    post = [noTailsDataSet[i].loc[:breakEnds[i], column] for i in range(0, len(noTailsDataSet))]\n",
    "\n",
    "    meanForTS = lambda timeSeries: sum(timeSeries)/len(timeSeries)\n",
    "    meansPre = [meanForTS(item) for item in pre]\n",
    "    meansPost = [meanForTS(item) for item in post]\n",
    "\n",
    "    print(f\"{column} : {stats.shapiro(meansPre)}\")\n",
    "    plt.hist(meansPre,20)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"{column} : {stats.shapiro(meansPost)}\")\n",
    "    plt.hist(meansPost,20)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"{column} : {stats.ttest_rel(meansPre, meansPost)}\")\n",
    "    print(f\"{column} : {stats.wilcoxon(meansPre, meansPost)}\")\n",
    "\n",
    "    #print(stats.ttest_rel(meansPre, meansPost, alternative = \"greater\"))\n",
    "    #print(stats.wilcoxon(meansPre, meansPost, alternative = \"greater\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5765bca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzeThis(\"bpm\") #Skin_Resistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d5f1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in columnNames:\n",
    "    try:\n",
    "        print(column)\n",
    "        analyzeThis(column)\n",
    "    except Exception as e:\n",
    "        print(\"can't count this!\")\n",
    "        print(e)\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e25d46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec0f59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "audioStarts = [dataFrame['Audio_clip'][dataFrame['Audio_clip'] == 'Audio_1'].index[0] for dataFrame in noTailsDataSet]\n",
    "audioEnds = [dataFrame['Audio_clip'][dataFrame['Audio_clip'] == 'Audio_1'].index[-1] for dataFrame in noTailsDataSet]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc043d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(audioStarts)\n",
    "print(audioEnds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8c6759",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(demographicDataSet[1].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32feb268",
   "metadata": {},
   "source": [
    "## Drawing all the columns data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54766c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawThis(column):\n",
    "    from bokeh.models.tools import HoverTool    \n",
    "    \n",
    "    print(f\"<+>==============Drawing={column}+++++++++++++++++++++++<<<\")\n",
    "    \"\"\"\n",
    "    print(demographicDataSet[1].head())\n",
    "    print(\"==============\")\n",
    "    print(noTailsDataSet[1].head())\n",
    "    print(pre[1])\n",
    "    print(\"==============\")\n",
    "    \"\"\"\n",
    "    \n",
    "    pre = [noTailsDataSet[i].loc[:breakStarts[i]-1, column] for i in range(0, len(noTailsDataSet))]\n",
    "    instruction = [noTailsDataSet[i].loc[audioStarts[i]:audioEnds[i]-1, column] for i in range(0, len(noTailsDataSet))]\n",
    "    post = [noTailsDataSet[i].loc[audioEnds[i]:breakEnds[i], column] for i in range(0, len(noTailsDataSet))]\n",
    "    #postAll = [noTailsDataSet[i].loc[:breakEnds[i], column] for i in range(0, len(noTailsDataSet))]\n",
    "\n",
    "    #MEANS not needed now\n",
    "    \"\"\"\n",
    "    meanForTS = lambda timeSeries: sum(timeSeries)/len(timeSeries)\n",
    "    meansPre = [meanForTS(item) for item in pre]\n",
    "    meansPost = [meanForTS(item) for item in post]\n",
    "\n",
    "    print(f\"{column} : {stats.shapiro(meansPre)}\")\n",
    "    plt.hist(meansPre,20)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"{column} : {stats.shapiro(meansPost)}\")\n",
    "    plt.hist(meansPost,20)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"{column} : {stats.ttest_rel(meansPre, meansPost)}\")\n",
    "    print(f\"{column} : {stats.wilcoxon(meansPre, meansPost)}\")\n",
    "    \"\"\"\n",
    "    \n",
    "    ###OTher way through DF\n",
    "    columnNames = [int(demographicDataSet[i].loc[\"ID\"]) for i in range(0, len(demographicDataSet))]\n",
    "    \n",
    "    print(columnNames)\n",
    "    \n",
    "    dfPre = pd.concat(pre, axis = 1, keys = columnNames)\n",
    "    #dfPre = pd.pivot_table(dfPre)\n",
    "    #print(dfPre) \n",
    "    dfInstruction = pd.concat(instruction, axis = 1, keys = columnNames)\n",
    "    #print(dfInstruction)\n",
    "    dfPost = pd.concat(post, axis = 1, keys = columnNames)    \n",
    "    #print(dfInstruction)\n",
    "    \n",
    "    graphPath = \"Graphs\\/\"+ column + \".html\";\n",
    "    output_file(graphPath)\n",
    "    \n",
    "    TOOLTIPS = [\n",
    "        (\"index\", \"$index\"),\n",
    "        (\"(x,y)\", \"($x, $y)\"),\n",
    "        (\"desc\", \"$name\"),\n",
    "    ]\n",
    "    \n",
    "    objectToPlot = figure(\n",
    "        #y_range = (0,1),\n",
    "        #x_range = (0,1),\n",
    "        title = column,\n",
    "        x_axis_label = 'Index[i]',\n",
    "        y_axis_label = column,\n",
    "        plot_width = 1600,\n",
    "        #plot_height = 900,\n",
    "        sizing_mode='scale_width',\n",
    "        tools=[HoverTool()],\n",
    "        tooltips=TOOLTIPS,\n",
    "        #tooltips=\"Data point @x for i has the value @y\",\n",
    "    )\n",
    "    \n",
    "    #dfPre.fillna(method = \"bfill\", inplace = True)\n",
    "    #dfPre.fillna(method = \"ffill\", inplace = True)\n",
    "    \n",
    "    mean = dfPre.mean(axis=1)\n",
    "    std = dfPre.std(axis=1)\n",
    "    n = dfPre.count(axis=1)\n",
    "    \n",
    "    dfPre['mean'] = mean\n",
    "    dfPre['std'] = std\n",
    "    dfPre['n'] = n\n",
    "    \n",
    "    mean = dfInstruction.mean(axis=1)\n",
    "    std = dfInstruction.std(axis=1)\n",
    "    n = dfInstruction.count(axis=1)\n",
    "    \n",
    "    dfInstruction['mean'] = mean\n",
    "    dfInstruction['std'] = std\n",
    "    dfInstruction['n'] = n\n",
    "    \n",
    "    mean = dfPost.mean(axis=1)\n",
    "    std = dfPost.std(axis=1)\n",
    "    n = dfPost.count(axis=1)\n",
    "    \n",
    "    dfPost['mean'] = mean\n",
    "    dfPost['std'] = std\n",
    "    dfPost['n'] = n\n",
    "    \n",
    "    \n",
    "    for colNAme in columnNames:\n",
    "        \n",
    "        \n",
    "        \n",
    "        objectToPlot.line(dfPre[colNAme].index.tolist(), dfPre[colNAme], name = f\"Participant : {colNAme}\", \n",
    "                          legend_label= str(colNAme),\n",
    "                          line_color=\"green\",\n",
    "                          line_width = .3,\n",
    "                          line_alpha = .6,\n",
    "                         )\n",
    "        objectToPlot.line(dfInstruction[colNAme].index.tolist(), dfInstruction[colNAme], name = f\"Participant : {colNAme}\", \n",
    "                          line_color=\"red\",\n",
    "                          line_width = .3,\n",
    "                          line_alpha = .6)\n",
    "        objectToPlot.line(dfPost[colNAme].index.tolist(), dfPost[colNAme], name = f\"Participant : {colNAme}\", \n",
    "                          line_color=\"blue\",\n",
    "                          line_width = .3,\n",
    "                          line_alpha = .6)\n",
    "    \n",
    "    objectToPlot.line(dfPre['mean'].index.tolist(), dfPre['mean'], name = f\"{column} mean at physical environment\",\n",
    "                      line_color=\"green\",\n",
    "                      line_width = 3,\n",
    "                      line_alpha = .8)\n",
    "    objectToPlot.line(dfInstruction['mean'].index.tolist(), dfInstruction['mean'], name = f\"{column} mean at instruction phase environment\",\n",
    "                      line_color=\"red\",\n",
    "                      line_width = 3,\n",
    "                      line_alpha = .8)\n",
    "    objectToPlot.line(dfPost['mean'].index.tolist(), dfPost['mean'], name = f\"{column} mean at break phase environment\",\n",
    "                      line_color=\"blue\",\n",
    "                      line_width = 3,\n",
    "                      line_alpha = .8)\n",
    "    \n",
    "    \n",
    "    objectToPlot.line(dfPre['mean'].index.tolist(), dfPre['mean'] + dfPre['std'], name = \"SD\",\n",
    "                      line_dash = 'dotted',\n",
    "                      line_color=\"green\",\n",
    "                      line_width = 3,\n",
    "                      line_alpha = .8)\n",
    "    data = dfInstruction['mean'] + dfInstruction['std']\n",
    "    objectToPlot.line(dfInstruction['mean'].index.tolist(),data , name = \"SD\",\n",
    "                      line_dash = 'dotted',\n",
    "                      line_color=\"red\",\n",
    "                      line_width = 3,\n",
    "                      line_alpha = .8)\n",
    "    objectToPlot.line(dfPost['mean'].index.tolist(), dfPost['mean'] + dfPost['std'], name = \"SD\",\n",
    "                      line_dash = 'dotted',\n",
    "                      line_color=\"blue\",\n",
    "                      line_width = 3,\n",
    "                      line_alpha = .8)\n",
    "    \n",
    "    objectToPlot.line(dfPre['mean'].index.tolist(), dfPre['mean'] - dfPre['std'], name = \"SD\",\n",
    "                      line_dash = 'dotted',\n",
    "                      line_color=\"green\",\n",
    "                      line_width = 3,\n",
    "                      line_alpha = .8)\n",
    "    objectToPlot.line(dfInstruction['mean'].index.tolist(), dfInstruction['mean'] - dfInstruction['std'], name = \"SD\",\n",
    "                      line_dash = 'dotted',\n",
    "                      line_color=\"red\",\n",
    "                      line_width = 3,\n",
    "                      line_alpha = .8)\n",
    "    objectToPlot.line(dfPost['mean'].index.tolist(), dfPost['mean'] - dfPost['std'], name = \"SD\",\n",
    "                      line_dash = 'dotted',\n",
    "                      line_color=\"blue\",\n",
    "                      line_width = 3,\n",
    "                      line_alpha = .8)\n",
    "    \n",
    "    objectToPlot.background_fill_color = \"beige\"\n",
    "    objectToPlot.background_fill_alpha = 0.5\n",
    "    save(objectToPlot)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f5aa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "drawThis(\"bpm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df62dcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in columnNames:\n",
    "    try:\n",
    "        print(column)\n",
    "        drawThis(column)\n",
    "    except Exception as e:\n",
    "        print(\"can't draw this!\")\n",
    "        print(e)\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e3dbda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae8cbe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0503467c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cfcf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawThisOLD(column):\n",
    "    from bokeh.models.tools import HoverTool\n",
    "\n",
    "    print(f\"<+>==============Drawing={column}+++++++++++++++++++++++<<<\")\n",
    "    print(noTailsDataSet[1].head())\n",
    "    print(pre[1])\n",
    "    print(\"==============\")\n",
    "    pre = [noTailsDataSet[i].loc[:breakStarts[i]-1, column] for i in range(0, len(noTailsDataSet))]\n",
    "    instruction = [noTailsDataSet[i].loc[audioStarts[i]:audioEnds[i]-1, column] for i in range(0, len(noTailsDataSet))]\n",
    "    #print(instruction[1])\n",
    "    post = [noTailsDataSet[i].loc[audioEnds[i]:breakEnds[i], column] for i in range(0, len(noTailsDataSet))]\n",
    "    #print(post[1])\n",
    "    #postAll = [noTailsDataSet[i].loc[:breakEnds[i], column] for i in range(0, len(noTailsDataSet))]\n",
    "\n",
    "    #MEANS not needed now\n",
    "    \"\"\"\n",
    "    meanForTS = lambda timeSeries: sum(timeSeries)/len(timeSeries)\n",
    "    meansPre = [meanForTS(item) for item in pre]\n",
    "    meansPost = [meanForTS(item) for item in post]\n",
    "\n",
    "    print(f\"{column} : {stats.shapiro(meansPre)}\")\n",
    "    plt.hist(meansPre,20)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"{column} : {stats.shapiro(meansPost)}\")\n",
    "    plt.hist(meansPost,20)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"{column} : {stats.ttest_rel(meansPre, meansPost)}\")\n",
    "    print(f\"{column} : {stats.wilcoxon(meansPre, meansPost)}\")\n",
    "    \"\"\"\n",
    "    \n",
    "    ###OTher way through DF\n",
    "    dfPre = pd.DataFrame(pre)\n",
    "    #dfPre = pd.pivot_table(dfPre)\n",
    "    #print(dfPre) \n",
    "    dfInstruction = pd.DataFrame(instruction)\n",
    "    #print(dfInstruction)\n",
    "    dfPost = pd.DataFrame(post)    \n",
    "    #print(dfInstruction)\n",
    "    \n",
    "    graphPath = \"Graphs\\/\"+ column + \".html\";\n",
    "    output_file(graphPath)\n",
    "    \n",
    "    TOOLTIPS = [\n",
    "            (\"index\", \"$i\"),\n",
    "            (\"(x,y)\", \"($pre[i], $pre[i].index.tolist())\"),\n",
    "            (\"desc\", \"@subjectNo\"),\n",
    "        ]\n",
    "    \n",
    "    objectToPlot = figure(\n",
    "        #y_range = (0,1),\n",
    "        #x_range = (0,1),\n",
    "        title = column,\n",
    "        x_axis_label = 'Index[i]',\n",
    "        y_axis_label = column,\n",
    "        plot_width = 1600,\n",
    "        #plot_height = 900,\n",
    "        sizing_mode='scale_width',\n",
    "        tools=[HoverTool()],\n",
    "        #tooltips=\"Data point @x for i has the value @y\",\n",
    "    )\n",
    "    \n",
    "    sumPre = pd.DataFrame()\n",
    "    sumInstruction = pd.DataFrame()\n",
    "    sumPost = pd.DataFrame()\n",
    "    \n",
    "    for i in range(0, len(noTailsDataSet)):\n",
    "        \n",
    "        if i == 0:\n",
    "            sumPre['TS'] = pre[i]\n",
    "            sumPre['Weights'] = 1\n",
    "            \n",
    "            sumInstruction['TS'] = instruction[i]\n",
    "            sumInstruction['Weights'] = 1\n",
    "            \n",
    "            sumPost['TS'] = post[i]\n",
    "            sumPost['Weights'] = 1\n",
    "        \n",
    "        else:\n",
    "            sumPre['TS'].iloc[:len(pre[i])-1] += pre[i]\n",
    "            sumPre['Weights'].iloc[:len(pre[i])-1] += 1\n",
    "            \n",
    "            sumInstruction['TS'].iloc[:len(instruction[i])-1] += instruction[i]\n",
    "            sumInstruction['Weights'].iloc[:len(instruction[i])-1] += 1\n",
    "            \n",
    "            sumPost['TS'].iloc[:len(post[i])-1] += post[i]\n",
    "            sumPost['Weights'].iloc[:len(post[i])-1] += 1\n",
    "        \n",
    "        \n",
    "        objectToPlot.line(pre[i].index.tolist(), pre[i], \n",
    "                          #legend_label='Consensys_HR',\n",
    "                          line_color=\"green\",\n",
    "                          line_width = .5,\n",
    "                          line_alpha = .5,\n",
    "                          )\n",
    "        objectToPlot.line(instruction[i].index.tolist(), instruction[i], \n",
    "                          #legend_label='Consensys_HR',\n",
    "                          line_color=\"red\",\n",
    "                          line_width = .5,\n",
    "                          line_alpha = .5)\n",
    "        objectToPlot.line(post[i].index.tolist(), post[i], \n",
    "                          #legend_label='Consensys_HR',\n",
    "                          line_color=\"blue\",\n",
    "                          line_width = .5,\n",
    "                          line_alpha = .5)\n",
    "    # for mean:\n",
    "    sumPre['TS'] = sumPre['TS'] / sumPre['Weights']\n",
    "    sumInstruction['TS'] = sumInstruction['TS'] / sumInstruction['Weights']\n",
    "    sumPost['TS'] = sumPost['TS'] / sumPost['Weights']\n",
    "    \n",
    "    objectToPlot.line(sumPre['TS'].index.tolist(), sumPre['TS'], \n",
    "                      #legend_label='Consensys_HR',\n",
    "                      line_color=\"green\",\n",
    "                      #line_width = (sumPre['Weights'] + 1)/10,\n",
    "                      line_width = 5,\n",
    "                      line_alpha = .9)\n",
    "    objectToPlot.line(sumInstruction['TS'].index.tolist(), sumInstruction['TS'], \n",
    "                      #legend_label='Consensys_HR',\n",
    "                      line_color=\"red\",\n",
    "                      #line_width = (sumInstruction['Weights'] + 1)/10,\n",
    "                      line_width = 5,\n",
    "                      line_alpha = .9)\n",
    "    objectToPlot.line(sumPost['TS'].index.tolist(), sumPost['TS'], \n",
    "                      #legend_label='Consensys_HR',\n",
    "                      line_color=\"blue\",\n",
    "                      #line_width = (sumPost['Weights'] + 1)/10,\n",
    "                      line_width = 5,\n",
    "                      line_alpha = .9)\n",
    "    \n",
    "    #for i in range(0, len(noTailsDataSet)):\n",
    "    \n",
    "    objectToPlot.background_fill_color = \"beige\"\n",
    "    objectToPlot.background_fill_alpha = 0.5\n",
    "    save(objectToPlot)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9664aa29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1699846e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198fac65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35505c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528faac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graphPath = \"Graphs\\/\"+ participantID + \"_\" + str(participantGR) + \"-HR.html\";\n",
    "    output_file(graphPath)\n",
    "    \n",
    "    objectToPlot = figure(\n",
    "        #y_range = (0,1),\n",
    "        #x_range = (0,1),\n",
    "        title = 'Heart Rate data - ID: ' + str(participantID) + ' GR: ' + str(participantGR),\n",
    "        x_axis_label = 'Index[i]',\n",
    "        y_axis_label = 'HR[BPM]',\n",
    "        plot_width = 1600,\n",
    "        #plot_height = 900,\n",
    "        sizing_mode='scale_width'\n",
    "    )\n",
    "    \n",
    "    objectToPlot.line(HP_data.index.tolist(), HP_data['HR_HeartPy_interpolated'], \n",
    "                      legend_label='HP_HR-interpolated', line_color=\"red\", line_width = 1)\n",
    "    objectToPlot.line(HP_data.index.tolist(), HP_data['bpm'], \n",
    "                      legend_label='HeartPy_HR', line_color=\"blue\", line_width = 4,\n",
    "                      line_dash = 'dotted', line_alpha = .7)\n",
    "    objectToPlot.line(Consensys_data.index.tolist(), Consensys_data['PPG_to_HR_CONSENSYS'], \n",
    "                      legend_label='Consensys_HR', line_color=\"green\", line_width = 4,\n",
    "                      line_dash = 'dotted', line_alpha = .7)\n",
    "    objectToPlot.line(Consensys_data.index.tolist(), Consensys_data['HR_CONSENSYS_interpolated'], \n",
    "                      legend_label='Consensys_HR-interpolated', line_color=\"green\", line_width = 1)\n",
    "    \n",
    "    objectToPlot.background_fill_color = \"beige\"\n",
    "    objectToPlot.background_fill_alpha = 0.5\n",
    "    save(objectToPlot)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
